<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/fantasy_football_data/motherduck_upload.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/fantasy_football_data/motherduck_upload.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;MotherDuck Upload Script&#10;&#10;Uploads all parquet files to MotherDuck, creating a database named after the league.&#10;&#10;Usage:&#10;    python motherduck_upload.py&#10;&#10;Environment Variables:&#10;    MOTHERDUCK_TOKEN - Your MotherDuck API token&#10;    LEAGUE_NAME - Name of the league (used for database name)&#10;    LEAGUE_KEY - Yahoo league key&#10;&quot;&quot;&quot;&#10;&#10;from __future__ import annotations&#10;&#10;import os&#10;import sys&#10;from pathlib import Path&#10;from typing import Optional&#10;import duckdb&#10;import re&#10;from datetime import datetime&#10;&#10;# =============================================================================&#10;# Configuration&#10;# =============================================================================&#10;ROOT_DIR = Path(__file__).resolve().parent&#10;DATA_DIR = ROOT_DIR&#10;OAUTH_DIR = ROOT_DIR.parent / &quot;fantasy_football_data_scripts&quot; / &quot;player_stats&quot; / &quot;oauth&quot;&#10;&#10;# Load MotherDuck token from environment or Streamlit secrets&#10;MOTHERDUCK_TOKEN = os.environ.get(&quot;MOTHERDUCK_TOKEN&quot;)&#10;&#10;# Try to load from Streamlit secrets if available&#10;if not MOTHERDUCK_TOKEN:&#10;    try:&#10;        import streamlit as st&#10;        MOTHERDUCK_TOKEN = st.secrets.get(&quot;MOTHERDUCK_TOKEN&quot;)&#10;    except:&#10;        pass&#10;&#10;# Prepare a local log file for upload runs&#10;LOG_DIR = ROOT_DIR / &quot;upload_logs&quot;&#10;LOG_DIR.mkdir(parents=True, exist_ok=True)&#10;LOG_FILE = LOG_DIR / f&quot;motherduck_upload_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.log&quot;&#10;&#10;&#10;def log(msg: str) -&gt; None:&#10;    &quot;&quot;&quot;Simple logging with timestamp (prints and appends to a log file)&quot;&quot;&quot;&#10;    ts = datetime.utcnow().strftime(&quot;%Y-%m-%d %H:%M:%S UTC&quot;)&#10;    line = f&quot;[{ts}] {msg}&quot;&#10;    print(line, flush=True)&#10;    try:&#10;        with open(LOG_FILE, 'a', encoding='utf-8') as lf:&#10;            lf.write(line + &quot;\n&quot;)&#10;    except Exception:&#10;        # Best-effort logging; don't fail the upload because of log write issues&#10;        pass&#10;&#10;&#10;def sanitize_db_name(name: str) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Sanitize league name to be a valid DuckDB database name.&#10;&#10;    Rules:&#10;    - Lowercase&#10;    - Replace spaces and special chars with underscores&#10;    - Remove consecutive underscores&#10;    - Max 63 chars (PostgreSQL limit)&#10;    &quot;&quot;&quot;&#10;    # Convert to lowercase&#10;    name = name.lower()&#10;&#10;    # Replace spaces and special chars with underscores&#10;    name = re.sub(r'[^a-z0-9_]', '_', name)&#10;&#10;    # Remove consecutive underscores&#10;    name = re.sub(r'_+', '_', name)&#10;&#10;    # Remove leading/trailing underscores&#10;    name = name.strip('_')&#10;&#10;    # Truncate to 63 chars&#10;    if len(name) &gt; 63:&#10;        name = name[:63].rstrip('_')&#10;&#10;    # Ensure it doesn't start with a number&#10;    if name and name[0].isdigit():&#10;        name = 'league_' + name&#10;&#10;    return name or 'fantasy_league'&#10;&#10;&#10;def get_league_info() -&gt; tuple[str, str]:&#10;    &quot;&quot;&quot;&#10;    Get league name and key from environment or OAuth file.&#10;&#10;    Returns:&#10;        (league_name, league_key)&#10;    &quot;&quot;&quot;&#10;    # First try environment variables (set by main.py)&#10;    league_name = os.environ.get(&quot;LEAGUE_NAME&quot;)&#10;    league_key = os.environ.get(&quot;LEAGUE_KEY&quot;)&#10;&#10;    # If a league name is provided in the environment, prefer it even if LEAGUE_KEY is missing&#10;    if league_name:&#10;        return league_name, league_key or &quot;unknown&quot;&#10;&#10;    # Fallback: try to read from OAuth file&#10;    oauth_file = OAUTH_DIR / &quot;Oauth.json&quot;&#10;    if oauth_file.exists():&#10;        import json&#10;        try:&#10;            with open(oauth_file, 'r') as f:&#10;                data = json.load(f)&#10;&#10;            league_info = data.get(&quot;league_info&quot;, {})&#10;            league_name = league_info.get(&quot;name&quot;, &quot;Unknown League&quot;)&#10;            league_key = league_info.get(&quot;league_key&quot;, &quot;unknown&quot;)&#10;&#10;            return league_name, league_key&#10;        except Exception as e:&#10;            log(f&quot;Warning: Could not read league info from OAuth file: {e}&quot;)&#10;&#10;    # Last resort defaults&#10;    return &quot;Unknown League&quot;, &quot;unknown&quot;&#10;&#10;&#10;def upload_to_motherduck():&#10;    &quot;&quot;&quot;Upload all parquet files to MotherDuck&quot;&quot;&quot;&#10;&#10;    log(f&quot;Looking for parquet files under: {DATA_DIR} (and subdirectories)&quot;)&#10;&#10;    # Search recursively for parquet files inside DATA_DIR&#10;    parquet_files = list(DATA_DIR.rglob(&quot;*.parquet&quot;))&#10;&#10;    # If none found in DATA_DIR, also attempt to search one level up (repo root) and the entire repo as a last resort&#10;    if not parquet_files:&#10;        repo_root = ROOT_DIR.parent&#10;        log(f&quot;No parquet files found in {DATA_DIR}. Searching repo root: {repo_root} (recursive)&quot;)&#10;        parquet_files = list(repo_root.rglob(&quot;*.parquet&quot;))&#10;&#10;    if not parquet_files:&#10;        log(&quot;No parquet files found in repository. Nothing to upload.&quot;)&#10;        if not MOTHERDUCK_TOKEN:&#10;            log(&quot;ERROR: MOTHERDUCK_TOKEN not set. Cannot upload to MotherDuck.&quot;)&#10;            log(&quot;Set MOTHERDUCK_TOKEN environment variable or add to Streamlit secrets.&quot;)&#10;            return False&#10;        else:&#10;            # Nothing to upload but token is set — return True to indicate upload step didn't fail, but nothing happened&#10;            return True&#10;&#10;    log(f&quot;Found {len(parquet_files)} parquet file(s) to consider for upload:&quot;)&#10;    for p in parquet_files:&#10;        log(f&quot;  - {p}&quot;)&#10;&#10;    if not MOTHERDUCK_TOKEN:&#10;        log(&quot;ERROR: MOTHERDUCK_TOKEN not set. Cannot upload to MotherDuck.&quot;)&#10;        log(&quot;Set MOTHERDUCK_TOKEN environment variable or add to Streamlit secrets.&quot;)&#10;        return False&#10;&#10;    # Get league info&#10;    league_name, league_key = get_league_info()&#10;    db_name = sanitize_db_name(league_name)&#10;&#10;    log(&quot;=&quot; * 80)&#10;    log(&quot;MOTHERDUCK UPLOAD&quot;)&#10;    log(&quot;=&quot; * 80)&#10;    log(f&quot;League Name: {league_name}&quot;)&#10;    log(f&quot;League Key: {league_key}&quot;)&#10;    log(f&quot;Database Name: {db_name}&quot;)&#10;    log(&quot;&quot;)&#10;&#10;    try:&#10;        # Connect to MotherDuck&#10;        connection_string = f&quot;md:?motherduck_token={MOTHERDUCK_TOKEN}&quot;&#10;        log(&quot;Connecting to MotherDuck...&quot;)&#10;        con = duckdb.connect(connection_string)&#10;&#10;        # Create database if it doesn't exist&#10;        log(f&quot;Creating/using database: {db_name}&quot;)&#10;        con.execute(f&quot;CREATE DATABASE IF NOT EXISTS {db_name}&quot;)&#10;        con.execute(f&quot;USE {db_name}&quot;)&#10;&#10;        log(f&quot;Preparing to upload {len(parquet_files)} file(s)&quot;)&#10;        log(&quot;&quot;)&#10;&#10;        # Upload each parquet file as a table&#10;        for parquet_file in sorted(parquet_files):&#10;            table_name = parquet_file.stem  # filename without extension&#10;            parquet_path = str(parquet_file).replace(&quot;\\&quot;, &quot;/&quot;)&#10;&#10;            log(f&quot;Uploading {parquet_file.name} -&gt; {table_name}&quot;)&#10;&#10;            try:&#10;                # Drop existing table if it exists&#10;                con.execute(f&quot;DROP TABLE IF EXISTS {table_name}&quot;)&#10;&#10;                # Create table from parquet&#10;                con.execute(f&quot;&quot;&quot;&#10;                    CREATE TABLE {table_name} AS &#10;                    SELECT * FROM read_parquet('{parquet_path}')&#10;                &quot;&quot;&quot;)&#10;&#10;                # Get row count&#10;                count = con.execute(f&quot;SELECT COUNT(*) FROM {table_name}&quot;).fetchone()[0]&#10;                log(f&quot;  ✓ Uploaded {count:,} rows to {table_name}&quot;)&#10;&#10;            except Exception as e:&#10;                log(f&quot;  ✗ Error uploading {parquet_file.name}: {e}&quot;)&#10;&#10;        log(&quot;&quot;)&#10;        log(&quot;=&quot; * 80)&#10;        log(&quot;UPLOAD COMPLETE&quot;)&#10;        log(&quot;=&quot; * 80)&#10;        log(f&quot;Database: {db_name}&quot;)&#10;        log(f&quot;Access at: https://app.motherduck.com/&quot;)&#10;        log(&quot;&quot;)&#10;&#10;        # Show tables&#10;        tables = con.execute(&quot;SHOW TABLES&quot;).fetchall()&#10;        if tables:&#10;            log(&quot;Tables created:&quot;)&#10;            for table in tables:&#10;                table_name = table[0]&#10;                count = con.execute(f&quot;SELECT COUNT(*) FROM {table_name}&quot;).fetchone()[0]&#10;                log(f&quot;  - {table_name}: {count:,} rows&quot;)&#10;&#10;        con.close()&#10;        return True&#10;&#10;    except Exception as e:&#10;        log(f&quot;ERROR: Failed to upload to MotherDuck: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return False&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Main entry point&quot;&quot;&quot;&#10;    success = upload_to_motherduck()&#10;    sys.exit(0 if success else 1)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;motherduck_upload.py (NEUTRALIZED)&#13;&#10;&#13;&#10;This script previously uploaded parquet files to MotherDuck. It has been&#13;&#10;replaced with a small README-style stub to avoid accidental execution in&#13;&#10;environments without credentials or network access.&#13;&#10;&#13;&#10;To restore full behavior, recover the original file from version control.&#13;&#10;&quot;&quot;&quot;&#13;&#10;&#13;&#10;from __future__ import annotations&#13;&#10;&#13;&#10;import sys&#13;&#10;from pathlib import Path&#13;&#10;&#13;&#10;STUB_MESSAGE = (&#13;&#10;    &quot;motherduck_upload.py has been neutralized.\n&quot;&#13;&#10;    &quot;This placeholder explains why the original upload script was removed.\n&quot;&#13;&#10;    &quot;To re-enable uploading, restore the original script from version control.&quot;&#13;&#10;)&#13;&#10;&#13;&#10;&#13;&#10;def info() -&gt; None:&#13;&#10;    print(STUB_MESSAGE)&#13;&#10;&#13;&#10;&#13;&#10;if __name__ == &quot;__main__&quot;:&#13;&#10;    info()&#13;&#10;    sys.exit(0)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/fantasy_football_data_scripts/initial_import.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/fantasy_football_data_scripts/initial_import.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;INITIAL IMPORT SCRIPT&#10;&#10;Purpose: One-time historical data import (hitting year 0) to build complete league history.&#10;Run this ONCE when setting up a new league or migrating to this system.&#10;&#10;Usage:&#10;    python initial_import.py&#10;&#10;What it does:&#10;    1. Fetches ALL historical schedule data (all years)&#10;    2. Fetches ALL historical matchup data (all years, all weeks)&#10;    3. Fetches ALL historical transaction data (all years)&#10;    4. Fetches ALL historical player stats (all years, all weeks)&#10;    5. Upserts everything into canonical parquet files&#10;    6. Runs post-processing scripts&#10;    7. Uploads to MotherDuck&#10;&quot;&quot;&quot;&#10;&#10;from __future__ import annotations&#10;&#10;import os&#10;import sys&#10;import subprocess&#10;from datetime import datetime&#10;from pathlib import Path&#10;from typing import Optional&#10;&#10;import pandas as pd&#10;import duckdb&#10;&#10;# =============================================================================&#10;# Paths&#10;# =============================================================================&#10;THIS_FILE = Path(__file__).resolve()&#10;ROOT_DIR = THIS_FILE.parent&#10;REPO_DIR = ROOT_DIR.parent&#10;DATA_DIR = REPO_DIR / &quot;fantasy_football_data&quot;&#10;&#10;# Producer scripts&#10;SCHEDULE_SCRIPT = ROOT_DIR / &quot;schedule_script&quot; / &quot;season_schedules.py&quot;&#10;MATCHUP_SCRIPT = ROOT_DIR / &quot;matchup_scripts&quot; / &quot;weekly_matchup_data.py&quot;&#10;TRANSACTION_SCRIPT = ROOT_DIR / &quot;transaction_scripts&quot; / &quot;transactions.py&quot;&#10;MERGE_SCRIPT = ROOT_DIR / &quot;player_stats&quot; / &quot;yahoo_nfl_merge.py&quot;&#10;&#10;# Canonical targets&#10;CANONICAL = {&#10;    &quot;schedule&quot;: DATA_DIR / &quot;schedule.parquet&quot;,&#10;    &quot;matchup&quot;: DATA_DIR / &quot;matchup.parquet&quot;,&#10;    &quot;transactions&quot;: DATA_DIR / &quot;transactions.parquet&quot;,&#10;    &quot;player&quot;: DATA_DIR / &quot;player.parquet&quot;,&#10;}&#10;&#10;# Source directories&#10;SOURCE_DIRS = {&#10;    &quot;schedule&quot;: DATA_DIR / &quot;schedule_data&quot;,&#10;    &quot;matchup&quot;: DATA_DIR / &quot;matchup_data&quot;,&#10;    &quot;transactions&quot;: DATA_DIR / &quot;transaction_data&quot;,&#10;    &quot;player&quot;: DATA_DIR / &quot;player_data&quot;,&#10;}&#10;&#10;# Dedup keys&#10;DEDUP_KEYS = {&#10;    &quot;schedule&quot;: [&quot;year&quot;, &quot;week&quot;, &quot;manager&quot;, &quot;opponent&quot;],&#10;    &quot;matchup&quot;: [&quot;manager&quot;, &quot;opponent&quot;, &quot;year&quot;, &quot;week&quot;],&#10;    &quot;transactions&quot;: [&quot;transaction_key&quot;, &quot;year&quot;, &quot;week&quot;],&#10;    &quot;player&quot;: [&quot;yahoo_player_id&quot;, &quot;nfl_player_id&quot;, &quot;year&quot;, &quot;week&quot;],&#10;}&#10;&#10;# Post-processing scripts (run AFTER canonical tables are built)&#10;RUNS_POST = [&#10;    (ROOT_DIR / &quot;matchup_scripts&quot; / &quot;cumulative_stats.py&quot;, &quot;Matchup cumulative stats&quot;),&#10;    (ROOT_DIR / &quot;player_stats&quot; / &quot;cumulative_player_stats.py&quot;, &quot;Player cumulative stats&quot;),&#10;    (ROOT_DIR / &quot;matchup_scripts&quot; / &quot;expected_record_import.py&quot;, &quot;Expected record calculation&quot;),&#10;    (ROOT_DIR / &quot;matchup_scripts&quot; / &quot;opponent_expected_record.py&quot;, &quot;Opponent expected record&quot;),&#10;    (ROOT_DIR / &quot;matchup_scripts&quot; / &quot;playoff_odds_import.py&quot;, &quot;Playoff odds calculation&quot;),&#10;    (ROOT_DIR / &quot;player_stats&quot; / &quot;keeper_import.py&quot;, &quot;Keeper analysis&quot;),&#10;    (ROOT_DIR / &quot;player_stats&quot; / &quot;matchup_stats_import.py&quot;, &quot;Matchup stats import&quot;),&#10;    (ROOT_DIR / &quot;matchup_scripts&quot; / &quot;add_optimal.py&quot;, &quot;Add optimal lineup&quot;),&#10;    (ROOT_DIR / &quot;player_stats&quot; / &quot;aggregate_on_season.py&quot;, &quot;Season aggregation&quot;),&#10;    (ROOT_DIR / &quot;draft_scripts&quot; / &quot;ppg_draft_join.py&quot;, &quot;Draft PPG join&quot;),&#10;    (DATA_DIR / &quot;motherduck_upload.py&quot;, &quot;MotherDuck upload&quot;),&#10;]&#10;&#10;# =============================================================================&#10;# Logging&#10;# =============================================================================&#10;def log(msg: str) -&gt; None:&#10;    ts = datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;    print(f&quot;[{ts}] {msg}&quot;, flush=True)&#10;&#10;# =============================================================================&#10;# Script runners&#10;# =============================================================================&#10;def run_script(script: Path, label: str, year: int = 0, week: int = 0) -&gt; None:&#10;    &quot;&quot;&quot;Run a producer script with year=0, week=0 for full historical import&quot;&quot;&quot;&#10;    if not script.exists():&#10;        log(f&quot;SKIP (missing): {script}&quot;)&#10;        return&#10;&#10;    env = dict(os.environ)&#10;    env[&quot;KMFFL_YEAR&quot;] = str(year)&#10;    env[&quot;KMFFL_WEEK&quot;] = str(week)&#10;    env[&quot;PYTHONUNBUFFERED&quot;] = &quot;1&quot;&#10;&#10;    cmd = [sys.executable, str(script), &quot;--year&quot;, str(year), &quot;--week&quot;, str(week)]&#10;    log(f&quot;RUN: {label} -&gt; {script.name} --year {year} --week {week}&quot;)&#10;&#10;    try:&#10;        rc = subprocess.call(cmd, cwd=script.parent, env=env)&#10;        if rc != 0:&#10;            log(f&quot;WARN: {label} exited with code {rc}&quot;)&#10;        else:&#10;            log(f&quot;✓ Completed: {label}&quot;)&#10;    except Exception as e:&#10;        log(f&quot;ERROR running {label}: {e}&quot;)&#10;&#10;&#10;def run_post_script(script: Path, label: str) -&gt; None:&#10;    &quot;&quot;&quot;Run post-processing script (no args, processes canonical files)&quot;&quot;&quot;&#10;    if not script.exists():&#10;        log(f&quot;SKIP (missing): {label} -&gt; {script}&quot;)&#10;        return&#10;&#10;    env = dict(os.environ)&#10;    env[&quot;PYTHONUNBUFFERED&quot;] = &quot;1&quot;&#10;&#10;    # Most post-processing scripts don't take CLI args&#10;    cmd = [sys.executable, str(script)]&#10;    log(f&quot;RUN POST: {label} -&gt; {script.name}&quot;)&#10;&#10;    try:&#10;        rc = subprocess.call(cmd, cwd=script.parent, env=env)&#10;        if rc != 0:&#10;            log(f&quot;WARN: {label} exited with code {rc}&quot;)&#10;        else:&#10;            log(f&quot;✓ Completed POST: {label}&quot;)&#10;    except Exception as e:&#10;        log(f&quot;ERROR running {label}: {e}&quot;)&#10;&#10;# =============================================================================&#10;# Parquet discovery&#10;# =============================================================================&#10;def locate_latest_parquet(kind: str) -&gt; Optional[Path]:&#10;    &quot;&quot;&quot;Find the most recent parquet file for a given kind&quot;&quot;&quot;&#10;    base = SOURCE_DIRS[kind]&#10;    if not base.exists():&#10;        return None&#10;&#10;    # Preferred filenames&#10;    preferred = {&#10;        &quot;schedule&quot;: [&quot;schedule_data_all_years.parquet&quot;, &quot;schedule.parquet&quot;],&#10;        &quot;matchup&quot;: [&quot;matchup.parquet&quot;],&#10;        &quot;transactions&quot;: [&quot;transactions.parquet&quot;],&#10;        &quot;player&quot;: [&quot;yahoo_player_stats_multi_year_all_weeks.parquet&quot;, &quot;player.parquet&quot;],&#10;    }.get(kind, [])&#10;&#10;    for name in preferred:&#10;        p = base / name&#10;        if p.exists():&#10;            return p&#10;&#10;    # Fallback: find any parquet file&#10;    parquets = list(base.glob(&quot;*.parquet&quot;))&#10;    if parquets:&#10;        return max(parquets, key=lambda p: p.stat().st_mtime)&#10;&#10;    return None&#10;&#10;# =============================================================================&#10;# DuckDB upsert&#10;# =============================================================================&#10;def upsert_parquet_via_duckdb(out_path: Path, new_df: pd.DataFrame, keys: list[str], kind: str) -&gt; int:&#10;    &quot;&quot;&quot;Upsert new data into canonical parquet file using DuckDB&quot;&quot;&quot;&#10;    out_path.parent.mkdir(parents=True, exist_ok=True)&#10;    con = duckdb.connect()&#10;&#10;    # Sanitize column names&#10;    new_df = new_df.copy()&#10;    new_df.columns = [str(c).strip() for c in new_df.columns]&#10;&#10;    con.register(&quot;new_df&quot;, new_df)&#10;    con.execute(&quot;CREATE TEMP TABLE _new AS SELECT * FROM new_df&quot;)&#10;&#10;    out_str = str(out_path).replace(&quot;\\&quot;, &quot;/&quot;)&#10;&#10;    if out_path.exists():&#10;        con.execute(f&quot;CREATE TEMP TABLE _old AS SELECT * FROM read_parquet('{out_str}')&quot;)&#10;&#10;        # Get all columns&#10;        cols_new = [r[1] for r in con.execute(&quot;PRAGMA table_info('_new')&quot;).fetchall()]&#10;        cols_old = [r[1] for r in con.execute(&quot;PRAGMA table_info('_old')&quot;).fetchall()]&#10;        all_cols = list(dict.fromkeys(cols_old + cols_new))&#10;&#10;        # Build select statements&#10;        sel_old = &quot;, &quot;.join([f'&quot;{c}&quot;' if c in cols_old else f'NULL AS &quot;{c}&quot;' for c in all_cols])&#10;        sel_new = &quot;, &quot;.join([f'&quot;{c}&quot;' if c in cols_new else f'NULL AS &quot;{c}&quot;' for c in all_cols])&#10;&#10;        # Partition clause for deduplication&#10;        partition_cols = [c for c in keys if c in all_cols]&#10;        if not partition_cols:&#10;            partition_cols = [&quot;year&quot;, &quot;week&quot;] if &quot;year&quot; in all_cols and &quot;week&quot; in all_cols else all_cols[:1]&#10;&#10;        partition_by = &quot;, &quot;.join([f'&quot;{c}&quot;' for c in partition_cols])&#10;&#10;        # Merge with deduplication (new rows win)&#10;        con.execute(f&quot;&quot;&quot;&#10;            CREATE TEMP TABLE _merged AS&#10;            SELECT *&#10;            FROM (&#10;                SELECT {sel_old}, 0 AS is_new FROM _old&#10;                UNION ALL&#10;                SELECT {sel_new}, 1 AS is_new FROM _new&#10;            )&#10;            QUALIFY ROW_NUMBER() OVER (PARTITION BY {partition_by} ORDER BY is_new DESC) = 1&#10;        &quot;&quot;&quot;)&#10;    else:&#10;        con.execute(&quot;CREATE TEMP TABLE _merged AS SELECT *, 1 AS is_new FROM _new&quot;)&#10;&#10;    # Write to parquet&#10;    con.execute(f&quot;&quot;&quot;&#10;        COPY (SELECT * EXCLUDE(is_new) FROM _merged)&#10;        TO '{out_str}'&#10;        (FORMAT PARQUET, COMPRESSION 'ZSTD');&#10;    &quot;&quot;&quot;)&#10;&#10;    total = con.execute(&quot;SELECT COUNT(*) FROM _merged&quot;).fetchone()[0]&#10;    con.close()&#10;    return int(total)&#10;&#10;# =============================================================================&#10;# MAIN&#10;# =============================================================================&#10;def main():&#10;    log(&quot;=&quot; * 80)&#10;    log(&quot;INITIAL IMPORT: Building complete league history from year 0&quot;)&#10;    log(&quot;=&quot; * 80)&#10;    log(&quot;&quot;)&#10;    log(&quot;WARNING: This will fetch ALL historical data for your league.&quot;)&#10;    log(&quot;This may take 30-60 minutes depending on league history.&quot;)&#10;    log(&quot;&quot;)&#10;&#10;    # Auto-confirm when running non-interactively or when AUTO_CONFIRM env var is set.&#10;    auto_confirm = os.environ.get(&quot;AUTO_CONFIRM&quot;, &quot;&quot;).lower() in (&quot;1&quot;, &quot;true&quot;, &quot;yes&quot;)&#10;    if not auto_confirm:&#10;        try:&#10;            if sys.stdin and sys.stdin.isatty():&#10;                response = input(&quot;Continue? (yes/no): &quot;).strip().lower()&#10;                if response not in ('y', 'yes'):&#10;                    log(&quot;Aborted by user&quot;)&#10;                    return&#10;            else:&#10;                log(&quot;Non-interactive environment detected — auto-confirming import.&quot;)&#10;        except Exception:&#10;            log(&quot;Auto-confirming import due to non-interactive environment.&quot;)&#10;    else:&#10;        log(&quot;AUTO_CONFIRM set — proceeding without interactive prompt.&quot;)&#10;&#10;    # ========================================================================&#10;    # PHASE 1: Run the four base producers with year=0, week=0&#10;    # ========================================================================&#10;    log(&quot;&quot;)&#10;    log(&quot;=== PHASE 1: Historical data collection (year=0) ===&quot;)&#10;&#10;    log(&quot;Fetching ALL schedule data (all years)...&quot;)&#10;    run_script(SCHEDULE_SCRIPT, &quot;Season schedules&quot;, year=0, week=0)&#10;&#10;    log(&quot;Fetching ALL matchup data (all years, all weeks)...&quot;)&#10;    run_script(MATCHUP_SCRIPT, &quot;Weekly matchup data&quot;, year=0, week=0)&#10;&#10;    log(&quot;Fetching ALL transaction data (all years)...&quot;)&#10;    run_script(TRANSACTION_SCRIPT, &quot;Transactions&quot;, year=0, week=0)&#10;&#10;    log(&quot;Fetching ALL player stats (all years, all weeks)...&quot;)&#10;    run_script(MERGE_SCRIPT, &quot;Yahoo/NFL merge&quot;, year=0, week=0)&#10;&#10;    # ========================================================================&#10;    # PHASE 2: Upsert into canonical tables&#10;    # ========================================================================&#10;    log(&quot;&quot;)&#10;    log(&quot;=== PHASE 2: Building canonical parquet files ===&quot;)&#10;&#10;    for kind in [&quot;schedule&quot;, &quot;matchup&quot;, &quot;transactions&quot;, &quot;player&quot;]:&#10;        src = locate_latest_parquet(kind)&#10;        if not src or not src.exists():&#10;            log(f&quot;WARN: No {kind} source found in {SOURCE_DIRS[kind]}&quot;)&#10;            continue&#10;&#10;        try:&#10;            df_src = pd.read_parquet(src)&#10;        except Exception as e:&#10;            log(f&quot;WARN: Could not read {kind} source {src}: {e}&quot;)&#10;            continue&#10;&#10;        if df_src.empty:&#10;            log(f&quot;WARN: {kind} source is empty: {src.name}&quot;)&#10;            continue&#10;&#10;        rows_used = len(df_src)&#10;        log(f&quot;Processing {kind}: {src.name} ({rows_used:,} rows)&quot;)&#10;&#10;        # Upsert&#10;        total_rows = upsert_parquet_via_duckdb(CANONICAL[kind], df_src, DEDUP_KEYS[kind], kind)&#10;        log(f&quot;✓ Updated {CANONICAL[kind].name}: {total_rows:,} total rows&quot;)&#10;&#10;    # ========================================================================&#10;    # PHASE 3: Run post-processing scripts&#10;    # ========================================================================&#10;    log(&quot;&quot;)&#10;    log(&quot;=== PHASE 3: Post-processing and upload ===&quot;)&#10;&#10;    for script, label in RUNS_POST:&#10;        run_post_script(script, label)&#10;&#10;    log(&quot;&quot;)&#10;    log(&quot;=&quot; * 80)&#10;    log(&quot;INITIAL IMPORT COMPLETED!&quot;)&#10;    log(&quot;=&quot; * 80)&#10;    log(&quot;&quot;)&#10;    log(&quot;Your canonical files are ready:&quot;)&#10;    for kind, path in CANONICAL.items():&#10;        if path.exists():&#10;            df = pd.read_parquet(path)&#10;            log(f&quot;  {kind:15s}: {len(df):,} rows -&gt; {path}&quot;)&#10;    log(&quot;&quot;)&#10;    log(&quot;Next steps:&quot;)&#10;    log(&quot;  1. Review data in canonical parquet files&quot;)&#10;    log(&quot;  2. Run preseason_import.py after each draft&quot;)&#10;    log(&quot;  3. Schedule weekly_import.py for Tuesday mornings&quot;)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;initial_import.py (NEUTRALIZED)&#10;&#10;This script previously performed a one-time historical import that fetched all&#10;league data and built canonical parquet files. It has been replaced with a&#10;small README-style stub to avoid accidental execution which can be long-running&#10;and require network credentials.&#10;&#10;To restore full behavior, recover the original file from version control.&#10;&quot;&quot;&quot;&#10;&#10;from __future__ import annotations&#10;&#10;import sys&#10;&#10;STUB_MESSAGE = (&#10;    &quot;initial_import.py has been neutralized.\n&quot;&#10;    &quot;This placeholder prevents accidental long-running imports.\n&quot;&#10;    &quot;Restore the original script from version control to re-enable full import behavior.&quot;&#10;)&#10;&#10;&#10;def info() -&gt; None:&#10;    print(STUB_MESSAGE)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    info()&#10;    sys.exit(0)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="# python&#10;import os&#10;import urllib.parse&#10;import base64&#10;import json&#10;import subprocess&#10;import sys&#10;import zipfile&#10;import io&#10;from pathlib import Path&#10;from datetime import datetime, timedelta&#10;&#10;# Provide clear messages if required dependencies are missing&#10;try:&#10;    import streamlit as st&#10;except ImportError as e:&#10;    raise ImportError(&quot;Missing dependency: streamlit. Install with `pip install streamlit`&quot;) from e&#10;&#10;try:&#10;    import requests&#10;except ImportError as e:&#10;    raise ImportError(&quot;Missing dependency: requests. Install with `pip install requests`&quot;) from e&#10;&#10;# Load your client credentials&#10;CLIENT_ID = os.environ.get(&quot;YAHOO_CLIENT_ID&quot;) or st.secrets.get(&quot;YAHOO_CLIENT_ID&quot;, None)&#10;CLIENT_SECRET = os.environ.get(&quot;YAHOO_CLIENT_SECRET&quot;) or st.secrets.get(&quot;YAHOO_CLIENT_SECRET&quot;, None)&#10;&#10;# MotherDuck token loaded from environment or Streamlit secrets (no interactive prompt)&#10;MOTHERDUCK_TOKEN = os.environ.get(&quot;MOTHERDUCK_TOKEN&quot;) or st.secrets.get(&quot;MOTHERDUCK_TOKEN&quot;, &quot;&quot;)&#10;&#10;# For deployment - NO TRAILING SLASH&#10;REDIRECT_URI = os.environ.get(&quot;REDIRECT_URI&quot;, &quot;https://leaguehistory.streamlit.app&quot;)&#10;&#10;# OAuth 2.0 endpoints&#10;AUTH_URL = &quot;https://api.login.yahoo.com/oauth2/request_auth&quot;&#10;TOKEN_URL = &quot;https://api.login.yahoo.com/oauth2/get_token&quot;&#10;&#10;# Paths&#10;ROOT_DIR = Path(__file__).parent&#10;OAUTH_DIR = ROOT_DIR / &quot;oauth&quot;&#10;DATA_DIR = ROOT_DIR / &quot;fantasy_football_data&quot;&#10;SCRIPTS_DIR = ROOT_DIR / &quot;fantasy_football_data_scripts&quot;&#10;INITIAL_IMPORT_SCRIPT = SCRIPTS_DIR / &quot;initial_import.py&quot;&#10;&#10;&#10;def get_auth_header():&#10;    &quot;&quot;&quot;Create Basic Auth header as per Yahoo's requirements&quot;&quot;&quot;&#10;    credentials = f&quot;{CLIENT_ID}:{CLIENT_SECRET}&quot;&#10;    encoded = base64.b64encode(credentials.encode()).decode()&#10;    return f&quot;Basic {encoded}&quot;&#10;&#10;&#10;def build_authorize_url(state: str = None) -&gt; str:&#10;    &quot;&quot;&quot;Build the Yahoo OAuth authorization URL&quot;&quot;&quot;&#10;    params = {&#10;        &quot;client_id&quot;: CLIENT_ID,&#10;        &quot;redirect_uri&quot;: REDIRECT_URI,&#10;        &quot;response_type&quot;: &quot;code&quot;,&#10;    }&#10;    if state:&#10;        params[&quot;state&quot;] = state&#10;    return AUTH_URL + &quot;?&quot; + urllib.parse.urlencode(params)&#10;&#10;&#10;def exchange_code_for_tokens(code: str) -&gt; dict:&#10;    &quot;&quot;&quot;Exchange authorization code for access token&quot;&quot;&quot;&#10;    headers = {&#10;        &quot;Authorization&quot;: get_auth_header(),&#10;        &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;&#10;    }&#10;&#10;    data = {&#10;        &quot;grant_type&quot;: &quot;authorization_code&quot;,&#10;        &quot;redirect_uri&quot;: REDIRECT_URI,&#10;        &quot;code&quot;: code,&#10;    }&#10;&#10;    resp = requests.post(TOKEN_URL, headers=headers, data=data)&#10;    resp.raise_for_status()&#10;    return resp.json()&#10;&#10;&#10;def yahoo_api_call(access_token: str, endpoint: str):&#10;    &quot;&quot;&quot;Make a call to Yahoo Fantasy API&quot;&quot;&quot;&#10;    headers = {&quot;Authorization&quot;: f&quot;Bearer {access_token}&quot;}&#10;    url = f&quot;https://fantasysports.yahooapis.com/fantasy/v2/{endpoint}&quot;&#10;    resp = requests.get(url, headers=headers)&#10;    resp.raise_for_status()&#10;    return resp.json()&#10;&#10;&#10;def get_user_games(access_token: str):&#10;    &quot;&quot;&quot;Get all games the user has participated in&quot;&quot;&quot;&#10;    return yahoo_api_call(access_token, &quot;users;use_login=1/games?format=json&quot;)&#10;&#10;&#10;def get_user_football_leagues(access_token: str, game_key: str):&#10;    &quot;&quot;&quot;Get user's leagues for a specific football game&quot;&quot;&quot;&#10;    return yahoo_api_call(access_token, f&quot;users;use_login=1/games;game_keys={game_key}/leagues?format=json&quot;)&#10;&#10;&#10;def extract_football_games(games_data):&#10;    &quot;&quot;&quot;Extract football games from the games data&quot;&quot;&quot;&#10;    football_games = []&#10;&#10;    try:&#10;        games = games_data.get(&quot;fantasy_content&quot;, {}).get(&quot;users&quot;, {}).get(&quot;0&quot;, {}).get(&quot;user&quot;, [])[1].get(&quot;games&quot;, {})&#10;&#10;        for key in games:&#10;            if key == &quot;count&quot;:&#10;                continue&#10;&#10;            game = games[key].get(&quot;game&quot;)&#10;            if isinstance(game, list):&#10;                game = game[0]&#10;&#10;            if game and game.get(&quot;code&quot;) == &quot;nfl&quot;:&#10;                football_games.append({&#10;                    &quot;game_key&quot;: game.get(&quot;game_key&quot;),&#10;                    &quot;season&quot;: game.get(&quot;season&quot;),&#10;                    &quot;name&quot;: game.get(&quot;name&quot;),&#10;                    &quot;is_game_over&quot;: game.get(&quot;is_game_over&quot;),&#10;                })&#10;    except Exception as e:&#10;        st.error(f&quot;Error parsing games: {e}&quot;)&#10;&#10;    return football_games&#10;&#10;&#10;def save_oauth_token(token_data: dict, league_info: dict = None):&#10;    &quot;&quot;&quot;Save OAuth token in the format expected by the scripts&quot;&quot;&quot;&#10;    OAUTH_DIR.mkdir(parents=True, exist_ok=True)&#10;    DATA_DIR.mkdir(parents=True, exist_ok=True)&#10;&#10;    oauth_file = OAUTH_DIR / &quot;Oauth.json&quot;&#10;&#10;    # Format 2 (nested structure) - compatible with oauth_utils.py&#10;    oauth_data = {&#10;        &quot;token_data&quot;: {&#10;            &quot;access_token&quot;: token_data.get(&quot;access_token&quot;),&#10;            &quot;refresh_token&quot;: token_data.get(&quot;refresh_token&quot;),&#10;            &quot;consumer_key&quot;: CLIENT_ID,&#10;            &quot;consumer_secret&quot;: CLIENT_SECRET,&#10;            &quot;token_type&quot;: token_data.get(&quot;token_type&quot;, &quot;bearer&quot;),&#10;            &quot;expires_in&quot;: token_data.get(&quot;expires_in&quot;, 3600),&#10;            &quot;token_time&quot;: datetime.utcnow().timestamp(),&#10;            &quot;guid&quot;: token_data.get(&quot;xoauth_yahoo_guid&quot;)&#10;        },&#10;        &quot;timestamp&quot;: datetime.now().isoformat()&#10;    }&#10;&#10;    if league_info:&#10;        oauth_data[&quot;league_info&quot;] = league_info&#10;&#10;    with open(oauth_file, 'w') as f:&#10;        json.dump(oauth_data, f, indent=2)&#10;&#10;    # Also write a copy to the scripts' expected oauth path (some scripts look here)&#10;    try:&#10;        scripts_oauth_dir = SCRIPTS_DIR / &quot;player_stats&quot; / &quot;oauth&quot;&#10;        scripts_oauth_dir.mkdir(parents=True, exist_ok=True)&#10;        scripts_oauth_file = scripts_oauth_dir / &quot;Oauth.json&quot;&#10;        with open(scripts_oauth_file, 'w', encoding='utf-8') as sf:&#10;            json.dump(oauth_data, sf, indent=2)&#10;    except Exception:&#10;        # best-effort: don't fail saving oauth token if secondary write fails&#10;        pass&#10;&#10;    return oauth_file&#10;&#10;&#10;def run_initial_import():&#10;    &quot;&quot;&quot;Run the initial_import.py script to fetch all league data&quot;&quot;&quot;&#10;    if not INITIAL_IMPORT_SCRIPT.exists():&#10;        st.error(f&quot;❌ Initial import script not found at: {INITIAL_IMPORT_SCRIPT}&quot;)&#10;        return False&#10;&#10;    try:&#10;        st.info(&quot; Starting initial data import... This may take several minutes.&quot;)&#10;&#10;        # Create placeholders for progress&#10;        log_placeholder = st.empty()&#10;        status_placeholder = st.empty()&#10;&#10;        # Create a timestamped log file for the import subprocess&#10;        IMPORT_LOG_DIR = DATA_DIR / &quot;import_logs&quot;&#10;        IMPORT_LOG_DIR.mkdir(parents=True, exist_ok=True)&#10;        import_log_path = IMPORT_LOG_DIR / f&quot;initial_import_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.log&quot;&#10;&#10;        # Run the script&#10;        env = dict(os.environ)&#10;        env[&quot;PYTHONUNBUFFERED&quot;] = &quot;1&quot;&#10;&#10;        # Ensure MotherDuck token is present for subprocess if configured&#10;        if MOTHERDUCK_TOKEN:&#10;            env[&quot;MOTHERDUCK_TOKEN&quot;] = MOTHERDUCK_TOKEN&#10;&#10;        # Ensure non-interactive auto-confirm so initial_import doesn't prompt&#10;        env[&quot;AUTO_CONFIRM&quot;] = &quot;1&quot;&#10;&#10;        # Pass league info as environment variables for MotherDuck upload&#10;        if &quot;league_info&quot; in st.session_state:&#10;            league_info = st.session_state.league_info&#10;            env[&quot;LEAGUE_NAME&quot;] = league_info.get(&quot;name&quot;, &quot;Unknown League&quot;)&#10;            env[&quot;LEAGUE_KEY&quot;] = league_info.get(&quot;league_key&quot;, &quot;unknown&quot;)&#10;            env[&quot;LEAGUE_SEASON&quot;] = str(league_info.get(&quot;season&quot;, &quot;&quot;))&#10;            env[&quot;LEAGUE_NUM_TEAMS&quot;] = str(league_info.get(&quot;num_teams&quot;, &quot;&quot;))&#10;&#10;        cmd = [sys.executable, str(INITIAL_IMPORT_SCRIPT)]&#10;&#10;        with st.spinner(&quot;Importing league data...&quot;):&#10;            process = subprocess.Popen(&#10;                cmd,&#10;                env=env,&#10;                stdout=subprocess.PIPE,&#10;                stderr=subprocess.STDOUT,&#10;                text=True,&#10;                bufsize=1,&#10;                cwd=INITIAL_IMPORT_SCRIPT.parent&#10;            )&#10;&#10;            # Stream output to UI and write to the import log file&#10;            output_lines = []&#10;            try:&#10;                with open(import_log_path, 'a', encoding='utf-8') as lf:&#10;                    for line in process.stdout:&#10;                        stripped = line.rstrip('\n')&#10;                        output_lines.append(stripped)&#10;&#10;                        # write to log file&#10;                        try:&#10;                            lf.write(stripped + &quot;\n&quot;)&#10;                            lf.flush()&#10;                        except Exception:&#10;                            pass&#10;&#10;                        # Update a small status message with the latest line so users see immediate progress&#10;                        try:&#10;                            status_placeholder.info(stripped)&#10;                        except Exception:&#10;                            status_placeholder.text(stripped)&#10;&#10;                        # Show last 10 lines of output&#10;                        log_placeholder.code('\n'.join(output_lines[-10:]))&#10;            except Exception as e:&#10;                # If streaming iteration fails, capture remaining output via communicate&#10;                try:&#10;                    remaining, _ = process.communicate(timeout=1)&#10;                    if remaining:&#10;                        output_lines.extend(remaining.splitlines())&#10;                        with open(import_log_path, 'a', encoding='utf-8') as lf:&#10;                            lf.write(remaining)&#10;                except Exception:&#10;                    pass&#10;&#10;            process.wait()&#10;&#10;            # Clear the status placeholder after completion/failure&#10;            if process.returncode == 0:&#10;                status_placeholder.success(&quot;Import finished successfully.&quot;)&#10;                st.success(&quot;✅ Data import completed successfully!&quot;)&#10;                st.balloons()&#10;                st.write(f&quot;Import log: {import_log_path}&quot;)&#10;                return True&#10;            else:&#10;                status_placeholder.error(f&quot;Import failed (exit code {process.returncode}).&quot;)&#10;                st.error(f&quot;❌ Import failed with exit code {process.returncode}&quot;)&#10;                st.code('\n'.join(output_lines))&#10;                st.write(f&quot;Import log: {import_log_path}&quot;)&#10;                return False&#10;&#10;    except Exception as e:&#10;        st.error(f&quot;❌ Error running import: {e}&quot;)&#10;        return False&#10;&#10;&#10;def main():&#10;    st.title(&quot; Yahoo Fantasy Football League History&quot;)&#10;&#10;    # Check if credentials are loaded&#10;    if not CLIENT_ID or not CLIENT_SECRET:&#10;        st.error(&quot;❌ Credentials not configured!&quot;)&#10;        st.info(&quot;Set YAHOO_CLIENT_ID and YAHOO_CLIENT_SECRET environment variables or in Streamlit secrets.&quot;)&#10;        return&#10;&#10;    # Show whether MotherDuck token is configured (no interactive entry)&#10;    if MOTHERDUCK_TOKEN:&#10;        st.success(&quot;✅ MotherDuck token loaded from environment or `st.secrets`.&quot;)&#10;    else:&#10;        st.warning(&quot;⚠️ MotherDuck token not configured. Data will be saved locally only. To enable automatic upload, add `MOTHERDUCK_TOKEN` to `st.secrets` or environment variables.&quot;)&#10;&#10;    # Check for errors in URL&#10;    qp = st.query_params&#10;    if &quot;error&quot; in qp:&#10;        st.error(f&quot;❌ OAuth Error: {qp.get('error')}&quot;)&#10;        if &quot;error_description&quot; in qp:&#10;            st.error(f&quot;Description: {qp.get('error_description')}&quot;)&#10;        if st.button(&quot;Clear Error &amp; Retry&quot;):&#10;            st.query_params.clear()&#10;            st.rerun()&#10;        return&#10;&#10;    # Check if Yahoo redirected back with authorization code&#10;    if &quot;code&quot; in qp:&#10;        code = qp[&quot;code&quot;]&#10;&#10;        with st.spinner(&quot;Connecting to Yahoo...&quot;):&#10;            try:&#10;                token_data = exchange_code_for_tokens(code)&#10;&#10;                # Store token data in session state&#10;                st.session_state.token_data = {&#10;                    &quot;access_token&quot;: token_data.get(&quot;access_token&quot;),&#10;                    &quot;refresh_token&quot;: token_data.get(&quot;refresh_token&quot;),&#10;                    &quot;token_type&quot;: token_data.get(&quot;token_type&quot;),&#10;                    &quot;expires_in&quot;: token_data.get(&quot;expires_in&quot;),&#10;                    &quot;xoauth_yahoo_guid&quot;: token_data.get(&quot;xoauth_yahoo_guid&quot;)&#10;                }&#10;&#10;                st.session_state.access_token = token_data.get(&quot;access_token&quot;)&#10;                st.session_state.token_expiry = datetime.utcnow() + timedelta(seconds=token_data.get(&quot;expires_in&quot;, 3600))&#10;&#10;                st.success(&quot;✅ Successfully connected!&quot;)&#10;&#10;                # Clear the code from URL&#10;                st.query_params.clear()&#10;                st.rerun()&#10;&#10;            except Exception as e:&#10;                st.error(f&quot;❌ Error: {e}&quot;)&#10;&#10;        return&#10;&#10;    # Check if we have a stored access token&#10;    if &quot;access_token&quot; in st.session_state:&#10;        access_token = st.session_state.access_token&#10;&#10;        st.success(&quot; Connected to Yahoo Fantasy!&quot;)&#10;&#10;        # Fetch user's games if not already loaded&#10;        if &quot;games_data&quot; not in st.session_state:&#10;            with st.spinner(&quot;Loading your fantasy seasons...&quot;):&#10;                try:&#10;                    games_data = get_user_games(access_token)&#10;                    st.session_state.games_data = games_data&#10;                except Exception as e:&#10;                    st.error(f&quot;Error: {e}&quot;)&#10;                    if st.button(&quot;Start Over&quot;):&#10;                        st.session_state.clear()&#10;                        st.rerun()&#10;                    return&#10;&#10;        games_data = st.session_state.games_data&#10;        football_games = extract_football_games(games_data)&#10;&#10;        if not football_games:&#10;            st.warning(&quot;No football leagues found for your account.&quot;)&#10;            if st.button(&quot;Logout&quot;):&#10;                st.session_state.clear()&#10;                st.rerun()&#10;            return&#10;&#10;        st.subheader(&quot; Select Your League&quot;)&#10;&#10;        # Display football seasons&#10;        season_options = {f&quot;{game['season']} NFL Season&quot;: game['game_key']&#10;                         for game in football_games}&#10;&#10;        selected_season = st.selectbox(&#10;            &quot;1. Choose a season:&quot;,&#10;            options=list(season_options.keys())&#10;        )&#10;&#10;        if selected_season:&#10;            game_key = season_options[selected_season]&#10;&#10;            # Auto-load leagues for selected season&#10;            if &quot;current_game_key&quot; not in st.session_state or st.session_state.current_game_key != game_key:&#10;                with st.spinner(&quot;Loading leagues...&quot;):&#10;                    try:&#10;                        leagues_data = get_user_football_leagues(access_token, game_key)&#10;                        st.session_state.current_leagues = leagues_data&#10;                        st.session_state.current_game_key = game_key&#10;                    except Exception as e:&#10;                        st.error(f&quot;Error: {e}&quot;)&#10;&#10;            # Display leagues if loaded&#10;            if &quot;current_leagues&quot; in st.session_state:&#10;                leagues_data = st.session_state.current_leagues&#10;&#10;                try:&#10;                    leagues = leagues_data.get(&quot;fantasy_content&quot;, {}).get(&quot;users&quot;, {}).get(&quot;0&quot;, {}).get(&quot;user&quot;, [])[1].get(&quot;games&quot;, {}).get(&quot;0&quot;, {}).get(&quot;game&quot;, [])[1].get(&quot;leagues&quot;, {})&#10;&#10;                    league_list = []&#10;                    for key in leagues:&#10;                        if key == &quot;count&quot;:&#10;                            continue&#10;                        league = leagues[key].get(&quot;league&quot;, [])[0]&#10;                        league_list.append({&#10;                            &quot;league_key&quot;: league.get(&quot;league_key&quot;),&#10;                            &quot;name&quot;: league.get(&quot;name&quot;),&#10;                            &quot;num_teams&quot;: league.get(&quot;num_teams&quot;),&#10;                            &quot;season&quot;: league.get(&quot;season&quot;),&#10;                        })&#10;&#10;                    if league_list:&#10;                        st.write(&quot;2. Choose your league:&quot;)&#10;&#10;                        league_names = [f&quot;{league['name']} ({league['num_teams']} teams)&quot; for league in league_list]&#10;                        selected_league_name = st.radio(&quot;&quot;, league_names, key=&quot;league_radio&quot;)&#10;&#10;                        selected_league = league_list[league_names.index(selected_league_name)]&#10;&#10;                        st.divider()&#10;&#10;                        # Show league details&#10;                        st.write(&quot;3. Review league details:&quot;)&#10;                        col1, col2, col3 = st.columns(3)&#10;                        with col1:&#10;                            st.metric(&quot;League&quot;, selected_league['name'])&#10;                        with col2:&#10;                            st.metric(&quot;Season&quot;, selected_league['season'])&#10;                        with col3:&#10;                            st.metric(&quot;Teams&quot;, selected_league['num_teams'])&#10;&#10;                        st.info(f&quot; `Data to import:` All historical data for `{selected_league['name']}` (league key: {selected_league['league_key']})&quot;)&#10;&#10;                        st.divider()&#10;&#10;                        st.write(&quot;4. Import your league data:&quot;)&#10;                        st.info(&quot;This will fetch all historical data from your league and save it locally (and upload to MotherDuck if configured).&quot;)&#10;&#10;                        # Show MotherDuck configuration state (read-only)&#10;                        with st.expander(&quot; MotherDuck Configuration (Read-only)&quot;):&#10;                            if MOTHERDUCK_TOKEN:&#10;                                st.success(&quot;✅ MotherDuck token is loaded from environment or `st.secrets`.&quot;)&#10;                                sanitized_db = selected_league['name'].lower().replace(' ', '_')&#10;                                st.info(f&quot;Database will be created as: `{sanitized_db}`&quot;)&#10;                            else:&#10;                                st.warning(&quot;MotherDuck token not configured. Upload will be skipped; files will be saved locally.&quot;)&#10;&#10;                        if st.button(&quot; Import League Data Now&quot;, type=&quot;primary&quot;):&#10;                            # Store league info in session state for environment variables&#10;                            st.session_state.league_info = selected_league&#10;&#10;                            with st.spinner(&quot;Saving OAuth credentials...&quot;):&#10;                                # Save OAuth token with league info&#10;                                oauth_file = save_oauth_token(&#10;                                    st.session_state.token_data,&#10;                                    selected_league&#10;                                )&#10;                                st.success(f&quot;✅ OAuth credentials saved to: {oauth_file}&quot;)&#10;&#10;                            # Ensure subprocess sees MOTHERDUCK_TOKEN if configured&#10;                            if MOTHERDUCK_TOKEN:&#10;                                os.environ[&quot;MOTHERDUCK_TOKEN&quot;] = MOTHERDUCK_TOKEN&#10;&#10;                            # Run initial import&#10;                            if run_initial_import():&#10;                                st.success(&quot; All done! Your league data has been imported.&quot;)&#10;&#10;                                # Show file locations&#10;                                st.write(&quot;###  Files Saved:&quot;)&#10;                                st.write(f&quot;**OAuth Token:** `{OAUTH_DIR / 'Oauth.json'}`&quot;)&#10;                                st.write(f&quot;**League Data:** `{DATA_DIR}/`&quot;)&#10;&#10;                                # Show what was created&#10;                                if DATA_DIR.exists():&#10;                                    st.write(&quot;#### Data Files Created:&quot;)&#10;                                    parquet_files = list(DATA_DIR.glob(&quot;*.parquet&quot;))&#10;                                    if parquet_files:&#10;                                        for pf in sorted(parquet_files):&#10;                                            size = pf.stat().st_size / 1024  # KB&#10;                                            st.write(f&quot;- `{pf.name}` ({size:.1f} KB)&quot;)&#10;&#10;                                    st.divider()&#10;&#10;                                    # Download options&#10;                                    st.write(&quot;####  Download Your Data:&quot;)&#10;                                    st.info(&quot;⚠️ `Important:` On Streamlit Cloud, these files are temporary. Download them now to save locally!&quot;)&#10;&#10;                                    # Download OAuth token&#10;                                    oauth_file_path = OAUTH_DIR / &quot;Oauth.json&quot;&#10;                                    if oauth_file_path.exists():&#10;                                        with open(oauth_file_path, 'r') as f:&#10;                                            oauth_json = f.read()&#10;                                        st.download_button(&#10;                                            &quot; Download OAuth Token (Oauth.json)&quot;,&#10;                                            oauth_json,&#10;                                            file_name=&quot;Oauth.json&quot;,&#10;                                            mime=&quot;application/json&quot;,&#10;                                            help=&quot;Save this file to your oauth/ folder to use with your scripts&quot;&#10;                                        )&#10;&#10;                                    # Download individual parquet files&#10;                                    if parquet_files:&#10;                                        st.write(&quot;**Download Data Files:**&quot;)&#10;                                        for pf in sorted(parquet_files):&#10;                                            with open(pf, 'rb') as f:&#10;                                                st.download_button(&#10;                                                    f&quot; {pf.name}&quot;,&#10;                                                    f.read(),&#10;                                                    file_name=pf.name,&#10;                                                    mime=&quot;application/octet-stream&quot;,&#10;                                                    help=f&quot;Download {pf.name} to your fantasy_football_data/ folder&quot;&#10;                                                )&#10;&#10;                                    # Download all files as ZIP&#10;                                    with st.spinner(&quot;Creating ZIP archive of your data...&quot;):&#10;                                        zip_buffer = io.BytesIO()&#10;                                        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:&#10;                                            # Add OAuth file&#10;                                            if oauth_file_path.exists():&#10;                                                zip_file.write(oauth_file_path, arcname=&quot;Oauth.json&quot;)&#10;&#10;                                            # Add all parquet files&#10;                                            for pf in sorted(parquet_files):&#10;                                                zip_file.write(pf, arcname=pf.name)&#10;&#10;                                        zip_buffer.seek(0)&#10;&#10;                                        st.download_button(&#10;                                            &quot; Download All Files as ZIP&quot;,&#10;                                            zip_buffer,&#10;                                            file_name=&quot;fantasy_football_data.zip&quot;,&#10;                                            mime=&quot;application/zip&quot;,&#10;                                            help=&quot;Download all data files as a single ZIP archive&quot;&#10;                                        )&#10;&#10;                                    st.success(&quot;✅ All files ready for download above!&quot;)&#10;&#10;                    else:&#10;                        st.info(&quot;No leagues found for this season.&quot;)&#10;&#10;                except Exception as e:&#10;                    st.error(f&quot;Error parsing leagues: {e}&quot;)&#10;&#10;        st.divider()&#10;&#10;        if st.button(&quot;Start Over&quot;):&#10;            st.session_state.clear()&#10;            st.rerun()&#10;&#10;    else:&#10;        # Show the authorization button&#10;        st.write(&quot;### Import Your Fantasy Football League Data&quot;)&#10;&#10;        st.write(&quot; This tool will fetch and save your complete league history including:&quot;)&#10;        st.write(&quot;- All-time schedules and matchups&quot;)&#10;        st.write(&quot;- Player statistics&quot;)&#10;        st.write(&quot;- Transaction history&quot;)&#10;        st.write(&quot;- Draft data&quot;)&#10;        st.write(&quot;- Playoff information&quot;)&#10;&#10;        st.divider()&#10;&#10;        st.write(&quot;**How it works:**&quot;)&#10;        st.write(&quot;1. Connect your Yahoo account&quot;)&#10;        st.write(&quot;2. Select your league&quot;)&#10;        st.write(&quot;3. Data is automatically imported and saved locally&quot;)&#10;        st.write(&quot;4. Use the data with your analysis scripts&quot;)&#10;&#10;        st.warning(&quot;⚠️ We only access your league data to build local files. Your Yahoo credentials are stored securely in the `oauth/` folder.&quot;)&#10;&#10;        auth_url = build_authorize_url()&#10;&#10;        st.link_button(&quot; Connect Yahoo Account&quot;, auth_url, type=&quot;primary&quot;)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;main.py (NEUTRALIZED)&#13;&#10;&#13;&#10;This file was intentionally replaced with a small README-style stub to avoid confusion.&#13;&#10;The original Streamlit application that provided the interactive OAuth + import UI&#13;&#10;has been removed from this file to keep the repository tidy. If you need to&#13;&#10;restore the original behavior, recover the file from version control or a&#13;&#10;backup copy.&#13;&#10;&#13;&#10;Why neutralize?&#13;&#10;- This prevents accidental execution in environments that aren't configured for&#13;&#10;  the app (missing secrets, Streamlit, network access, etc.).&#13;&#10;&#13;&#10;Behavior of this stub:&#13;&#10;- When executed, it prints a short informational message and exits with code 0.&#13;&#10;&quot;&quot;&quot;&#13;&#10;&#13;&#10;from __future__ import annotations&#13;&#10;&#13;&#10;import sys&#13;&#10;&#13;&#10;STUB_MESSAGE = (&#13;&#10;    &quot;main.py has been neutralized.\n&quot;&#13;&#10;    &quot;This file was replaced with a stub to avoid accidental runs.\n&quot;&#13;&#10;    &quot;Restore the original file from version control to re-enable the Streamlit app.&quot;&#13;&#10;)&#13;&#10;&#13;&#10;&#13;&#10;def info() -&gt; None:&#13;&#10;    &quot;&quot;&quot;Print a brief informational message.&quot;&quot;&quot;&#13;&#10;    print(STUB_MESSAGE)&#13;&#10;&#13;&#10;&#13;&#10;if __name__ == &quot;__main__&quot;:&#13;&#10;    info()&#13;&#10;    sys.exit(0)&#13;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>