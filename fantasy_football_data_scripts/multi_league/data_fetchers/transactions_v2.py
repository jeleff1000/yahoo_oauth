#!/usr/bin/env python3
"""
Transaction Data Fetcher V2 - Multi-League Edition

Fetches Yahoo Fantasy Football transaction data for any league using LeagueContext.
Compatible with the multi-league infrastructure.

Key improvements over V1:
- Multi-league support via LeagueContext
- RunLogger integration for structured logging
- Extracts yahoo_player_id for reliable joins
- No player name cleaning (preserves original Yahoo names)
- Backward compatible with old config.py system
- Cleaner API with context-based configuration
- Better error handling and retry logic
- Per-year file output (transactions_year_YYYY.parquet)
- Automatic caching for completed years
- Fixed deduplication (preserves multi-player trades)

Data Dictionary Compliance:
- Primary keys: (transaction_id, yahoo_player_id) composite
- Foreign keys: yahoo_player_id → player.yahoo_player_id, manager → matchup.manager
- Join keys: (yahoo_player_id, year, week) for player stats enrichment

Output Files:
- transactions_year_2024.parquet (one file per year)
- transactions_year_2024.csv
- cache/transactions_year_2024.parquet (cached for faster re-runs)

Usage:
    # With LeagueContext - single year
    from multi_league.core.league_context import LeagueContext
    ctx = LeagueContext.load("leagues/kmffl/league_context.json")
    df = fetch_transactions(ctx, year=2024)  # Saves to transactions_year_2024.parquet

    # All years (creates one file per year)
    results = fetch_all_transaction_years(ctx)  # Returns dict: {year: DataFrame}

    # CLI with context - single year
    python transactions_v2.py --context leagues/kmffl/league_context.json --year 2024

    # CLI all years (creates separate file for each year)
    python transactions_v2.py --context leagues/kmffl/league_context.json --all-years
"""

import sys
import argparse
import re
import time
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass
from datetime import datetime
from xml.etree import ElementTree as ET
from io import StringIO

import pandas as pd
import requests
import yahoo_fantasy_api as yfa

# Add paths for imports
_script_file = Path(__file__).resolve()
_multi_league_dir = _script_file.parent.parent  # multi_league directory
sys.path.insert(0, str(_multi_league_dir / "core"))
sys.path.insert(0, str(_multi_league_dir / "utils"))

# Multi-league infrastructure
try:
    from league_context import LeagueContext
    LEAGUE_CONTEXT_AVAILABLE = True
except ImportError:
    LeagueContext = None
    LEAGUE_CONTEXT_AVAILABLE = False

try:
    from run_metadata import RunLogger
    RUN_LOGGER_AVAILABLE = True
except ImportError:
    RunLogger = None
    RUN_LOGGER_AVAILABLE = False

# Default paths (for standalone mode)
THIS_FILE = Path(__file__).resolve()
SCRIPT_ROOT = THIS_FILE.parent.parent.parent  # Back to scripts root
DEFAULT_DATA_ROOT = SCRIPT_ROOT.parent / "fantasy_football_data" / "transaction_data"

# -----------------------------------------------------------------------------
# Matchup window loader (reuse the exact windows the matchup job writes)
# -----------------------------------------------------------------------------
def load_matchup_windows(ctx, year: int) -> pd.DataFrame:
    """
    Year-scoped window loader with independent Yahoo fallback.
    Order of attempts:
      1) year-specific cache: matchup_windows_{year}.parquet
      2) build from Yahoo API (scoreboard) -> then cache
      3) (optional) read matchup_data_week_all_year_{year}.parquet if present
    """
    mdir = ctx.matchup_data_directory
    cache = mdir / f"matchup_windows_{year}.parquet"  # year-scoped!

    # 1) Use year-specific cache if valid
    if cache.exists():
        try:
            df = pd.read_parquet(cache)
            cols = ["year","week","week_start","week_end","cumulative_week"]
            df = df[[c for c in cols if c in df.columns]].drop_duplicates()
            if not df.empty:
                return df
        except Exception:
            pass

    # 2) Try building directly from Yahoo (independent of matchup job)
    try:
        win = build_week_windows_from_yahoo(ctx, year)
        if not win.empty:
            try:
                cache.parent.mkdir(parents=True, exist_ok=True)
                win.to_parquet(cache, index=False)
            except Exception:
                pass
            return win
    except Exception:
        pass

    # 3) Last resort: borrow from the matchup “all weeks” artifact if it exists
    all_path = mdir / f"matchup_data_week_all_year_{year}.parquet"
    if all_path.exists():
        try:
            mdf = pd.read_parquet(all_path)
            win = (
                mdf[["year","week","week_start","week_end"]]
                .dropna(subset=["year","week"])
                .drop_duplicates()
                .sort_values(["year","week"])
                .copy()
            )
            win["year"] = pd.to_numeric(win["year"], errors="coerce").astype("Int64")
            win["week"] = pd.to_numeric(win["week"], errors="coerce").astype("Int64")
            win["cumulative_week"] = (
                win["year"].astype("Int64").astype(str) +
                win["week"].astype("Int64").astype(str).str.zfill(2)
            )
            with pd.option_context("mode.use_inf_as_na", True):
                win["cumulative_week"] = pd.to_numeric(win["cumulative_week"], errors="coerce").astype("Int64")
            # Save to the year cache for next time
            try:
                win.to_parquet(cache, index=False)
            except Exception:
                pass
            return win
        except Exception:
            pass

    # If everything failed, return an empty frame with the right schema
    return pd.DataFrame(columns=["year","week","week_start","week_end","cumulative_week"])

def build_week_windows_from_yahoo(ctx, year: int) -> pd.DataFrame:
    """
    Build (year, week, week_start, week_end, cumulative_week=YYYYWW) directly
    from Yahoo's scoreboard API. No dependency on matchup outputs.
    """
    oauth = ctx.get_oauth_session()
    gm = yfa.Game(oauth, ctx.game_code)

    # CRITICAL: Use specific league_id from context to avoid data mixing
    league_key = None
    if hasattr(ctx, 'get_league_id_for_year'):
        league_key = ctx.get_league_id_for_year(year)
        if league_key:
            print(f"[transactions] Using league_id from context for {year}: {league_key}")

    # Fallback to API discovery (may mix leagues!)
    if not league_key:
        league_ids = gm.league_ids(year=year)
        if not league_ids:
            return pd.DataFrame(columns=["year","week","week_start","week_end","cumulative_week"])
        if len(league_ids) > 1:
            print(f"[transactions] WARNING: Multiple leagues found for {year}: {league_ids}")
            print(f"[transactions] WARNING: Using last one - this may cause data mixing!")
        league_key = league_ids[-1]

    league = gm.to_league(league_key)

    # Figure out min/max weeks from settings; fall back to 1..17
    try:
        settings = league.settings() or {}
        start_week = int(settings.get("start_week") or 1)
        end_week   = int(settings.get("end_week")   or 17)
    except Exception:
        start_week, end_week = 1, 17

    rows = []
    for w in range(start_week, end_week + 1):
        url = f"https://fantasysports.yahooapis.com/fantasy/v2/league/{league_key}/scoreboard;week={w}"
        try:
            root = fetch_url(url, oauth)
        except Exception:
            continue
        wstarts, wends = [], []
        for m in root.findall(".//matchup"):
            ws = (m.findtext("week_start") or "").strip()
            we = (m.findtext("week_end") or "").strip()
            if ws: wstarts.append(ws)
            if we: wends.append(we)
        week_start = min(wstarts) if wstarts else None
        week_end   = max(wends)   if wends   else None
        rows.append({
            "year": int(year),
            "week": int(w),
            "week_start": week_start,
            "week_end": week_end,
        })
    win = pd.DataFrame(rows)
    if win.empty:
        return pd.DataFrame(columns=["year","week","week_start","week_end","cumulative_week"])
    win["year"] = pd.to_numeric(win["year"], errors="coerce").astype("Int64")
    win["week"] = pd.to_numeric(win["week"], errors="coerce").astype("Int64")
    win["cumulative_week"] = (
        win["year"].astype("Int64").astype(str) +
        win["week"].astype("Int64").astype(str).str.zfill(2)
    )
    with pd.option_context("mode.use_inf_as_na", True):
        win["cumulative_week"] = pd.to_numeric(win["cumulative_week"], errors="coerce").astype("Int64")
    return (
        win.dropna(subset=["year","week"])
           .drop_duplicates(subset=["year","week"])
           .sort_values(["year","week"])
           .reset_index(drop=True)
    )


# =============================================================================
# Data Models
# =============================================================================

@dataclass
class Transaction:
    """Represents a single transaction."""
    transaction_id: str
    year: int
    week: int
    timestamp: str
    status: str
    transaction_type: str
    manager: str
    player_name: str
    yahoo_player_id: Optional[str]
    player_key: str
    faab_bid: int
    source_type: Optional[str]
    destination: Optional[str]
    week_start: Optional[pd.Timestamp]
    week_end: Optional[pd.Timestamp]
    cumulative_week: Optional[int]


# =============================================================================
# API Retry Logic
# =============================================================================

class APITimeoutError(Exception):
    """Recoverable API timeout."""
    pass


class RecoverableAPIError(APITimeoutError):
    """Transient API failures (rate-limit, 403/429, 'Request denied', empty XML, etc.)."""
    pass


_XML_NS_RE = re.compile(r' xmlns="[^"]+"')


def fetch_url(url: str, oauth, max_retries: int = 6, backoff: float = 0.5) -> ET.Element:
    """
    Fetch URL with retry logic and exponential backoff.

    Args:
        url: URL to fetch
        oauth: OAuth2 session
        max_retries: Maximum retry attempts
        backoff: Initial backoff time in seconds

    Returns:
        ET.Element: Parsed XML root element

    Raises:
        APITimeoutError: If timeout occurs after retries
        RecoverableAPIError: If recoverable error occurs after retries
    """
    last_exc = None
    for attempt in range(max_retries):
        try:
            r = oauth.session.get(url, timeout=30)
            try:
                r.raise_for_status()
            except requests.HTTPError as he:
                code = getattr(he.response, "status_code", None)
                if code in (429, 403, 502, 503, 504):
                    if attempt == max_retries - 1:
                        raise RecoverableAPIError(f"HTTP {code} on {url}") from he
                    time.sleep(backoff * (2 ** attempt))
                    continue
                raise

            text = (r.text or "").strip()
            if not text or "Request denied" in text:
                if attempt == max_retries - 1:
                    raise RecoverableAPIError(f"Empty or denied response from {url}")
                time.sleep(backoff * (2 ** attempt))
                continue

            xmlstring = _XML_NS_RE.sub("", text, count=1)
            return ET.fromstring(xmlstring)

        except requests.exceptions.Timeout as e:
            last_exc = e
            if attempt == max_retries - 1:
                raise APITimeoutError(f"Timeout fetching {url}") from e
            time.sleep(backoff * (2 ** attempt))

        except (requests.RequestException, ET.ParseError, ValueError) as e:
            last_exc = e
            if attempt == max_retries - 1:
                raise RecoverableAPIError(f"API error on {url}: {e}") from e
            time.sleep(backoff * (2 ** attempt))

    raise RecoverableAPIError(f"Failed after {max_retries} retries") from last_exc


# =============================================================================
# Utility Functions
# =============================================================================

def extract_yahoo_player_id(player_key: str) -> Optional[str]:
    """
    Extract yahoo_player_id from player_key.
    Player keys are in format: <game_key>.p.<player_id>
    Example: 461.p.33376 -> 33376
    """
    if not player_key:
        return None
    parts = player_key.split('.')
    if len(parts) >= 3 and parts[-2] == 'p':
        return parts[-1]
    return None


def convert_timestamp(ts: str) -> str:
    """Convert Yahoo timestamp (seconds since epoch) to human readable format."""
    try:
        dt = datetime.fromtimestamp(int(ts))
        return dt.strftime('%b %d %Y %I:%M:%S %p').upper()
    except (ValueError, TypeError):
        return "UNKNOWN"


def map_transaction_to_week(
    ts: str,
    year: int,
    matchup_windows: pd.DataFrame
) -> Tuple[int, Optional[str], Optional[str]]:
    """
    Map a Yahoo transaction timestamp to (week, week_start, week_end) using the
    exact windows emitted by weekly_matchup_data_v2.
    Returns string week_start/week_end to match the matchup file.
    """
    rows = matchup_windows[matchup_windows['year'] == year].sort_values('week')
    if rows.empty:
        return 1, None, None

    # Parse the transaction timestamp
    try:
        t_pd = pd.to_datetime(datetime.fromtimestamp(int(ts)))
    except (ValueError, TypeError, OverflowError):
        # Unknown timestamp → default to the first week
        r0 = rows.iloc[0]
        return int(r0['week']), r0.get('week_start'), r0.get('week_end')

    # Filter to only rows with valid week_start and week_end dates
    # This prevents using incomplete/future weeks from matchup windows
    valid_rows = rows[rows['week_start'].notna() & rows['week_end'].notna()].copy()

    if valid_rows.empty:
        # No valid windows at all - default to week 1
        return 1, None, None

    # Prepare typed windows for comparison, but preserve original strings for return
    comp = valid_rows.copy()
    comp['_ws'] = pd.to_datetime(comp['week_start'], errors='coerce')
    comp['_we'] = pd.to_datetime(comp['week_end'], errors='coerce')

    # Add one day to week_end to include the entire end date (not just midnight)
    # This ensures transactions on the last day of the week are included
    comp['_we_inclusive'] = comp['_we'] + pd.Timedelta(days=1)

    # Remove any rows where date parsing failed
    comp = comp[comp['_ws'].notna() & comp['_we'].notna()]

    if comp.empty:
        return 1, None, None

    # Before the first window → week 1
    first = comp.iloc[0]
    if pd.notna(first['_ws']) and t_pd < first['_ws']:
        return int(first['week']), first.get('week_start'), first.get('week_end')

    # Inside any [week_start, week_end] (inclusive of entire end day)
    # Use < with _we_inclusive (next day) instead of <= with _we (midnight)
    m = (comp['_ws'].notna()) & (comp['_we'].notna()) & (t_pd >= comp['_ws']) & (t_pd < comp['_we_inclusive'])
    if m.any():
        r = comp[m].iloc[0]
        return int(r['week']), r.get('week_start'), r.get('week_end')

    # After final valid window → estimate the week based on date
    last = comp.iloc[-1]
    last_week_num = int(last['week'])
    last_week_end_inclusive = last['_we_inclusive']

    # Calculate days since last known week ended (using inclusive end)
    days_since_last = (t_pd - last_week_end_inclusive).days

    # Estimate week (assuming 7 days per week)
    # Add 1 because if transaction is 1-7 days after, it's the next week
    estimated_week = last_week_num + max(1, (days_since_last // 7) + 1)

    # Cap at reasonable maximum (18 weeks for NFL season including playoffs)
    estimated_week = min(estimated_week, 18)

    # Return estimated week with no date boundaries (since they don't exist yet)
    return estimated_week, None, None


# =============================================================================
# Transaction Fetching
# =============================================================================

def fetch_team_mappings(oauth, league_key: str) -> Dict[str, str]:
    """
    Fetch team ID to manager name mappings.

    Args:
        oauth: OAuth2 session
        league_key: Yahoo league key (e.g., "nfl.l.123456")

    Returns:
        Dict mapping team_key to manager nickname
    """
    print(f"  Fetching team mappings for {league_key}")

    teams_url = f"https://fantasysports.yahooapis.com/fantasy/v2/league/{league_key}/teams"

    try:
        root = fetch_url(teams_url, oauth)
        mgr_df = pd.read_xml(StringIO(ET.tostring(root, encoding='unicode')), xpath=".//manager", parser="etree")
        mgr_df["team"] = f"{league_key}.t." + mgr_df["manager_id"].astype(str)
        mgr_df = mgr_df[["team", "nickname"]]
        team_id_to_manager = {row["team"]: row["nickname"] for _, row in mgr_df.iterrows()}
        print(f"  Loaded {len(team_id_to_manager)} team mappings")
        return team_id_to_manager
    except Exception as e:
        print(f"  ERROR: Failed to fetch team mappings: {e}")
        return {}


def fetch_transactions_for_year(
    oauth,
    league_key: str,
    year: int,
    team_mappings: Dict[str, str],
    matchup_windows: pd.DataFrame
) -> List[Transaction]:
    """
    Fetch all transactions for a specific year.

    Args:
        oauth: OAuth2 session
        league_key: Yahoo league key
        year: Season year
        team_mappings: Team key to manager name mapping
        matchup_windows: DataFrame with week windows for cumulative_week calculation

    Returns:
        List of Transaction objects
    """
    print(f"  Fetching transactions for {year} (league: {league_key})")

    all_transactions = []
    start = 0
    count = 50  # Yahoo's max per request

    while True:
        url = f"https://fantasysports.yahooapis.com/fantasy/v2/league/{league_key}/transactions;start={start};count={count}"
        print(f"    API Call: start={start}, count={count}")

        try:
            root = fetch_url(url, oauth)
            tx_elements = root.findall(".//transaction")

            if not tx_elements:
                break

            print(f"    Retrieved {len(tx_elements)} transactions")

            # Parse transactions
            for tr in tx_elements:
                tx_type = (tr.findtext("type") or "").strip()
                status = (tr.findtext("status") or "").strip()
                ts = tr.findtext("timestamp") or "0"

                # Resolve week via matchup windows (exactly how matchup file defines them)
                week_val, week_start, week_end = map_transaction_to_week(ts, year, matchup_windows)

                # Team roles (mapped to manager names)
                def map_team(tag: str) -> str:
                    node = tr.find(tag)
                    if node is None or node.text is None:
                        return "Unknown"
                    return team_mappings.get(node.text.strip(), "Unknown")

                team_keys = {
                    "destination_team_key": map_team("destination_team_key"),
                    "source_team_key": map_team("source_team_key"),
                    "waiver_team_key": map_team("waiver_team_key"),
                    "trader_team_key": map_team("trader_team_key"),
                    "tradee_team_key": map_team("tradee_team_key"),
                }

                faab_bid = (tr.findtext("faab_bid") or "0").strip()
                transaction_key = (tr.findtext("transaction_key") or "").strip()

                # Parse players in transaction
                for player in tr.findall("players/player"):
                    player_key = (player.findtext("player_key") or "").strip()
                    name = player.findtext("name/full") or ""
                    # NO NAME CLEANING - preserve original Yahoo names
                    yahoo_player_id = extract_yahoo_player_id(player_key)

                    ptype = (player.findtext("transaction_data/type") or "").strip()
                    source_type = (player.findtext("transaction_data/source_type") or "").strip()
                    dest_el = player.find("transaction_data/destination_type")
                    destination = (dest_el.text if dest_el is not None else "Unknown") or "Unknown"

                    if destination == "team":
                        tkey = player.findtext("transaction_data/destination_team_key") or ""
                    elif destination == "waivers":
                        tkey = player.findtext("transaction_data/source_team_key") or ""
                    else:
                        tkey = ""

                    nickname = team_mappings.get(tkey, "Unknown") if tkey else "Unknown"

                    # Derive cumulative_week.  Prefer to look up from
                    # matchup_windows (which contains week-start/end
                    # boundaries for cross-season alignment).  If a record
                    # isn't found or the value is missing, fallback to
                    # constructing it directly from year and week (zero-padded).
                    cumulative_week = None
                    if not matchup_windows.empty:
                        cw_match = matchup_windows[
                            (matchup_windows['year'] == year) &
                            (matchup_windows['week'] == week_val)
                        ]
                        if len(cw_match) > 0:
                            cw_value = cw_match.iloc[0]["cumulative_week"]
                            if pd.notna(cw_value):
                                try:
                                    cumulative_week = int(cw_value)
                                except Exception:
                                    cumulative_week = None
                    # Fallback: build YYYYWW if windows missing
                    if cumulative_week is None:
                        if pd.notna(week_val) and pd.notna(year):
                            try:
                                wk_int = int(week_val)
                                cumulative_week = int(f"{int(year)}{wk_int:02d}")  # e.g., 2024 + 01 → 202401
                            except Exception:
                                cumulative_week = None

                    transaction = Transaction(
                        transaction_id=transaction_key,
                        year=year,
                        week=week_val,
                        timestamp=ts,
                        status=status,
                        transaction_type=ptype,
                        manager=nickname,
                        player_name=name,
                        yahoo_player_id=yahoo_player_id,
                        player_key=player_key,
                        faab_bid=int(faab_bid) if faab_bid.isdigit() else 0,
                        source_type=source_type,
                        destination=destination,
                        # preserve string week_start/week_end to match matchup file
                        week_start=week_start if pd.notna(week_start) else None,
                        week_end=week_end if pd.notna(week_end) else None,
                        cumulative_week=cumulative_week
                    )

                    all_transactions.append(transaction)

            if len(tx_elements) < count:
                break

            start += count

        except (APITimeoutError, RecoverableAPIError) as e:
            print(f"    ERROR: {e}")
            break
        except Exception as e:
            print(f"    ERROR: Unexpected error: {e}")
            break

    print(f"  Fetched {len(all_transactions)} transaction records for {year}")
    return all_transactions


def transactions_to_dataframe(transactions: List[Transaction]) -> pd.DataFrame:
    """
    Convert list of Transaction objects to DataFrame.

    Args:
        transactions: List of Transaction objects

    Returns:
        DataFrame with proper column order per data dictionary
    """
    if not transactions:
        return pd.DataFrame()

    rows = []
    for tx in transactions:
        human_ts = convert_timestamp(tx.timestamp)

        rows.append({
            # Primary key
            "transaction_id": tx.transaction_id,

            # Foreign keys (for joins)
            "yahoo_player_id": tx.yahoo_player_id,
            "manager": tx.manager,

            # Time columns
            "year": tx.year,
            "week": tx.week,
            "cumulative_week": tx.cumulative_week,
            "week_start": tx.week_start,
            "week_end": tx.week_end,

            # Player info
            "player_name": tx.player_name,
            "player_key": tx.player_key,

            # Transaction details
            "transaction_type": tx.transaction_type,
            "source_type": tx.source_type,
            "destination": tx.destination,
            "status": tx.status,
            "faab_bid": tx.faab_bid,

            # Timestamps
            "timestamp": tx.timestamp,
            "human_readable_timestamp": human_ts,
        })

    df = pd.DataFrame(rows)

    # After df = transactions_to_dataframe(transactions)
    if 'cumulative_week' in df.columns:
        # Cast to Int64, keep NA if any
        df['cumulative_week'] = pd.to_numeric(df['cumulative_week'], errors='coerce').astype('Int64')

    # Create composite keys (per data dictionary)
    df['manager_week'] = df.apply(
        lambda row: f"{str(row['manager']).replace(' ', '')}{int(row['cumulative_week'])}"
        if pd.notna(row['cumulative_week']) else "", axis=1
    )
    df['manager_year'] = df.apply(
        lambda row: f"{str(row['manager']).replace(' ', '')}{int(row['year'])}", axis=1
    )

    # Create player_week and player_year to match player table format
    # player_week: player_name+year+week (e.g., "LamarJackson202104")
    # player_year: player_name+year (to match player table - uses player_name for compatibility with draft)
    df['player_week'] = df.apply(
        lambda row: f"{str(row['player_name']).replace(' ', '')}{int(row['year'])}{int(row['week']):02d}"
        if pd.notna(row['player_name']) and pd.notna(row['year']) and pd.notna(row['week']) else "", axis=1
    )
    df['player_year'] = df.apply(
        lambda row: f"{str(row['player_name']).replace(' ', '')}{int(row['year'])}"
        if pd.notna(row['player_name']) and pd.notna(row['year']) else "", axis=1
    )

    # Final column order (per data dictionary)
    column_order = [
        # Primary key
        "transaction_id",

        # Foreign keys
        "manager", "yahoo_player_id", "player_name",

        # Time columns
        "year", "week", "cumulative_week", "week_start", "week_end",

        # Transaction details
        "transaction_type", "source_type", "destination", "status", "faab_bid",

        # Metadata
        "player_key", "timestamp", "human_readable_timestamp",

        # Composite keys
        "manager_week", "manager_year", "player_week", "player_year",
    ]

    # Ensure all columns exist
    for col in column_order:
        if col not in df.columns:
            df[col] = pd.NA

    return df[column_order]


# =============================================================================
# Transaction Processing
# =============================================================================

def process_transactions_chronologically(transactions_df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure transactions are processed in chronological order.

    This is critical for roster state accuracy - transactions must be applied
    in the exact order they occurred to correctly track player movements.

    Args:
        transactions_df: DataFrame with transaction data

    Returns:
        DataFrame sorted chronologically with transaction_sequence added
    """
    if transactions_df.empty:
        return transactions_df

    # Parse transaction dates properly
    # First, try to use timestamp (Yahoo's epoch timestamp)
    if 'timestamp' in transactions_df.columns:
        transactions_df['transaction_datetime'] = pd.to_datetime(
            pd.to_numeric(transactions_df['timestamp'], errors='coerce'),
            unit='s',
            errors='coerce'
        )
    elif 'transaction_date' in transactions_df.columns:
        # Fallback to transaction_date if available
        transactions_df['transaction_datetime'] = pd.to_datetime(
            transactions_df['transaction_date'],
            errors='coerce'
        )
    else:
        # Last resort: create from year/week
        # Use week_start if available, otherwise construct from year/week
        if 'week_start' in transactions_df.columns:
            transactions_df['transaction_datetime'] = pd.to_datetime(
                transactions_df['week_start'],
                errors='coerce'
            )
        else:
            # Construct from year/week (assuming week 1 = first Monday)
            transactions_df['transaction_datetime'] = pd.to_datetime(
                transactions_df['year'].astype(str) + '-W' +
                transactions_df['week'].astype(str).str.zfill(2) + '-1',
                format='%Y-W%W-%w',
                errors='coerce'
            )

    # Sort chronologically by:
    # 1. Year (earliest to latest)
    # 2. Week (earliest to latest)
    # 3. Actual datetime (earliest to latest)
    # 4. Transaction ID (for stability)
    sort_cols = ['year', 'week', 'transaction_datetime']
    if 'transaction_id' in transactions_df.columns:
        sort_cols.append('transaction_id')

    transactions_df = transactions_df.sort_values(
        sort_cols,
        na_position='last'
    ).reset_index(drop=True)

    # Add sequence number for same-day transactions per manager
    # This helps track the order of multiple transactions on the same day
    if 'manager' in transactions_df.columns:
        transactions_df['transaction_sequence'] = transactions_df.groupby(
            ['year', 'week', 'manager'],
            dropna=False
        ).cumcount()
    else:
        transactions_df['transaction_sequence'] = transactions_df.groupby(
            ['year', 'week'],
            dropna=False
        ).cumcount()

    print(f"  Sorted {len(transactions_df)} transactions chronologically")

    # Report on date coverage
    if 'transaction_datetime' in transactions_df.columns:
        valid_dates = transactions_df['transaction_datetime'].notna()
        if valid_dates.any():
            min_date = transactions_df.loc[valid_dates, 'transaction_datetime'].min()
            max_date = transactions_df.loc[valid_dates, 'transaction_datetime'].max()
            print(f"  Date range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}")

    return transactions_df


# =============================================================================
# Main Fetcher Functions
# =============================================================================

def fetch_transactions(
    ctx: LeagueContext,
    year: int,
    matchup_windows: Optional[pd.DataFrame] = None,
    save_output: bool = True
) -> pd.DataFrame:
    """
    Fetch transaction data for a specific year and optionally save to file.

    Args:
        ctx: LeagueContext with league configuration
        year: Season year to fetch
        matchup_windows: Optional matchup windows for week calculation
        save_output: If True, save to year-specific files (default: True)

    Returns:
        DataFrame with transaction data
    """
    print(f"\n=== Fetching Transactions for {year} ===")
    print(f"League: {ctx.league_name} ({ctx.league_id})")

    # Check cache first if saving (meaning not current year or force refresh not requested)
    current_year = datetime.now().year
    cache_dir = ctx.transaction_data_directory / "cache"
    cache_path = cache_dir / f"transactions_year_{year}.parquet"

    if cache_path.exists() and year < current_year:
        print(f"  Using cached data from {cache_path}")
        try:
            df = pd.read_parquet(cache_path)

            # Apply manager name overrides even when loading from cache
            if ctx.manager_name_overrides and 'manager' in df.columns:
                override_count = len(ctx.manager_name_overrides)
                print(f"  Applying {override_count} manager name override(s)")
                df['manager'] = df['manager'].replace(ctx.manager_name_overrides)

            print(f"=== Completed (from cache): {len(df)} transaction records ===\n")

            # Still create output files if save_output=True (even when loading from cache)
            if save_output and not df.empty:
                ctx.transaction_data_directory.mkdir(parents=True, exist_ok=True)

                parquet_path = ctx.transaction_data_directory / f"transactions_year_{year}.parquet"
                csv_path = ctx.transaction_data_directory / f"transactions_year_{year}.csv"

                # Only save if files don't exist (avoid unnecessary writes)
                if not parquet_path.exists():
                    df.to_parquet(parquet_path, index=False)
                    print(f"[OK] Saved from cache: {parquet_path}")

                if not csv_path.exists():
                    df.to_csv(csv_path, index=False, encoding="utf-8", errors="replace")
                    print(f"[OK] Saved from cache: {csv_path}")

            return df
        except Exception as e:
            print(f"  Warning: Failed to read cache ({e}), fetching from API...")

    # Initialize OAuth
    oauth = ctx.get_oauth_session()
    gm = yfa.Game(oauth, ctx.game_code)

    # CRITICAL: Use specific league_id from context to avoid data mixing
    league_key = None
    if hasattr(ctx, 'get_league_id_for_year'):
        league_key = ctx.get_league_id_for_year(year)
        if league_key:
            print(f"[transactions] Using league_id from context for {year}: {league_key}")

    # Fallback to API discovery (may mix leagues!)
    if not league_key:
        try:
            league_ids = gm.league_ids(year=year)
            if not league_ids:
                print(f"  No league IDs found for year {year}")
                return pd.DataFrame()
            if len(league_ids) > 1:
                print(f"[transactions] WARNING: Multiple leagues found for {year}: {league_ids}")
                print(f"[transactions] WARNING: Using last one - this may cause data mixing!")
            league_key = league_ids[-1]
        except Exception as e:
            print(f"  ERROR: Failed to get league key for {year}: {e}")
            return pd.DataFrame()

    # Load matchup windows exactly like the matchup job (and build if missing)
    if matchup_windows is None:
        matchup_windows = load_matchup_windows(ctx, year)

    # Fetch team mappings
    team_mappings = fetch_team_mappings(oauth, league_key)

    # Fetch transactions
    transactions = fetch_transactions_for_year(oauth, league_key, year, team_mappings, matchup_windows)

    # Convert to DataFrame
    df = transactions_to_dataframe(transactions)

    # Process transactions in chronological order for roster state accuracy
    if not df.empty:
        df = process_transactions_chronologically(df)

    # Add league_id for multi-league isolation
    df["league_id"] = ctx.league_id

    # Apply manager name overrides from context (e.g., "--hidden--" -> "Ilan")
    if ctx.manager_name_overrides and 'manager' in df.columns:
        override_count = len(ctx.manager_name_overrides)
        print(f"  Applying {override_count} manager name override(s)")
        df['manager'] = df['manager'].replace(ctx.manager_name_overrides)

    print(f"=== Completed: {len(df)} transaction records ===\n")

    # Save to year-specific files if requested
    if save_output and not df.empty:
        # Ensure output directory exists
        ctx.transaction_data_directory.mkdir(parents=True, exist_ok=True)

        # Save year-specific files
        parquet_path = ctx.transaction_data_directory / f"transactions_year_{year}.parquet"
        csv_path = ctx.transaction_data_directory / f"transactions_year_{year}.csv"

        df.to_parquet(parquet_path, index=False)
        df.to_csv(csv_path, index=False, encoding="utf-8", errors="replace")

        print(f"[OK] Saved: {parquet_path}")
        print(f"[OK] Saved: {csv_path}")

        # Cache completed years (not current year)
        if year < current_year:
            cache_dir.mkdir(parents=True, exist_ok=True)
            df.to_parquet(cache_path, index=False)
            print(f"[CACHE] Saved to cache: {cache_path}")

    return df


def fetch_all_transaction_years(
    ctx: LeagueContext,
    matchup_windows: Optional[pd.DataFrame] = None
) -> Dict[int, pd.DataFrame]:
    """
    Fetch transaction data for all years in the league context.

    Each year is saved to a separate file: transactions_year_YYYY.parquet

    Args:
        ctx: LeagueContext with league configuration
        matchup_windows: Optional matchup windows for week calculation

    Returns:
        Dictionary mapping year to DataFrame (for backward compatibility)
    """
    current_year = datetime.now().year
    start_year = ctx.start_year if ctx.start_year else 2014
    end_year = ctx.end_year if ctx.end_year else current_year

    years = list(range(end_year, start_year - 1, -1))

    print(f"\n{'=' * 80}")
    print(f"Fetching transaction data for {len(years)} years: {min(years)}-{max(years)}")
    print(f"League: {ctx.league_name} ({ctx.league_id})")
    print(f"Output: One file per year (transactions_year_YYYY.parquet)")
    print(f"{'=' * 80}\n")

    results = {}
    total_records = 0

    for year in years:
        try:
            # fetch_transactions now saves to year-specific files automatically
            df = fetch_transactions(ctx, year, matchup_windows, save_output=True)
            if not df.empty:
                results[year] = df
                total_records += len(df)
        except Exception as e:
            print(f"ERROR: Failed to fetch transactions for {year}: {e}")
            continue

    if not results:
        print("No transaction data fetched")
        return {}

    _safe_print(f"\n{'=' * 80}")
    _safe_print(f"Total transaction records: {total_records:,}")
    _safe_print(f"Years covered: {sorted(results.keys())}")
    _safe_print(f"Files created: {len(results)} year-specific files")
    _safe_print(f"{'=' * 80}\n")

    return results


# =============================================================================
# CLI
# =============================================================================

def _safe_print(*args, **kwargs):
    try:
        print(*args, **kwargs)
    except UnicodeEncodeError:
        print(*(str(a).encode("ascii","replace").decode("ascii") for a in args), **kwargs)

def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Fetch Yahoo Fantasy Football transaction data (Multi-League V2)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Fetch specific year
  python transactions_v2.py --context /path/to/league_context.json --year 2024

  # Fetch all years
  python transactions_v2.py --context /path/to/league_context.json --all-years

  # Save to custom location
  python transactions_v2.py --context /path/to/league_context.json --year 2024 --output /path/to/output.parquet
        """
    )

    parser.add_argument('--context', type=Path, required=True, help='Path to league_context.json')
    parser.add_argument('--year', '-y', type=int, help='Specific year to fetch')
    parser.add_argument('--all-years', '-a', action='store_true', help='Fetch all available years')
    parser.add_argument('--output', '-o', type=Path, help='Output file path (default: ctx.transaction_data_directory/transactions.parquet)')

    args = parser.parse_args()

    # Validate context file
    if not LEAGUE_CONTEXT_AVAILABLE:
        print("ERROR: LeagueContext not available. Ensure multi_league package is installed.")
        return 1

    if not args.context.exists():
        print(f"ERROR: Context file not found: {args.context}")
        return 1

    # Load context
    try:
        ctx = LeagueContext.load(args.context)
    except Exception as e:
        print(f"ERROR: Failed to load league context: {e}")
        return 1

    # Determine years to fetch
    if args.all_years:
        results = fetch_all_transaction_years(ctx)
        if not results:
            print("No data fetched")
            return 1

        # If --output specified, combine all years and save to that location
        if args.output:
            all_dfs = list(results.values())
            combined = pd.concat(all_dfs, ignore_index=True)

            # Deduplicate by (transaction_id, yahoo_player_id)
            if "transaction_id" in combined.columns and "yahoo_player_id" in combined.columns:
                initial_count = len(combined)
                combined = combined.drop_duplicates(subset=["transaction_id", "yahoo_player_id"], keep="first")
                if len(combined) < initial_count:
                    _safe_print(f"\n[DEDUP] Removed {initial_count - len(combined):,} duplicate transaction records")

            output_path = args.output
            output_path.parent.mkdir(parents=True, exist_ok=True)

            try:
                combined.to_parquet(output_path, index=False)
                print(f"\n[OK] Saved combined file: {output_path}")

                csv_path = output_path.with_suffix('.csv')
                combined.to_csv(csv_path, index=False, encoding="utf-8", errors="replace")
                print(f"[OK] Saved combined CSV: {csv_path}")

                _safe_print(f"\n✓ Transaction data processing completed successfully ({len(combined)} records)")
            except Exception as e:
                _safe_print(f"ERROR: Failed to save combined output: {e}")
                return 1

        else:
            # Year-specific files already saved by fetch_transactions
            total_records = sum(len(df) for df in results.values())
            _safe_print(f"\n✓ Transaction data processing completed successfully ({total_records:,} records across {len(results)} years)")

    elif args.year:
        # Save to year-specific file unless custom output specified
        save_output = args.output is None
        df = fetch_transactions(ctx, args.year, save_output=save_output)

        if df.empty:
            print("No data fetched")
            return 1

        # If custom output path specified, save there
        if args.output:
            output_path = args.output
            output_path.parent.mkdir(parents=True, exist_ok=True)

            try:
                df.to_parquet(output_path, index=False)
                print(f"[OK] Saved: {output_path}")

                csv_path = output_path.with_suffix('.csv')
                df.to_csv(csv_path, index=False, encoding="utf-8", errors="replace")
                print(f"[OK] Saved: {csv_path}")

            except Exception as e:
                _safe_print(f"ERROR: Failed to save output: {e}")
                return 1

        _safe_print(f"\n✓ Transaction data processing completed successfully ({len(df)} records)")

    else:
        print("ERROR: Must specify --year or --all-years")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
