name: Fantasy Football Import Worker

on:
  workflow_dispatch:
    inputs:
      job_data:
        description: 'JSON encoded job data with league info and credentials'
        required: true
        type: string
      job_id:
        description: 'Unique job identifier'
        required: true
        type: string

  # Alternative: trigger via repository dispatch (API call)
  repository_dispatch:
    types: [import_job]

jobs:
  process-import:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    env:
      PYTHONUNBUFFERED: 1
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Parse job data
        id: parse
        run: |
          import json
          import os
          import sys
          
          # Parse job data from workflow input
          job_data_str = '''${{ github.event.inputs.job_data || github.event.client_payload.job_data }}'''
          job_id = '''${{ github.event.inputs.job_id || github.event.client_payload.job_id }}'''
          
          try:
              job_data = json.loads(job_data_str)
          except:
              print("ERROR: Failed to parse job data")
              sys.exit(1)
          
          # Set outputs for next steps
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"job_id={job_id}\n")
              f.write(f"league_key={job_data.get('league_key', '')}\n")
              f.write(f"league_name={job_data.get('league_name', '')}\n")
              f.write(f"season={job_data.get('season', '')}\n")
              f.write(f"database_name={job_data.get('database_name', '')}\n")
          
          # Save OAuth token to file for import scripts
          oauth_data = job_data.get('oauth_token', {})
          oauth_dir = "oauth"
          os.makedirs(oauth_dir, exist_ok=True)
          
          with open(f"{oauth_dir}/Oauth.json", 'w') as f:
              json.dump(oauth_data, f)
              
          print(f"Job {job_id}: Prepared for {job_data.get('league_name')} ({job_data.get('season')})")
        shell: python
        
      - name: Update job status - Starting
        env:
          JOB_ID: ${{ steps.parse.outputs.job_id }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create status artifact that Streamlit can check
          mkdir -p job_status
          echo '{
            "job_id": "'$JOB_ID'",
            "status": "running",
            "message": "Starting data import...",
            "started_at": "'$(date -Iseconds)'"
          }' > job_status/${JOB_ID}.json
          
      - name: Create league context
        id: context
        env:
          LEAGUE_KEY: ${{ steps.parse.outputs.league_key }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
          SEASON: ${{ steps.parse.outputs.season }}
        run: |
          import json
          import os
          from pathlib import Path
          
          # Create league context file for v2 scripts
          context_data = {
              "league_key": os.environ.get('LEAGUE_KEY'),
              "league_name": os.environ.get('LEAGUE_NAME'),
              "season": int(os.environ.get('SEASON', 2024)),
              "oauth_file": "oauth/Oauth.json",
              "data_dir": "fantasy_football_data",
              "cache_dir": ".cache"
          }
          
          context_file = Path("league_context.json")
          with open(context_file, 'w') as f:
              json.dump(context_data, f, indent=2)
              
          print(f"Created league context: {context_file}")
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"context_file={context_file}\n")
        shell: python
        
      - name: Run data import
        id: import
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          LEAGUE_CONTEXT: ${{ steps.context.outputs.context_file }}
          JOB_ID: ${{ steps.parse.outputs.job_id }}
          AUTO_CONFIRM: "1"
        run: |
          # Run the import script with context
          python initial_import_v2.py --context "$LEAGUE_CONTEXT" 2>&1 | tee import.log
          
          # Capture exit code
          EXIT_CODE=${PIPESTATUS[0]}
          echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Import completed successfully"
          else
            echo "‚ùå Import failed with exit code ${EXIT_CODE}"
            tail -100 import.log
          fi
          
          exit $EXIT_CODE
          
      - name: Upload to MotherDuck
        if: steps.import.outputs.exit_code == '0'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
          SEASON: ${{ steps.parse.outputs.season }}
        run: |
          python - <<EOF
          import os
          import duckdb
          from pathlib import Path
          
          # Connect to MotherDuck
          token = os.environ.get('MOTHERDUCK_TOKEN')
          if not token:
              print("WARNING: No MotherDuck token, skipping upload")
              exit(0)
              
          db_name = os.environ.get('DATABASE_NAME', '').strip()
          if not db_name:
              # Generate database name from league + season
              league = os.environ.get('LEAGUE_NAME', 'league').lower()
              season = os.environ.get('SEASON', '2024')
              db_name = f"{league}_{season}".replace(' ', '_').replace('-', '_')
              # Clean up special characters
              db_name = ''.join(c if c.isalnum() or c == '_' else '' for c in db_name)
          
          print(f"Connecting to MotherDuck database: {db_name}")
          
          con = duckdb.connect(f"md:{db_name}?motherduck_token={token}")
          
          # Find all parquet files
          data_dir = Path("fantasy_football_data")
          parquet_files = list(data_dir.glob("*.parquet"))
          
          if not parquet_files:
              print("WARNING: No parquet files found to upload")
              exit(0)
              
          # Upload each parquet file as a table
          for pf in parquet_files:
              table_name = pf.stem.lower().replace('-', '_')
              print(f"Uploading {table_name} from {pf}...")
              
              # Create or replace table from parquet
              con.execute(f"""
                  CREATE OR REPLACE TABLE {table_name} AS 
                  SELECT * FROM read_parquet('{pf}')
              """)
              
              # Get row count for verification
              count = con.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
              print(f"  ‚úÖ Uploaded {count:,} rows to {table_name}")
          
          # Create some useful views
          print("\nCreating analysis views...")
          
          # Season summary view
          con.execute("""
              CREATE OR REPLACE VIEW season_summary AS
              SELECT 
                  season,
                  manager_name,
                  COUNT(*) as games_played,
                  SUM(CASE WHEN won = 1 THEN 1 ELSE 0 END) as wins,
                  SUM(CASE WHEN won = 0 THEN 1 ELSE 0 END) as losses,
                  ROUND(AVG(team_points), 2) as avg_points,
                  MAX(team_points) as max_points,
                  MIN(team_points) as min_points
              FROM matchup
              GROUP BY season, manager_name
              ORDER BY season DESC, wins DESC
          """)
          
          print("‚úÖ MotherDuck upload complete!")
          print(f"üìä Database: {db_name}")
          print(f"üìÅ Tables created: {len(parquet_files)}")
          
          con.close()
          EOF
          
      - name: Update job status - Complete
        if: always()
        env:
          JOB_ID: ${{ steps.parse.outputs.job_id }}
          EXIT_CODE: ${{ steps.import.outputs.exit_code }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
        run: |
          mkdir -p job_status
          
          if [ "$EXIT_CODE" == "0" ]; then
            STATUS="complete"
            MESSAGE="Import successful! Data available in MotherDuck."
          else
            STATUS="failed"
            MESSAGE="Import failed. Check logs for details."
          fi
          
          echo '{
            "job_id": "'$JOB_ID'",
            "status": "'$STATUS'",
            "message": "'$MESSAGE'",
            "database_name": "'$DATABASE_NAME'",
            "completed_at": "'$(date -Iseconds)'"
          }' > job_status/${JOB_ID}.json
          
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: import-job-${{ steps.parse.outputs.job_id }}
          path: |
            import.log
            job_status/
            fantasy_football_data/*.parquet
          retention-days: 7
          
      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const jobId = '${{ steps.parse.outputs.job_id }}';
            const leagueName = '${{ steps.parse.outputs.league_name }}';
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Import Failed: ${leagueName} (Job ${jobId})`,
              body: `Import job failed for ${leagueName}.\n\nJob ID: ${jobId}\n\nCheck the [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`,
              labels: ['import-failure', 'automated']
            });