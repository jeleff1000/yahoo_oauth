name: League Import Worker

# This workflow is triggered on-demand from Streamlit when a user requests
# to create their own fantasy football analytics site

on:
  workflow_dispatch:
    inputs:
      league_data_b64:
        description: 'Base64-encoded JSON with league configuration'
        required: false
        type: string
      league_data:
        description: 'DEPRECATED - raw JSON (for backward compat)'
        required: false
        type: string
      user_id:
        description: 'Unique identifier for this user/import job'
        required: true
        type: string

  repository_dispatch:
    types: [league_import]

# Permissions needed to trigger the playoff odds workflow
permissions:
  actions: write
  contents: read

jobs:
  import-league-data:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Allow up to 2 hours for full import

    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Parse league data
        id: parse
        env:
          YAHOO_CLIENT_ID: ${{ secrets.YAHOO_CLIENT_ID }}
          YAHOO_CLIENT_SECRET: ${{ secrets.YAHOO_CLIENT_SECRET }}
          LEAGUE_DATA_B64: ${{ github.event.inputs.league_data_b64 || github.event.client_payload.league_data_b64 || '' }}
          LEAGUE_DATA_RAW: ${{ github.event.inputs.league_data || github.event.client_payload.league_data || '' }}
          USER_ID_INPUT: ${{ github.event.inputs.user_id || github.event.client_payload.user_id }}
        run: |
          python - <<'PYEOF'
          import json
          import os
          import sys
          import base64

          # Get input data from environment (avoids shell escaping issues)
          league_data_b64 = os.environ.get('LEAGUE_DATA_B64', '').strip()
          league_data_raw = os.environ.get('LEAGUE_DATA_RAW', '').strip()
          user_id = os.environ.get('USER_ID_INPUT', '')

          print(f"[DEBUG] Received league_data_b64 length: {len(league_data_b64)}")
          print(f"[DEBUG] Received league_data_raw length: {len(league_data_raw)}")

          league_data = None

          # Try base64-encoded format first (new format)
          if league_data_b64:
              print("[DEBUG] Using base64-encoded format")
              print(f"[DEBUG] First 50 chars of base64: {league_data_b64[:50]}...")
              try:
                  league_data_json = base64.b64decode(league_data_b64).decode('utf-8')
                  league_data = json.loads(league_data_json)
                  print(f"‚úÖ Successfully decoded base64 league data")
              except Exception as e:
                  print(f"WARNING: Failed to decode base64: {e}")
                  league_data = None

          # Fallback to raw JSON format (old format - may be truncated!)
          if league_data is None and league_data_raw:
              print("[DEBUG] Falling back to raw JSON format (WARNING: may be truncated)")
              print(f"[DEBUG] First 100 chars of raw: {league_data_raw[:100]}...")
              try:
                  league_data = json.loads(league_data_raw)
                  print(f"‚úÖ Successfully parsed raw JSON")
              except Exception as e:
                  print(f"ERROR: Failed to parse raw JSON: {e}")
                  league_data = None

          if league_data is None:
              print("ERROR: No valid league data provided (neither base64 nor raw JSON)")
              print("Make sure your Streamlit app is sending league_data_b64")
              sys.exit(1)

          print(f"[DEBUG] Decoded league_id: {league_data.get('league_id')}")
          print(f"[DEBUG] Decoded league_name: {league_data.get('league_name')}")
          print(f"[DEBUG] Decoded season: {league_data.get('season')}")
          print(f"[DEBUG] Decoded start_year: {league_data.get('start_year')}")

          # Extract league configuration
          league_id = league_data.get('league_id', '')
          league_name = league_data.get('league_name', 'Unknown League')
          season = league_data.get('season', league_data.get('end_year', 2024))
          start_year = league_data.get('start_year', season)
          oauth_raw = league_data.get('oauth_token', {})

          # Add Yahoo app credentials from secrets (required for yahoo_oauth library)
          consumer_key = os.environ.get('YAHOO_CLIENT_ID')
          consumer_secret = os.environ.get('YAHOO_CLIENT_SECRET')

          if not consumer_key or not consumer_secret:
              print("‚ö†Ô∏è  WARNING: YAHOO_CLIENT_ID and YAHOO_CLIENT_SECRET not set")
              print("   Import will likely fail - add these to GitHub secrets")

          # Build oauth_data in the format yahoo_oauth library expects
          # The library checks for token_time to determine if refresh is needed
          import time
          oauth_data = {
              'access_token': oauth_raw.get('access_token'),
              'refresh_token': oauth_raw.get('refresh_token'),
              'consumer_key': consumer_key,
              'consumer_secret': consumer_secret,
              'token_type': oauth_raw.get('token_type', 'bearer'),
              'expires_in': oauth_raw.get('expires_in', 3600),
              # token_time is CRITICAL - yahoo_oauth uses this to check expiration
              # Use the provided token_time or set to now (token will try refresh)
              'token_time': oauth_raw.get('token_time', time.time()),
              'guid': oauth_raw.get('xoauth_yahoo_guid') or oauth_raw.get('guid'),
          }

          print(f"‚úÖ Built OAuth data with consumer credentials")
          print(f"   access_token present: {bool(oauth_data.get('access_token'))}")
          print(f"   refresh_token present: {bool(oauth_data.get('refresh_token'))}")
          print(f"   consumer_key present: {bool(oauth_data.get('consumer_key'))}")
          print(f"   token_time: {oauth_data.get('token_time')}")

          # Create sanitized database name (just league name, no year - data contains all historical years)
          db_name = league_name.lower().replace(' ', '_').replace('-', '_')
          db_name = ''.join(c if c.isalnum() or c == '_' else '' for c in db_name)
          # Add 'l_' prefix if name starts with a digit (SQL identifiers can't start with digits)
          if db_name and db_name[0].isdigit():
              db_name = f"l_{db_name}"

          # Set outputs for next steps
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"user_id={user_id}\n")
              f.write(f"league_id={league_id}\n")
              f.write(f"league_name={league_name}\n")
              f.write(f"season={season}\n")
              f.write(f"start_year={start_year}\n")
              f.write(f"database_name={db_name}\n")

          # Save OAuth token to file (now includes consumer credentials)
          os.makedirs("oauth", exist_ok=True)
          with open("oauth/Oauth.json", 'w') as f:
              json.dump(oauth_data, f, indent=2)

          # Save full league data for context creation
          with open("league_data_input.json", 'w') as f:
              json.dump(league_data, f, indent=2)

          print(f"‚úÖ Parsed league: {league_name} ({season})")
          print(f"   League ID: {league_id}")
          print(f"   Database: {db_name}")
          print(f"   User ID: {user_id}")
          PYEOF

      - name: Update job status - Starting
        run: |
          mkdir -p job_status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "running",
            "phase": "starting",
            "message": "Initializing import...",
            "started_at": "$(date -Iseconds)"
          }
          EOF

      - name: Create league context
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Load the league data
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          # Get absolute path to OAuth file (script runs from fantasy_football_data_scripts subdirectory)
          oauth_file = Path("oauth/Oauth.json").resolve()

          # Ensure numeric values are integers (not strings from JSON)
          start_year = int(league_data.get('start_year', 2014))
          end_year = int(league_data.get('season', league_data.get('end_year', 2024)))
          num_teams = int(league_data.get('num_teams', 10))
          playoff_teams = int(league_data.get('playoff_teams', 6))
          regular_season_weeks = int(league_data.get('regular_season_weeks', 14))

          # Create league context file for initial_import_v2.py
          context = {
              "league_id": league_data.get('league_id'),
              "league_name": league_data.get('league_name'),
              "oauth_file_path": str(oauth_file),  # Use absolute path
              "game_code": league_data.get('game_code', 'nfl'),
              "start_year": start_year,
              "end_year": end_year,
              "num_teams": num_teams,
              "playoff_teams": playoff_teams,
              "regular_season_weeks": regular_season_weeks,
              "data_directory": str(Path("fantasy_football_data").resolve()),  # Absolute path
              "max_workers": 5,
              "enable_caching": True,
              "rate_limit_per_sec": 4.0,
              "manager_name_overrides": league_data.get('manager_name_overrides', {}),
              "keeper_rules": league_data.get('keeper_rules'),  # Pass keeper rules for economics calculation
              "has_external_data": league_data.get('has_external_data', False),  # Flag for staging merge
          }

          # Write context file
          with open("league_context.json", 'w') as f:
              json.dump(context, f, indent=2)

          print("‚úÖ League context created")
          print(json.dumps(context, indent=2))
          PYEOF

      - name: Create data directory
        run: |
          mkdir -p fantasy_football_data
          echo "‚úÖ Data directory ready"

      - name: Run initial import
        id: import
        env:
          AUTO_CONFIRM: "1"
        working-directory: fantasy_football_data_scripts
        run: |
          echo "========================================"
          echo "Starting import for ${{ steps.parse.outputs.league_name }}"
          echo "Season: ${{ steps.parse.outputs.season }}"
          echo "User ID: ${{ steps.parse.outputs.user_id }}"
          echo "========================================"

          # Update status
          cat > ../job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "importing",
            "message": "Running initial_import_v2.py...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          # Run the import
          python initial_import_v2.py --context ../league_context.json 2>&1 | tee ../import.log

          # Capture exit code
          EXIT_CODE=${PIPESTATUS[0]}
          echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT

          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Import completed successfully"
          else
            echo "‚ùå Import failed with exit code ${EXIT_CODE}"
            echo "Last 100 lines of log:"
            tail -100 ../import.log
            exit $EXIT_CODE
          fi

      - name: Verify parquet files created
        if: steps.import.outputs.exit_code == '0'
        id: verify
        run: |
          echo "Verifying parquet files..."

          if [ ! -d "fantasy_football_data" ]; then
            echo "‚ùå ERROR: Data directory not found!"
            exit 1
          fi

          PARQUET_FILES=$(find fantasy_football_data -name "*.parquet" -type f)
          PARQUET_COUNT=$(echo "$PARQUET_FILES" | grep -c ".parquet" || echo "0")

          if [ "$PARQUET_COUNT" -eq 0 ]; then
            echo "‚ùå ERROR: No parquet files were created!"
            exit 1
          fi

          echo "‚úÖ Found ${PARQUET_COUNT} parquet files:"
          ls -lh fantasy_football_data/*.parquet

          # Save file list
          echo "$PARQUET_FILES" > parquet_files.txt
          echo "file_count=${PARQUET_COUNT}" >> $GITHUB_OUTPUT

      - name: Diagnose parquet files
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Diagnostic: Parquet file columns"
          echo "========================================"
          python - <<'PYEOF'
          import pandas as pd
          from pathlib import Path

          data_dir = Path("fantasy_football_data")

          # Check canonical files
          canonical_files = ["player.parquet", "matchup.parquet", "draft.parquet", "transactions.parquet", "schedule.parquet"]

          for fname in canonical_files:
              fpath = data_dir / fname
              print(f"\n{'='*60}")
              print(f"File: {fname}")
              print('='*60)
              if fpath.exists():
                  try:
                      df = pd.read_parquet(fpath)
                      print(f"  Rows: {len(df):,}")
                      print(f"  Columns: {len(df.columns)}")

                      # Check for critical columns
                      critical = ["manager_week", "cumulative_week", "manager", "year", "week", "fantasy_points"]
                      for col in critical:
                          if col in df.columns:
                              non_null = df[col].notna().sum()
                              print(f"  ‚úì {col}: {non_null:,} non-null values")
                          else:
                              print(f"  ‚úó {col}: MISSING")

                      # Show first few column names
                      print(f"  Sample columns: {list(df.columns[:15])}...")
                  except Exception as e:
                      print(f"  ERROR reading file: {e}")
              else:
                  print(f"  FILE NOT FOUND")

          # Check subdirectories for intermediate files
          print(f"\n{'='*60}")
          print("Subdirectory file counts:")
          print('='*60)
          for subdir in ["player_data", "matchup_data", "draft_data", "transaction_data", "schedule_data"]:
              subpath = data_dir / subdir
              if subpath.exists():
                  files = list(subpath.glob("*.parquet"))
                  print(f"  {subdir}/: {len(files)} parquet files")
              else:
                  print(f"  {subdir}/: DIRECTORY NOT FOUND")
          PYEOF

      - name: Consolidate league settings to parquet
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Consolidating league settings to parquet"
          echo "========================================"
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import pandas as pd

          settings_dir = Path("fantasy_football_data/league_settings")
          output_file = Path("fantasy_football_data/league_settings.parquet")

          if not settings_dir.exists():
              print(f"‚ö†Ô∏è  No league_settings directory found at {settings_dir}")
              print("   Playoff odds will use default configuration")
              exit(0)

          settings_files = list(settings_dir.glob("league_settings_*.json"))

          if not settings_files:
              print(f"‚ö†Ô∏è  No league settings JSON files found in {settings_dir}")
              exit(0)

          print(f"Found {len(settings_files)} league settings files")

          rows = []
          for sf in sorted(settings_files):
              try:
                  with open(sf, 'r', encoding='utf-8') as f:
                      data = json.load(f)

                  # Extract key fields for playoff odds calculation
                  metadata = data.get('metadata', {})
                  row = {
                      'year': data.get('year'),
                      'league_key': data.get('league_key'),
                      'num_teams': metadata.get('num_teams'),
                      'num_playoff_teams': metadata.get('num_playoff_teams') or metadata.get('playoff_teams'),
                      'playoff_start_week': metadata.get('playoff_start_week'),
                      'uses_playoff_reseeding': metadata.get('uses_playoff_reseeding'),
                      'bye_teams': metadata.get('bye_teams'),
                      'current_week': metadata.get('current_week'),
                      'is_finished': metadata.get('is_finished'),
                      # Store full settings as JSON string for flexibility
                      'settings_json': json.dumps(data)
                  }
                  rows.append(row)
                  print(f"  ‚úì {sf.name}: year={row['year']}, playoff_teams={row['num_playoff_teams']}")
              except Exception as e:
                  print(f"  ‚úó {sf.name}: {e}")

          if rows:
              df = pd.DataFrame(rows)
              df.to_parquet(output_file, index=False)
              print(f"\n‚úÖ Created {output_file} with {len(rows)} years of settings")
          else:
              print("‚ö†Ô∏è  No valid settings to save")
          PYEOF

      - name: Upload to MotherDuck
        if: steps.import.outputs.exit_code == '0'
        id: motherduck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
          SEASON: ${{ steps.parse.outputs.season }}
        run: |
          # Update status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "uploading",
            "message": "Uploading to MotherDuck...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          if [ -z "$MOTHERDUCK_TOKEN" ]; then
            echo "‚ö†Ô∏è  WARNING: MOTHERDUCK_TOKEN not set!"
            echo "Cannot upload to MotherDuck. Please add MOTHERDUCK_TOKEN as a GitHub secret."
            exit 1
          fi

          echo "========================================"
          echo "Uploading to MotherDuck"
          echo "Database: ${DATABASE_NAME}"
          echo "========================================"

          # Use the motherduck_upload.py script
          cd fantasy_football_data
          python motherduck_upload.py "${DATABASE_NAME}" . 2>&1 | tee ../motherduck.log

          UPLOAD_EXIT=$?

          if [ $UPLOAD_EXIT -eq 0 ]; then
            echo "‚úÖ MotherDuck upload complete!"
            echo "üìä Database: ${DATABASE_NAME}"
            echo "üîó Access at: https://app.motherduck.com/"
            echo "upload_success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå MotherDuck upload failed!"
            tail -50 ../motherduck.log
            echo "upload_success=false" >> $GITHUB_OUTPUT
            exit $UPLOAD_EXIT
          fi

      - name: Merge external staging data
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
        run: |
          echo "========================================"
          echo "Checking for external staging data"
          echo "========================================"

          python - <<'PYEOF'
          import json
          import os
          import re
          import duckdb

          def _slug(s, prefix_if_digit="l"):
              """Consistent slug function matching Streamlit app."""
              x = re.sub(r"[^a-zA-Z0-9]+", "_", s.strip().lower()).strip("_")
              if re.match(r"^\d", x):
                  x = f"{prefix_if_digit}_{x}"
              return x[:63]

          # Check if has_external_data flag is set
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          has_external_data = league_data.get("has_external_data", False)

          if not has_external_data:
              print("‚ÑπÔ∏è No external data flag set - skipping staging merge")
              exit(0)

          # Use league_name and apply same slug as Streamlit upload
          league_name = league_data.get("league_name", "")
          db_name = _slug(league_name, "l")
          token = os.environ.get("MOTHERDUCK_TOKEN")

          if not token or not db_name:
              print("‚ö†Ô∏è Missing MOTHERDUCK_TOKEN or DATABASE_NAME")
              exit(0)

          print(f"üîÑ Merging staging data for database: {db_name}")
          print(f"   (from league_name: {league_name})")

          os.environ["MOTHERDUCK_TOKEN"] = token
          con = duckdb.connect("md:")

          # First, list all databases to verify our target exists
          print("\nüìã Checking available databases...")
          all_dbs = con.execute("SELECT database_name FROM information_schema.schemata WHERE database_name NOT IN ('memory', 'system', 'temp')").fetchall()
          print(f"   Available databases: {[d[0] for d in all_dbs]}")

          # USE the database to ensure correct context
          try:
              con.execute(f"USE {db_name}")
              print(f"   ‚úì Connected to database: {db_name}")
          except Exception as e:
              print(f"   ‚ùå Could not connect to database {db_name}: {e}")
              con.close()
              exit(0)

          # Check if staging schema exists
          try:
              staging_tables = con.execute(f"""
                  SELECT table_name
                  FROM information_schema.tables
                  WHERE table_catalog = '{db_name}'
                  AND table_schema = 'staging'
              """).fetchall()
              print(f"   Staging tables query returned: {staging_tables}")
          except Exception as e:
              print(f"   Staging tables query error: {e}")
              staging_tables = []

          if not staging_tables:
              print("‚ÑπÔ∏è No staging tables found - nothing to merge")
              # List all schemas/tables in the database for debugging
              try:
                  all_tables = con.execute(f"""
                      SELECT table_schema, table_name
                      FROM information_schema.tables
                      WHERE table_catalog = '{db_name}'
                  """).fetchall()
                  print(f"   All tables in {db_name}: {all_tables}")
              except:
                  pass
              con.close()
              exit(0)

          print(f"Found {len(staging_tables)} staging tables to merge")

          # Table mapping: staging name -> main table name
          table_map = {
              "staging_matchup": "matchup",
              "staging_player": "player",
              "staging_draft": "draft",
              "staging_transactions": "transactions",
              "staging_schedule": "schedule",
          }

          for (staging_name,) in staging_tables:
              main_table = table_map.get(staging_name, staging_name.replace("staging_", ""))
              print(f"  Merging {db_name}.staging.{staging_name} ‚Üí {db_name}.public.{main_table}")

              try:
                  # Check if main table exists
                  main_exists = con.execute(f"""
                      SELECT COUNT(*) FROM information_schema.tables
                      WHERE table_catalog = '{db_name}'
                      AND table_schema = 'public'
                      AND table_name = '{main_table}'
                  """).fetchone()[0] > 0

                  if main_exists:
                      # Get columns that exist in both tables
                      staging_cols = con.execute(f"DESCRIBE {db_name}.staging.{staging_name}").fetchall()
                      main_cols = con.execute(f"DESCRIBE {db_name}.public.{main_table}").fetchall()

                      staging_col_names = {row[0] for row in staging_cols}
                      main_col_names = {row[0] for row in main_cols}
                      common_cols = staging_col_names & main_col_names

                      if common_cols:
                          col_list = ", ".join(common_cols)
                          # Insert new rows (external data is typically for years not in Yahoo)
                          con.execute(f"""
                              INSERT INTO {db_name}.public.{main_table} ({col_list})
                              SELECT {col_list} FROM {db_name}.staging.{staging_name}
                          """)
                          cnt = con.execute(f"SELECT COUNT(*) FROM {db_name}.staging.{staging_name}").fetchone()[0]
                          print(f"    ‚úì Merged {cnt} rows")
                      else:
                          print(f"    ‚ö†Ô∏è No common columns between staging and main table")
                  else:
                      # Main table doesn't exist - create it from staging
                      con.execute(f"""
                          CREATE TABLE {db_name}.public.{main_table} AS
                          SELECT * FROM {db_name}.staging.{staging_name}
                      """)
                      cnt = con.execute(f"SELECT COUNT(*) FROM {db_name}.public.{main_table}").fetchone()[0]
                      print(f"    ‚úì Created new table with {cnt} rows")

              except Exception as e:
                  print(f"    ‚ùå Error merging {staging_name}: {e}")

          # Clean up staging tables
          print("\nüßπ Cleaning up staging tables...")
          for (staging_name,) in staging_tables:
              try:
                  con.execute(f"DROP TABLE IF EXISTS {db_name}.staging.{staging_name}")
                  print(f"    ‚úì Dropped {staging_name}")
              except:
                  pass

          con.close()
          print("\n‚úÖ External data merge complete")
          PYEOF

      - name: Store credentials for weekly updates
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          CREDENTIAL_ENCRYPTION_KEY: ${{ secrets.CREDENTIAL_ENCRYPTION_KEY }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_ID: ${{ steps.parse.outputs.league_id }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
        run: |
          echo "========================================"
          echo "Storing credentials for weekly updates"
          echo "========================================"

          # Check if encryption key is available
          if [ -z "$CREDENTIAL_ENCRYPTION_KEY" ]; then
            echo "‚ö†Ô∏è CREDENTIAL_ENCRYPTION_KEY not set - weekly updates will require manual auth"
            echo "To enable automatic weekly updates, add CREDENTIAL_ENCRYPTION_KEY to GitHub secrets"
            echo "Generate one with: python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\""
            exit 0
          fi

          python - <<'PYEOF'
          import json
          import sys
          sys.path.insert(0, 'fantasy_football_data_scripts')

          from multi_league.utils.credential_store import store_league_credentials
          import os

          # Read the refresh token from the OAuth file we created earlier
          with open("oauth/Oauth.json", 'r') as f:
              oauth_data = json.load(f)

          refresh_token = oauth_data.get('refresh_token')
          if not refresh_token:
              print("‚ö†Ô∏è No refresh_token in OAuth data - weekly updates may not work")
              exit(0)

          database_name = os.environ['DATABASE_NAME']
          league_id = os.environ['LEAGUE_ID']
          league_name = os.environ['LEAGUE_NAME']

          success = store_league_credentials(
              database_name=database_name,
              refresh_token=refresh_token,
              league_id=league_id,
              league_name=league_name
          )

          if success:
              print(f"‚úÖ Credentials stored - weekly updates enabled for {league_name}")
          else:
              print("‚ö†Ô∏è Failed to store credentials - weekly updates may require manual auth")
          PYEOF

      - name: Confirm weekly updates enabled
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          # Weekly updates are auto-discovered from MotherDuck
          # Any database with a 'matchup' table will receive weekly updates
          echo "‚úÖ League '${{ steps.parse.outputs.database_name }}' uploaded to MotherDuck"
          echo "‚ÑπÔ∏è Weekly updates will be auto-discovered (database has matchup table)"

      - name: Create deployment manifest
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Create a deployment manifest with all the info needed to create the user's site
          manifest = {
              "user_id": "${{ steps.parse.outputs.user_id }}",
              "league_name": "${{ steps.parse.outputs.league_name }}",
              "league_id": "${{ steps.parse.outputs.league_id }}",
              "season": "${{ steps.parse.outputs.season }}",
              "motherduck_database": "${{ steps.parse.outputs.database_name }}",
              "parquet_file_count": ${{ steps.verify.outputs.file_count }},
              "created_at": os.popen('date -Iseconds').read().strip(),
              "workflow_run_id": "${{ github.run_id }}",
              "status": "ready",
              "weekly_updates_enabled": os.environ.get('CREDENTIAL_ENCRYPTION_KEY') is not None,
              "next_steps": [
                  "Create KMFFLApp-based UI repository",
                  "Configure MotherDuck connection",
                  "Deploy Streamlit site"
              ]
          }

          with open("deployment_manifest.json", 'w') as f:
              json.dump(manifest, f, indent=2)

          print("‚úÖ Deployment manifest created")
          print(json.dumps(manifest, indent=2))
          PYEOF

      - name: Update job status - Complete
        if: always()
        run: |
          if [ "${{ steps.import.outputs.exit_code }}" == "0" ] && [ "${{ steps.motherduck.outputs.upload_success }}" == "true" ]; then
            STATUS="complete"
            MESSAGE="Import successful! Data uploaded to MotherDuck database: ${{ steps.parse.outputs.database_name }}"
          elif [ "${{ steps.import.outputs.exit_code }}" != "0" ]; then
            STATUS="failed"
            MESSAGE="Import failed during data collection phase"
          else
            STATUS="failed"
            MESSAGE="Import succeeded but MotherDuck upload failed"
          fi

          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "${STATUS}",
            "message": "${MESSAGE}",
            "parquet_files": ${{ steps.verify.outputs.file_count || 0 }},
            "motherduck_url": "https://app.motherduck.com/",
            "completed_at": "$(date -Iseconds)"
          }
          EOF

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: league-import-${{ steps.parse.outputs.user_id }}
          path: |
            import.log
            motherduck.log
            job_status/
            deployment_manifest.json
            fantasy_football_data/*.parquet
            league_context.json
          retention-days: 30

      - name: Create workflow summary
        if: always()
        run: |
          echo "# League Import Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## League Information" >> $GITHUB_STEP_SUMMARY
          echo "- **League**: ${{ steps.parse.outputs.league_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Season**: ${{ steps.parse.outputs.season }}" >> $GITHUB_STEP_SUMMARY
          echo "- **User ID**: ${{ steps.parse.outputs.user_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **MotherDuck Database**: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.import.outputs.exit_code }}" == "0" ]; then
            echo "## ‚úÖ Import Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Parquet files created: ${{ steps.verify.outputs.file_count }}" >> $GITHUB_STEP_SUMMARY
            echo "- MotherDuck upload: ${{ steps.motherduck.outputs.upload_success == 'true' && '‚úÖ Success' || '‚ùå Failed' }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Files Created" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -lh fantasy_football_data/*.parquet 2>/dev/null || echo "No files found"
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. Data is now available in MotherDuck" >> $GITHUB_STEP_SUMMARY
            echo "2. Create custom analytics site based on KMFFLApp" >> $GITHUB_STEP_SUMMARY
            echo "3. Configure site to use database: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ùå Import Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the logs for details" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Trigger Playoff Odds Calculation
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "========================================"
          echo "Triggering Playoff Odds Calculator"
          echo "========================================"

          gh workflow run playoff_odds_worker.yml \
            -f database_name="${{ steps.parse.outputs.database_name }}" \
            -f n_sims="10000" \
            -f user_id="${{ steps.parse.outputs.user_id }}"

          if [ $? -eq 0 ]; then
            echo "‚úÖ Playoff odds workflow triggered successfully"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Playoff Odds" >> $GITHUB_STEP_SUMMARY
            echo "A separate workflow has been triggered to calculate playoff odds with 10,000 Monte Carlo simulations." >> $GITHUB_STEP_SUMMARY
            echo "This may take up to 3 hours to complete." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è Could not trigger playoff odds workflow"
          fi

      # Note: Issue creation removed due to permission requirements
      # Failures are logged in workflow summary and artifacts instead
