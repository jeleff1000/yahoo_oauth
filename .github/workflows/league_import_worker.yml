name: League Import Worker

# This workflow is triggered on-demand from Streamlit when a user requests
# to create their own fantasy football analytics site

on:
  workflow_dispatch:
    inputs:
      league_data_b64:
        description: 'Base64-encoded JSON with league configuration'
        required: false
        type: string
      league_data:
        description: 'DEPRECATED - raw JSON (for backward compat)'
        required: false
        type: string
      user_id:
        description: 'Unique identifier for this user/import job'
        required: true
        type: string

  repository_dispatch:
    types: [league_import]

# Permissions needed to trigger the playoff odds workflow
permissions:
  actions: write
  contents: read

jobs:
  import-league-data:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Allow up to 2 hours for full import

    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Parse league data
        id: parse
        env:
          YAHOO_CLIENT_ID: ${{ secrets.YAHOO_CLIENT_ID }}
          YAHOO_CLIENT_SECRET: ${{ secrets.YAHOO_CLIENT_SECRET }}
          LEAGUE_DATA_B64: ${{ github.event.inputs.league_data_b64 || github.event.client_payload.league_data_b64 || '' }}
          LEAGUE_DATA_RAW: ${{ github.event.inputs.league_data || github.event.client_payload.league_data || '' }}
          USER_ID_INPUT: ${{ github.event.inputs.user_id || github.event.client_payload.user_id }}
        run: |
          python - <<'PYEOF'
          import json
          import os
          import sys
          import base64

          # Get input data from environment (avoids shell escaping issues)
          league_data_b64 = os.environ.get('LEAGUE_DATA_B64', '').strip()
          league_data_raw = os.environ.get('LEAGUE_DATA_RAW', '').strip()
          user_id = os.environ.get('USER_ID_INPUT', '')

          print(f"[DEBUG] Received league_data_b64 length: {len(league_data_b64)}")
          print(f"[DEBUG] Received league_data_raw length: {len(league_data_raw)}")

          league_data = None

          # Try base64-encoded format first (new format)
          if league_data_b64:
              print("[DEBUG] Using base64-encoded format")
              print(f"[DEBUG] First 50 chars of base64: {league_data_b64[:50]}...")
              try:
                  league_data_json = base64.b64decode(league_data_b64).decode('utf-8')
                  league_data = json.loads(league_data_json)
                  print(f"‚úÖ Successfully decoded base64 league data")
              except Exception as e:
                  print(f"WARNING: Failed to decode base64: {e}")
                  league_data = None

          # Fallback to raw JSON format (old format - may be truncated!)
          if league_data is None and league_data_raw:
              print("[DEBUG] Falling back to raw JSON format (WARNING: may be truncated)")
              print(f"[DEBUG] First 100 chars of raw: {league_data_raw[:100]}...")
              try:
                  league_data = json.loads(league_data_raw)
                  print(f"‚úÖ Successfully parsed raw JSON")
              except Exception as e:
                  print(f"ERROR: Failed to parse raw JSON: {e}")
                  league_data = None

          if league_data is None:
              print("ERROR: No valid league data provided (neither base64 nor raw JSON)")
              print("Make sure your Streamlit app is sending league_data_b64")
              sys.exit(1)

          print(f"[DEBUG] Decoded league_id: {league_data.get('league_id')}")
          print(f"[DEBUG] Decoded league_name: {league_data.get('league_name')}")
          print(f"[DEBUG] Decoded season: {league_data.get('season')}")
          print(f"[DEBUG] Decoded start_year: {league_data.get('start_year')}")

          # Extract league configuration
          league_id = league_data.get('league_id', '')
          league_name = league_data.get('league_name', 'Unknown League')
          season = league_data.get('season', league_data.get('end_year', 2024))
          start_year = league_data.get('start_year', season)
          oauth_raw = league_data.get('oauth_token', {})

          # Add Yahoo app credentials from secrets (required for yahoo_oauth library)
          consumer_key = os.environ.get('YAHOO_CLIENT_ID')
          consumer_secret = os.environ.get('YAHOO_CLIENT_SECRET')

          if not consumer_key or not consumer_secret:
              print("‚ö†Ô∏è  WARNING: YAHOO_CLIENT_ID and YAHOO_CLIENT_SECRET not set")
              print("   Import will likely fail - add these to GitHub secrets")

          # Build oauth_data in the format yahoo_oauth library expects
          # The library checks for token_time to determine if refresh is needed
          import time
          oauth_data = {
              'access_token': oauth_raw.get('access_token'),
              'refresh_token': oauth_raw.get('refresh_token'),
              'consumer_key': consumer_key,
              'consumer_secret': consumer_secret,
              'token_type': oauth_raw.get('token_type', 'bearer'),
              'expires_in': oauth_raw.get('expires_in', 3600),
              # token_time is CRITICAL - yahoo_oauth uses this to check expiration
              # Use the provided token_time or set to now (token will try refresh)
              'token_time': oauth_raw.get('token_time', time.time()),
              'guid': oauth_raw.get('xoauth_yahoo_guid') or oauth_raw.get('guid'),
          }

          print(f"‚úÖ Built OAuth data with consumer credentials")
          print(f"   access_token present: {bool(oauth_data.get('access_token'))}")
          print(f"   refresh_token present: {bool(oauth_data.get('refresh_token'))}")
          print(f"   consumer_key present: {bool(oauth_data.get('consumer_key'))}")
          print(f"   token_time: {oauth_data.get('token_time')}")

          # Create sanitized database name (just league name, no year - data contains all historical years)
          db_name = league_name.lower().replace(' ', '_').replace('-', '_')
          db_name = ''.join(c if c.isalnum() or c == '_' else '' for c in db_name)
          # Add 'l_' prefix if name starts with a digit (SQL identifiers can't start with digits)
          if db_name and db_name[0].isdigit():
              db_name = f"l_{db_name}"

          # Add collision prevention: if league_id looks like "449.l.123456", extract the unique part
          # This helps prevent two leagues with the same name from colliding
          import hashlib
          if league_id:
              # Create a short hash of the league_id to make names unique
              league_id_hash = hashlib.md5(league_id.encode()).hexdigest()[:6]
              # We'll pass both - main.py can check for collisions
              db_name_with_hash = f"{db_name}_{league_id_hash}"
          else:
              db_name_with_hash = db_name

          print(f"   Database name: {db_name}")
          print(f"   Database name (with hash): {db_name_with_hash}")

          # Check for database name collision and resolve if needed
          final_db_name = db_name
          try:
              import duckdb
              md_token = os.environ.get('MOTHERDUCK_TOKEN')
              if md_token:
                  con = duckdb.connect(f"md:?motherduck_token={md_token}")
                  # Check if this database already exists
                  existing = con.execute(f"""
                      SELECT COUNT(*) FROM information_schema.schemata
                      WHERE catalog_name = '{db_name}'
                  """).fetchone()[0]

                  if existing > 0:
                      # Database exists - check if it's for the same league
                      try:
                          ctx_result = con.execute(f"""
                              SELECT league_id FROM {db_name}.public.league_context LIMIT 1
                          """).fetchone()
                          existing_league_id = ctx_result[0] if ctx_result else None

                          if existing_league_id and existing_league_id != league_id:
                              # Different league! Use hashed name
                              print(f"‚ö†Ô∏è Database '{db_name}' exists for different league ({existing_league_id})")
                              print(f"   Using unique name: {db_name_with_hash}")
                              final_db_name = db_name_with_hash
                          else:
                              print(f"‚úì Database '{db_name}' exists for same league - will update")
                      except:
                          # No context table - assume it's the same league (legacy data)
                          print(f"‚úì Database '{db_name}' exists (no context table) - will update")
                  else:
                      print(f"‚úì Database '{db_name}' is new")
                  con.close()
          except Exception as e:
              print(f"‚ö†Ô∏è Could not check for collision: {e}")
              # If we can't check, use the base name

          # Set outputs for next steps
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"user_id={user_id}\n")
              f.write(f"league_id={league_id}\n")
              f.write(f"league_name={league_name}\n")
              f.write(f"season={season}\n")
              f.write(f"start_year={start_year}\n")
              f.write(f"database_name={final_db_name}\n")

          # Save OAuth token to file (now includes consumer credentials)
          os.makedirs("oauth", exist_ok=True)
          with open("oauth/Oauth.json", 'w') as f:
              json.dump(oauth_data, f, indent=2)

          # Save full league data for context creation
          with open("league_data_input.json", 'w') as f:
              json.dump(league_data, f, indent=2)

          print(f"‚úÖ Parsed league: {league_name} ({season})")
          print(f"   League ID: {league_id}")
          print(f"   Database: {db_name}")
          print(f"   User ID: {user_id}")
          PYEOF

      - name: Update job status - Starting
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p job_status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "running",
            "phase": "starting",
            "message": "Initializing import...",
            "started_at": "$(date -Iseconds)",
            "workflow_run_id": "${{ github.run_id }}"
          }
          EOF

          # Write job tracking to MotherDuck so Streamlit can find the run_id
          python - <<'PYEOF'
          import os
          import duckdb
          from datetime import datetime

          token = os.environ.get('MOTHERDUCK_TOKEN')
          if not token:
              print("‚ö†Ô∏è No MOTHERDUCK_TOKEN - skipping job tracking")
              exit(0)

          user_id = "${{ steps.parse.outputs.user_id }}"
          league_name = "${{ steps.parse.outputs.league_name }}"
          run_id = "${{ github.run_id }}"

          try:
              os.environ['MOTHERDUCK_TOKEN'] = token
              con = duckdb.connect("md:")

              # Create ops database and table if needed
              con.execute("CREATE DATABASE IF NOT EXISTS ops")
              con.execute("""
                  CREATE TABLE IF NOT EXISTS ops.import_jobs (
                      user_id TEXT PRIMARY KEY,
                      workflow_run_id TEXT,
                      league_name TEXT,
                      status TEXT,
                      started_at TIMESTAMP,
                      updated_at TIMESTAMP
                  )
              """)

              # Insert or update job record
              now = datetime.utcnow()
              con.execute("""
                  INSERT OR REPLACE INTO ops.import_jobs
                  (user_id, workflow_run_id, league_name, status, started_at, updated_at)
                  VALUES (?, ?, ?, 'running', ?, ?)
              """, [user_id, run_id, league_name, now, now])

              con.close()
              print(f"‚úÖ Job tracking saved: user_id={user_id}, run_id={run_id}")
          except Exception as e:
              print(f"‚ö†Ô∏è Could not save job tracking: {e}")
          PYEOF

      - name: Refresh OAuth token
        env:
          YAHOO_CLIENT_ID: ${{ secrets.YAHOO_CLIENT_ID }}
          YAHOO_CLIENT_SECRET: ${{ secrets.YAHOO_CLIENT_SECRET }}
        run: |
          echo "========================================"
          echo "Refreshing OAuth token"
          echo "========================================"
          python - <<'PYEOF'
          import json
          import os
          import time
          import requests

          # Read current OAuth data
          with open("oauth/Oauth.json", 'r') as f:
              oauth_data = json.load(f)

          refresh_token = oauth_data.get('refresh_token')
          if not refresh_token:
              print("‚ö†Ô∏è No refresh_token found - will try with existing access_token")
              exit(0)

          consumer_key = os.environ.get('YAHOO_CLIENT_ID')
          consumer_secret = os.environ.get('YAHOO_CLIENT_SECRET')

          if not consumer_key or not consumer_secret:
              print("‚ö†Ô∏è Missing Yahoo credentials - cannot refresh token")
              exit(0)

          print(f"Refreshing token using refresh_token...")

          # Yahoo OAuth token refresh endpoint
          token_url = "https://api.login.yahoo.com/oauth2/get_token"

          try:
              response = requests.post(
                  token_url,
                  data={
                      'grant_type': 'refresh_token',
                      'refresh_token': refresh_token,
                      'client_id': consumer_key,
                      'client_secret': consumer_secret,
                  },
                  headers={'Content-Type': 'application/x-www-form-urlencoded'},
                  timeout=30
              )

              if response.status_code == 200:
                  new_tokens = response.json()

                  # Update OAuth data with fresh tokens
                  oauth_data['access_token'] = new_tokens.get('access_token')
                  oauth_data['refresh_token'] = new_tokens.get('refresh_token', refresh_token)  # May return new refresh_token
                  oauth_data['token_time'] = time.time()  # Reset token_time to NOW
                  oauth_data['expires_in'] = new_tokens.get('expires_in', 3600)

                  # Save updated OAuth data
                  with open("oauth/Oauth.json", 'w') as f:
                      json.dump(oauth_data, f, indent=2)

                  print(f"‚úÖ Token refreshed successfully!")
                  print(f"   New token_time: {oauth_data['token_time']}")
                  print(f"   Expires in: {oauth_data['expires_in']} seconds")
              else:
                  print(f"‚ö†Ô∏è Token refresh failed: {response.status_code}")
                  print(f"   Response: {response.text[:500]}")
                  print("   Will try with existing access_token (may fail if expired)")

          except Exception as e:
              print(f"‚ö†Ô∏è Token refresh error: {e}")
              print("   Will try with existing access_token (may fail if expired)")
          PYEOF

      - name: Create league context
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Load the league data
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          # Get absolute path to OAuth file (script runs from fantasy_football_data_scripts subdirectory)
          oauth_file = Path("oauth/Oauth.json").resolve()

          # Ensure numeric values are integers (not strings from JSON)
          start_year = int(league_data.get('start_year', 2014))
          end_year = int(league_data.get('season', league_data.get('end_year', 2024)))
          num_teams = int(league_data.get('num_teams', 10))
          playoff_teams = int(league_data.get('playoff_teams', 6))
          regular_season_weeks = int(league_data.get('regular_season_weeks', 14))

          # Create league context file for initial_import_v2.py
          context = {
              "league_id": league_data.get('league_id'),
              "league_name": league_data.get('league_name'),
              "oauth_file_path": str(oauth_file),  # Use absolute path
              "game_code": league_data.get('game_code', 'nfl'),
              "start_year": start_year,
              "end_year": end_year,
              "num_teams": num_teams,
              "playoff_teams": playoff_teams,
              "regular_season_weeks": regular_season_weeks,
              "data_directory": str(Path("fantasy_football_data").resolve()),  # Absolute path
              "max_workers": 5,
              "enable_caching": True,
              "rate_limit_per_sec": 4.0,
              "manager_name_overrides": league_data.get('manager_name_overrides', {}),
              "keeper_rules": league_data.get('keeper_rules'),  # Pass keeper rules for economics calculation
              "has_external_data": league_data.get('has_external_data', False),  # Flag for staging merge
          }

          # Write context file
          with open("league_context.json", 'w') as f:
              json.dump(context, f, indent=2)

          print("‚úÖ League context created")
          print(json.dumps(context, indent=2))
          PYEOF

      - name: Create data directory
        run: |
          mkdir -p fantasy_football_data
          echo "‚úÖ Data directory ready"

      - name: Run initial import
        id: import
        env:
          AUTO_CONFIRM: "1"
        working-directory: fantasy_football_data_scripts
        run: |
          echo "========================================"
          echo "Starting import for ${{ steps.parse.outputs.league_name }}"
          echo "Season: ${{ steps.parse.outputs.season }}"
          echo "User ID: ${{ steps.parse.outputs.user_id }}"
          echo "========================================"

          # Update status
          cat > ../job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "importing",
            "message": "Running initial_import_v2.py...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          # Run the import
          python initial_import_v2.py --context ../league_context.json 2>&1 | tee ../import.log

          # Capture exit code
          EXIT_CODE=${PIPESTATUS[0]}
          echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT

          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Import completed successfully"
          else
            echo "‚ùå Import failed with exit code ${EXIT_CODE}"
            echo "Last 100 lines of log:"
            tail -100 ../import.log
            exit $EXIT_CODE
          fi

      - name: Verify parquet files created
        if: steps.import.outputs.exit_code == '0'
        id: verify
        run: |
          echo "Verifying parquet files..."

          if [ ! -d "fantasy_football_data" ]; then
            echo "‚ùå ERROR: Data directory not found!"
            exit 1
          fi

          PARQUET_FILES=$(find fantasy_football_data -name "*.parquet" -type f)
          PARQUET_COUNT=$(echo "$PARQUET_FILES" | grep -c ".parquet" || echo "0")

          if [ "$PARQUET_COUNT" -eq 0 ]; then
            echo "‚ùå ERROR: No parquet files were created!"
            exit 1
          fi

          echo "‚úÖ Found ${PARQUET_COUNT} parquet files:"
          ls -lh fantasy_football_data/*.parquet

          # Save file list
          echo "$PARQUET_FILES" > parquet_files.txt
          echo "file_count=${PARQUET_COUNT}" >> $GITHUB_OUTPUT

      - name: Diagnose parquet files
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Diagnostic: Parquet file columns"
          echo "========================================"
          python - <<'PYEOF'
          import pandas as pd
          from pathlib import Path

          data_dir = Path("fantasy_football_data")

          # Check canonical files
          canonical_files = ["player.parquet", "matchup.parquet", "draft.parquet", "transactions.parquet", "schedule.parquet"]

          for fname in canonical_files:
              fpath = data_dir / fname
              print(f"\n{'='*60}")
              print(f"File: {fname}")
              print('='*60)
              if fpath.exists():
                  try:
                      df = pd.read_parquet(fpath)
                      print(f"  Rows: {len(df):,}")
                      print(f"  Columns: {len(df.columns)}")

                      # Check for critical columns
                      critical = ["manager_week", "cumulative_week", "manager", "year", "week", "fantasy_points"]
                      for col in critical:
                          if col in df.columns:
                              non_null = df[col].notna().sum()
                              print(f"  ‚úì {col}: {non_null:,} non-null values")
                          else:
                              print(f"  ‚úó {col}: MISSING")

                      # Show first few column names
                      print(f"  Sample columns: {list(df.columns[:15])}...")
                  except Exception as e:
                      print(f"  ERROR reading file: {e}")
              else:
                  print(f"  FILE NOT FOUND")

          # Check subdirectories for intermediate files
          print(f"\n{'='*60}")
          print("Subdirectory file counts:")
          print('='*60)
          for subdir in ["player_data", "matchup_data", "draft_data", "transaction_data", "schedule_data"]:
              subpath = data_dir / subdir
              if subpath.exists():
                  files = list(subpath.glob("*.parquet"))
                  print(f"  {subdir}/: {len(files)} parquet files")
              else:
                  print(f"  {subdir}/: DIRECTORY NOT FOUND")
          PYEOF

      - name: Consolidate league settings to parquet
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Consolidating league settings to parquet"
          echo "========================================"
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import pandas as pd

          settings_dir = Path("fantasy_football_data/league_settings")
          output_file = Path("fantasy_football_data/league_settings.parquet")

          if not settings_dir.exists():
              print(f"‚ö†Ô∏è  No league_settings directory found at {settings_dir}")
              print("   Playoff odds will use default configuration")
              exit(0)

          settings_files = list(settings_dir.glob("league_settings_*.json"))

          if not settings_files:
              print(f"‚ö†Ô∏è  No league settings JSON files found in {settings_dir}")
              exit(0)

          print(f"Found {len(settings_files)} league settings files")

          rows = []
          for sf in sorted(settings_files):
              try:
                  with open(sf, 'r', encoding='utf-8') as f:
                      data = json.load(f)

                  # Extract key fields for playoff odds calculation
                  metadata = data.get('metadata', {})
                  row = {
                      'year': data.get('year'),
                      'league_key': data.get('league_key'),
                      'num_teams': metadata.get('num_teams'),
                      'num_playoff_teams': metadata.get('num_playoff_teams') or metadata.get('playoff_teams'),
                      'playoff_start_week': metadata.get('playoff_start_week'),
                      'uses_playoff_reseeding': metadata.get('uses_playoff_reseeding'),
                      'bye_teams': metadata.get('bye_teams'),
                      'current_week': metadata.get('current_week'),
                      'is_finished': metadata.get('is_finished'),
                      # Store full settings as JSON string for flexibility
                      'settings_json': json.dumps(data)
                  }
                  rows.append(row)
                  print(f"  ‚úì {sf.name}: year={row['year']}, playoff_teams={row['num_playoff_teams']}")
              except Exception as e:
                  print(f"  ‚úó {sf.name}: {e}")

          if rows:
              df = pd.DataFrame(rows)
              df.to_parquet(output_file, index=False)
              print(f"\n‚úÖ Created {output_file} with {len(rows)} years of settings")
          else:
              print("‚ö†Ô∏è  No valid settings to save")
          PYEOF

      - name: Save league context to parquet for MotherDuck
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Saving league context to parquet"
          echo "========================================"
          python - <<'PYEOF'
          import json
          import pandas as pd
          from pathlib import Path

          # Read the updated league_context.json (may have been updated by initial_import_v2.py with discovered league_ids)
          context_file = Path("league_context.json")
          if not context_file.exists():
              print("‚ö†Ô∏è league_context.json not found")
              exit(1)

          with open(context_file) as f:
              context = json.load(f)

          print(f"Saving context for: {context.get('league_name')}")
          print(f"  Years: {context.get('start_year')} - {context.get('end_year')}")
          print(f"  League IDs: {len(context.get('league_ids', {}))} years mapped")

          # Convert to a single-row DataFrame with JSON columns for complex fields
          row = {
              'league_id': context.get('league_id'),
              'league_name': context.get('league_name'),
              'game_code': context.get('game_code', 'nfl'),
              'start_year': context.get('start_year'),
              'end_year': context.get('end_year'),
              'num_teams': context.get('num_teams'),
              'playoff_teams': context.get('playoff_teams'),
              'regular_season_weeks': context.get('regular_season_weeks'),
              # Store complex objects as JSON strings
              'league_ids_json': json.dumps(context.get('league_ids', {})),
              'keeper_rules_json': json.dumps(context.get('keeper_rules')) if context.get('keeper_rules') else None,
              'manager_name_overrides_json': json.dumps(context.get('manager_name_overrides', {})),
              # Metadata
              'created_at': context.get('created_at'),
              'updated_at': context.get('updated_at'),
          }

          df = pd.DataFrame([row])

          # Save to fantasy_football_data so it gets uploaded with other parquets
          output_path = Path("fantasy_football_data/league_context.parquet")
          df.to_parquet(output_path, index=False)

          print(f"‚úÖ Saved {output_path}")
          print(f"   Columns: {list(df.columns)}")

          # Show the league_ids mapping
          if context.get('league_ids'):
              print(f"\n   League IDs mapping:")
              for year, lid in sorted(context.get('league_ids', {}).items()):
                  print(f"     {year}: {lid}")
          PYEOF

      - name: Upload to MotherDuck
        if: steps.import.outputs.exit_code == '0'
        id: motherduck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
          SEASON: ${{ steps.parse.outputs.season }}
        run: |
          # Update status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "uploading",
            "message": "Uploading to MotherDuck...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          if [ -z "$MOTHERDUCK_TOKEN" ]; then
            echo "‚ö†Ô∏è  WARNING: MOTHERDUCK_TOKEN not set!"
            echo "Cannot upload to MotherDuck. Please add MOTHERDUCK_TOKEN as a GitHub secret."
            exit 1
          fi

          echo "========================================"
          echo "Uploading to MotherDuck"
          echo "Database: ${DATABASE_NAME}"
          echo "========================================"

          # Use the motherduck_upload.py script
          cd fantasy_football_data
          python motherduck_upload.py "${DATABASE_NAME}" . 2>&1 | tee ../motherduck.log

          UPLOAD_EXIT=$?

          if [ $UPLOAD_EXIT -eq 0 ]; then
            echo "‚úÖ MotherDuck upload complete!"
            echo "üìä Database: ${DATABASE_NAME}"
            echo "üîó Access at: https://app.motherduck.com/"
            echo "upload_success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå MotherDuck upload failed!"
            tail -50 ../motherduck.log
            echo "upload_success=false" >> $GITHUB_OUTPUT
            exit $UPLOAD_EXIT
          fi

      - name: Merge external staging data
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
        run: |
          echo "========================================"
          echo "Checking for external staging data"
          echo "========================================"

          python - <<'PYEOF'
          import json
          import os
          import re
          import duckdb

          def _slug(s, prefix_if_digit="l"):
              """Consistent slug function matching Streamlit app."""
              x = re.sub(r"[^a-zA-Z0-9]+", "_", s.strip().lower()).strip("_")
              if re.match(r"^\d", x):
                  x = f"{prefix_if_digit}_{x}"
              return x[:63]

          # Check if has_external_data flag is set
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          has_external_data = league_data.get("has_external_data", False)

          if not has_external_data:
              print("‚ÑπÔ∏è No external data flag set - skipping staging merge")
              exit(0)

          # Use league_name and apply same slug as Streamlit upload
          league_name = league_data.get("league_name", "")
          db_name = _slug(league_name, "l")
          token = os.environ.get("MOTHERDUCK_TOKEN")

          if not token or not db_name:
              print("‚ö†Ô∏è Missing MOTHERDUCK_TOKEN or DATABASE_NAME")
              exit(0)

          print(f"üîÑ Merging staging data for database: {db_name}")
          print(f"   (from league_name: {league_name})")

          os.environ["MOTHERDUCK_TOKEN"] = token
          con = duckdb.connect("md:")

          # First, list all databases to verify our target exists
          print("\nüìã Checking available databases...")
          all_dbs = con.execute("SELECT DISTINCT catalog_name FROM information_schema.schemata WHERE catalog_name NOT IN ('memory', 'system', 'temp')").fetchall()
          print(f"   Available databases: {[d[0] for d in all_dbs]}")

          # USE the database to ensure correct context
          try:
              con.execute(f"USE {db_name}")
              print(f"   ‚úì Connected to database: {db_name}")
          except Exception as e:
              print(f"   ‚ùå Could not connect to database {db_name}: {e}")
              con.close()
              exit(0)

          # Check if staging schema exists
          try:
              staging_tables = con.execute(f"""
                  SELECT table_name
                  FROM information_schema.tables
                  WHERE table_catalog = '{db_name}'
                  AND table_schema = 'staging'
              """).fetchall()
              print(f"   Staging tables query returned: {staging_tables}")
          except Exception as e:
              print(f"   Staging tables query error: {e}")
              staging_tables = []

          if not staging_tables:
              print("‚ÑπÔ∏è No staging tables found - nothing to merge")
              # List all schemas/tables in the database for debugging
              try:
                  all_tables = con.execute(f"""
                      SELECT table_schema, table_name
                      FROM information_schema.tables
                      WHERE table_catalog = '{db_name}'
                  """).fetchall()
                  print(f"   All tables in {db_name}: {all_tables}")
              except:
                  pass
              con.close()
              exit(0)

          print(f"Found {len(staging_tables)} staging tables to merge")

          # Table mapping: staging name -> main table name
          table_map = {
              "staging_matchup": "matchup",
              "staging_player": "player",
              "staging_draft": "draft",
              "staging_transactions": "transactions",
              "staging_schedule": "schedule",
          }

          for (staging_name,) in staging_tables:
              main_table = table_map.get(staging_name, staging_name.replace("staging_", ""))
              print(f"  Merging {db_name}.staging.{staging_name} ‚Üí {db_name}.public.{main_table}")

              try:
                  # Check if main table exists
                  main_exists = con.execute(f"""
                      SELECT COUNT(*) FROM information_schema.tables
                      WHERE table_catalog = '{db_name}'
                      AND table_schema = 'public'
                      AND table_name = '{main_table}'
                  """).fetchone()[0] > 0

                  if main_exists:
                      # Get columns with types from both tables
                      staging_cols = con.execute(f"DESCRIBE {db_name}.staging.{staging_name}").fetchall()
                      main_cols = con.execute(f"DESCRIBE {db_name}.public.{main_table}").fetchall()

                      # Build type mappings: {col_name: col_type}
                      staging_col_types = {row[0]: row[1] for row in staging_cols}
                      main_col_types = {row[0]: row[1] for row in main_cols}

                      # Find common columns
                      common_cols = set(staging_col_types.keys()) & set(main_col_types.keys())

                      if common_cols:
                          # Build SELECT with type casting to handle empty strings and type mismatches
                          select_parts = []
                          for col in sorted(common_cols):
                              main_type = main_col_types[col].upper()
                              staging_type = staging_col_types[col].upper()

                              # Check if staging column is a string type (can have empty strings)
                              staging_is_string = 'VARCHAR' in staging_type or 'TEXT' in staging_type or 'CHAR' in staging_type or 'STRING' in staging_type
                              main_is_numeric = 'INT' in main_type or 'DOUBLE' in main_type or 'FLOAT' in main_type or 'DECIMAL' in main_type or 'BIGINT' in main_type

                              # If types match, just use column directly
                              if main_type == staging_type:
                                  select_parts.append(col)
                              # String ‚Üí Numeric: handle empty strings as NULL
                              elif staging_is_string and main_is_numeric:
                                  select_parts.append(f"CASE WHEN TRIM({col}) = '' OR {col} IS NULL THEN NULL ELSE TRY_CAST({col} AS {main_type}) END AS {col}")
                              # Numeric ‚Üí Numeric: just cast (no TRIM needed)
                              elif main_is_numeric:
                                  select_parts.append(f"TRY_CAST({col} AS {main_type}) AS {col}")
                              # Handle date/timestamp conversions
                              elif 'DATE' in main_type or 'TIME' in main_type:
                                  select_parts.append(f"TRY_CAST({col} AS {main_type}) AS {col}")
                              # Default: try cast with fallback
                              else:
                                  select_parts.append(f"TRY_CAST({col} AS {main_type}) AS {col}")

                          col_list = ", ".join(sorted(common_cols))
                          select_list = ", ".join(select_parts)

                          # Insert new rows with type-safe casting
                          con.execute(f"""
                              INSERT INTO {db_name}.public.{main_table} ({col_list})
                              SELECT {select_list} FROM {db_name}.staging.{staging_name}
                          """)
                          cnt = con.execute(f"SELECT COUNT(*) FROM {db_name}.staging.{staging_name}").fetchone()[0]
                          print(f"    ‚úì Merged {cnt} rows")
                      else:
                          print(f"    ‚ö†Ô∏è No common columns between staging and main table")
                  else:
                      # Main table doesn't exist - create it from staging
                      con.execute(f"""
                          CREATE TABLE {db_name}.public.{main_table} AS
                          SELECT * FROM {db_name}.staging.{staging_name}
                      """)
                      cnt = con.execute(f"SELECT COUNT(*) FROM {db_name}.public.{main_table}").fetchone()[0]
                      print(f"    ‚úì Created new table with {cnt} rows")

              except Exception as e:
                  print(f"    ‚ùå Error merging {staging_name}: {e}")

          # Clean up staging tables
          print("\nüßπ Cleaning up staging tables...")
          for (staging_name,) in staging_tables:
              try:
                  con.execute(f"DROP TABLE IF EXISTS {db_name}.staging.{staging_name}")
                  print(f"    ‚úì Dropped {staging_name}")
              except:
                  pass

          con.close()
          print("\n‚úÖ External data merge complete")
          PYEOF

      - name: Store credentials for weekly updates
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          CREDENTIAL_ENCRYPTION_KEY: ${{ secrets.CREDENTIAL_ENCRYPTION_KEY }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_ID: ${{ steps.parse.outputs.league_id }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
        run: |
          echo "========================================"
          echo "Storing credentials for weekly updates"
          echo "========================================"

          # Check if encryption key is available
          if [ -z "$CREDENTIAL_ENCRYPTION_KEY" ]; then
            echo "‚ö†Ô∏è CREDENTIAL_ENCRYPTION_KEY not set - weekly updates will require manual auth"
            echo "To enable automatic weekly updates, add CREDENTIAL_ENCRYPTION_KEY to GitHub secrets"
            echo "Generate one with: python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\""
            exit 0
          fi

          python - <<'PYEOF'
          import json
          import sys
          sys.path.insert(0, 'fantasy_football_data_scripts')

          from multi_league.utils.credential_store import store_league_credentials
          import os

          # Read the refresh token from the OAuth file we created earlier
          with open("oauth/Oauth.json", 'r') as f:
              oauth_data = json.load(f)

          refresh_token = oauth_data.get('refresh_token')
          if not refresh_token:
              print("‚ö†Ô∏è No refresh_token in OAuth data - weekly updates may not work")
              exit(0)

          database_name = os.environ['DATABASE_NAME']
          league_id = os.environ['LEAGUE_ID']
          league_name = os.environ['LEAGUE_NAME']

          success = store_league_credentials(
              database_name=database_name,
              refresh_token=refresh_token,
              league_id=league_id,
              league_name=league_name
          )

          if success:
              print(f"‚úÖ Credentials stored - weekly updates enabled for {league_name}")
          else:
              print("‚ö†Ô∏è Failed to store credentials - weekly updates may require manual auth")
          PYEOF

      - name: Confirm weekly updates enabled
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          # Weekly updates are auto-discovered from MotherDuck
          # Any database with a 'matchup' table will receive weekly updates
          echo "‚úÖ League '${{ steps.parse.outputs.database_name }}' uploaded to MotherDuck"
          echo "‚ÑπÔ∏è Weekly updates will be auto-discovered (database has matchup table)"

      - name: Create deployment manifest
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Create a deployment manifest with all the info needed to create the user's site
          manifest = {
              "user_id": "${{ steps.parse.outputs.user_id }}",
              "league_name": "${{ steps.parse.outputs.league_name }}",
              "league_id": "${{ steps.parse.outputs.league_id }}",
              "season": "${{ steps.parse.outputs.season }}",
              "motherduck_database": "${{ steps.parse.outputs.database_name }}",
              "parquet_file_count": ${{ steps.verify.outputs.file_count }},
              "created_at": os.popen('date -Iseconds').read().strip(),
              "workflow_run_id": "${{ github.run_id }}",
              "status": "ready",
              "weekly_updates_enabled": os.environ.get('CREDENTIAL_ENCRYPTION_KEY') is not None,
              "next_steps": [
                  "Create analytics_app-based UI repository",
                  "Configure MotherDuck connection",
                  "Deploy Streamlit site"
              ]
          }

          with open("deployment_manifest.json", 'w') as f:
              json.dump(manifest, f, indent=2)

          print("‚úÖ Deployment manifest created")
          print(json.dumps(manifest, indent=2))
          PYEOF

      - name: Cleanup on failure
        if: failure()
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
        run: |
          echo "========================================"
          echo "Cleaning up after failed import"
          echo "========================================"

          python - <<'PYEOF'
          import os
          import duckdb
          import re

          token = os.environ.get('MOTHERDUCK_TOKEN')
          db_name = os.environ.get('DATABASE_NAME', '')

          if not token or not db_name:
              print("‚ö†Ô∏è Missing token or database name - skipping cleanup")
              exit(0)

          def _slug(s, prefix_if_digit="l"):
              x = re.sub(r"[^a-zA-Z0-9]+", "_", s.strip().lower()).strip("_")
              if re.match(r"^\d", x):
                  x = f"{prefix_if_digit}_{x}"
              return x[:63]

          db = _slug(db_name, "l") if db_name else None

          try:
              con = duckdb.connect(f"md:?motherduck_token={token}")

              # Clean up staging tables if any exist
              try:
                  staging_tables = con.execute(f"""
                      SELECT table_name
                      FROM information_schema.tables
                      WHERE table_catalog = '{db}'
                      AND table_schema = 'staging'
                  """).fetchall()

                  if staging_tables:
                      print(f"üßπ Found {len(staging_tables)} staging tables to clean up")
                      for (table_name,) in staging_tables:
                          try:
                              con.execute(f"DROP TABLE IF EXISTS {db}.staging.{table_name}")
                              print(f"   ‚úì Dropped {table_name}")
                          except Exception as e:
                              print(f"   ‚úó Could not drop {table_name}: {e}")
                  else:
                      print("‚ÑπÔ∏è No staging tables to clean up")
              except Exception as e:
                  print(f"‚ÑπÔ∏è Could not check staging tables: {e}")

              con.close()
              print("‚úÖ Cleanup complete")
          except Exception as e:
              print(f"‚ö†Ô∏è Cleanup failed: {e}")
          PYEOF

      - name: Update job status - Complete
        if: always()
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
        run: |
          if [ "${{ steps.import.outputs.exit_code }}" == "0" ] && [ "${{ steps.motherduck.outputs.upload_success }}" == "true" ]; then
            STATUS="complete"
            MESSAGE="Import successful! Data uploaded to MotherDuck database: ${{ steps.parse.outputs.database_name }}"
          elif [ "${{ steps.import.outputs.exit_code }}" != "0" ]; then
            STATUS="failed"
            MESSAGE="Import failed during data collection phase"
          else
            STATUS="failed"
            MESSAGE="Import succeeded but MotherDuck upload failed"
          fi

          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "${STATUS}",
            "message": "${MESSAGE}",
            "parquet_files": ${{ steps.verify.outputs.file_count || 0 }},
            "motherduck_url": "https://app.motherduck.com/",
            "completed_at": "$(date -Iseconds)"
          }
          EOF

          # Update job status in MotherDuck
          python - <<'PYEOF'
          import os
          import duckdb
          from datetime import datetime

          token = os.environ.get('MOTHERDUCK_TOKEN')
          if not token:
              print("‚ö†Ô∏è No MOTHERDUCK_TOKEN - skipping job status update")
              exit(0)

          user_id = "${{ steps.parse.outputs.user_id }}"
          status = "${STATUS}"

          try:
              con = duckdb.connect(f"md:?motherduck_token={token}")
              now = datetime.utcnow()

              con.execute("""
                  UPDATE ops.import_jobs
                  SET status = ?, updated_at = ?
                  WHERE user_id = ?
              """, [status, now, user_id])

              con.close()
              print(f"‚úÖ Job status updated in MotherDuck: {status}")
          except Exception as e:
              print(f"‚ö†Ô∏è Could not update job status: {e}")
          PYEOF

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: league-import-${{ steps.parse.outputs.user_id }}
          path: |
            import.log
            motherduck.log
            job_status/
            deployment_manifest.json
            fantasy_football_data/*.parquet
            league_context.json
          retention-days: 30

      - name: Create workflow summary
        if: always()
        run: |
          echo "# League Import Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## League Information" >> $GITHUB_STEP_SUMMARY
          echo "- **League**: ${{ steps.parse.outputs.league_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Season**: ${{ steps.parse.outputs.season }}" >> $GITHUB_STEP_SUMMARY
          echo "- **User ID**: ${{ steps.parse.outputs.user_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **MotherDuck Database**: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.import.outputs.exit_code }}" == "0" ]; then
            echo "## ‚úÖ Import Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Parquet files created: ${{ steps.verify.outputs.file_count }}" >> $GITHUB_STEP_SUMMARY
            echo "- MotherDuck upload: ${{ steps.motherduck.outputs.upload_success == 'true' && '‚úÖ Success' || '‚ùå Failed' }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Files Created" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -lh fantasy_football_data/*.parquet 2>/dev/null || echo "No files found"
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. Data is now available in MotherDuck" >> $GITHUB_STEP_SUMMARY
            echo "2. Create custom analytics site based on analytics_app" >> $GITHUB_STEP_SUMMARY
            echo "3. Configure site to use database: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ùå Import Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the logs for details" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Trigger Playoff Odds Calculation
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "========================================"
          echo "Triggering Playoff Odds Calculator"
          echo "========================================"

          gh workflow run playoff_odds_worker.yml \
            -f database_name="${{ steps.parse.outputs.database_name }}" \
            -f n_sims="10000" \
            -f user_id="${{ steps.parse.outputs.user_id }}"

          if [ $? -eq 0 ]; then
            echo "‚úÖ Playoff odds workflow triggered successfully"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Playoff Odds" >> $GITHUB_STEP_SUMMARY
            echo "A separate workflow has been triggered to calculate playoff odds with 10,000 Monte Carlo simulations." >> $GITHUB_STEP_SUMMARY
            echo "This may take up to 3 hours to complete." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è Could not trigger playoff odds workflow"
          fi

      # Note: Issue creation removed due to permission requirements
      # Failures are logged in workflow summary and artifacts instead
