name: League Import Worker

# This workflow is triggered on-demand from Streamlit when a user requests
# to create their own fantasy football analytics site

on:
  workflow_dispatch:
    inputs:
      league_data_b64:
        description: 'Base64-encoded JSON with league configuration'
        required: false
        type: string
      league_data:
        description: 'DEPRECATED - raw JSON (for backward compat)'
        required: false
        type: string
      user_id:
        description: 'Unique identifier for this user/import job'
        required: true
        type: string

  repository_dispatch:
    types: [league_import]

# Permissions needed to trigger the playoff odds workflow
permissions:
  actions: write
  contents: read

jobs:
  import-league-data:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Allow up to 2 hours for full import

    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Parse league data
        id: parse
        env:
          YAHOO_CLIENT_ID: ${{ secrets.YAHOO_CLIENT_ID }}
          YAHOO_CLIENT_SECRET: ${{ secrets.YAHOO_CLIENT_SECRET }}
          LEAGUE_DATA_B64: ${{ github.event.inputs.league_data_b64 || github.event.client_payload.league_data_b64 || '' }}
          LEAGUE_DATA_RAW: ${{ github.event.inputs.league_data || github.event.client_payload.league_data || '' }}
          USER_ID_INPUT: ${{ github.event.inputs.user_id || github.event.client_payload.user_id }}
        run: |
          python - <<'PYEOF'
          import json
          import os
          import sys
          import base64

          # Get input data from environment (avoids shell escaping issues)
          league_data_b64 = os.environ.get('LEAGUE_DATA_B64', '').strip()
          league_data_raw = os.environ.get('LEAGUE_DATA_RAW', '').strip()
          user_id = os.environ.get('USER_ID_INPUT', '')

          print(f"[DEBUG] Received league_data_b64 length: {len(league_data_b64)}")
          print(f"[DEBUG] Received league_data_raw length: {len(league_data_raw)}")

          league_data = None

          # Try base64-encoded format first (new format)
          if league_data_b64:
              print("[DEBUG] Using base64-encoded format")
              print(f"[DEBUG] First 50 chars of base64: {league_data_b64[:50]}...")
              try:
                  league_data_json = base64.b64decode(league_data_b64).decode('utf-8')
                  league_data = json.loads(league_data_json)
                  print(f"âœ… Successfully decoded base64 league data")
              except Exception as e:
                  print(f"WARNING: Failed to decode base64: {e}")
                  league_data = None

          # Fallback to raw JSON format (old format - may be truncated!)
          if league_data is None and league_data_raw:
              print("[DEBUG] Falling back to raw JSON format (WARNING: may be truncated)")
              print(f"[DEBUG] First 100 chars of raw: {league_data_raw[:100]}...")
              try:
                  league_data = json.loads(league_data_raw)
                  print(f"âœ… Successfully parsed raw JSON")
              except Exception as e:
                  print(f"ERROR: Failed to parse raw JSON: {e}")
                  league_data = None

          if league_data is None:
              print("ERROR: No valid league data provided (neither base64 nor raw JSON)")
              print("Make sure your Streamlit app is sending league_data_b64")
              sys.exit(1)

          print(f"[DEBUG] Decoded league_id: {league_data.get('league_id')}")
          print(f"[DEBUG] Decoded league_name: {league_data.get('league_name')}")
          print(f"[DEBUG] Decoded season: {league_data.get('season')}")
          print(f"[DEBUG] Decoded start_year: {league_data.get('start_year')}")

          # Extract league configuration
          league_id = league_data.get('league_id', '')
          league_name = league_data.get('league_name', 'Unknown League')
          season = league_data.get('season', league_data.get('end_year', 2024))
          start_year = league_data.get('start_year', season)
          oauth_raw = league_data.get('oauth_token', {})

          # Add Yahoo app credentials from secrets (required for yahoo_oauth library)
          consumer_key = os.environ.get('YAHOO_CLIENT_ID')
          consumer_secret = os.environ.get('YAHOO_CLIENT_SECRET')

          if not consumer_key or not consumer_secret:
              print("âš ï¸  WARNING: YAHOO_CLIENT_ID and YAHOO_CLIENT_SECRET not set")
              print("   Import will likely fail - add these to GitHub secrets")

          # Build oauth_data in the format yahoo_oauth library expects
          # The library checks for token_time to determine if refresh is needed
          import time
          oauth_data = {
              'access_token': oauth_raw.get('access_token'),
              'refresh_token': oauth_raw.get('refresh_token'),
              'consumer_key': consumer_key,
              'consumer_secret': consumer_secret,
              'token_type': oauth_raw.get('token_type', 'bearer'),
              'expires_in': oauth_raw.get('expires_in', 3600),
              # token_time is CRITICAL - yahoo_oauth uses this to check expiration
              # Use the provided token_time or set to now (token will try refresh)
              'token_time': oauth_raw.get('token_time', time.time()),
              'guid': oauth_raw.get('xoauth_yahoo_guid') or oauth_raw.get('guid'),
          }

          print(f"âœ… Built OAuth data with consumer credentials")
          print(f"   access_token present: {bool(oauth_data.get('access_token'))}")
          print(f"   refresh_token present: {bool(oauth_data.get('refresh_token'))}")
          print(f"   consumer_key present: {bool(oauth_data.get('consumer_key'))}")
          print(f"   token_time: {oauth_data.get('token_time')}")

          # Create sanitized database name (just league name, no year - data contains all historical years)
          db_name = league_name.lower().replace(' ', '_').replace('-', '_')
          db_name = ''.join(c if c.isalnum() or c == '_' else '' for c in db_name)
          # Add 'l_' prefix if name starts with a digit (SQL identifiers can't start with digits)
          if db_name and db_name[0].isdigit():
              db_name = f"l_{db_name}"

          # Add collision prevention: if league_id looks like "449.l.123456", extract the unique part
          # This helps prevent two leagues with the same name from colliding
          import hashlib
          if league_id:
              # Create a short hash of the league_id to make names unique
              league_id_hash = hashlib.md5(league_id.encode()).hexdigest()[:6]
              # We'll pass both - main.py can check for collisions
              db_name_with_hash = f"{db_name}_{league_id_hash}"
          else:
              db_name_with_hash = db_name

          print(f"   Database name: {db_name}")
          print(f"   Database name (with hash): {db_name_with_hash}")

          # Check for database name collision and resolve if needed
          final_db_name = db_name
          try:
              import duckdb
              md_token = os.environ.get('MOTHERDUCK_TOKEN')
              if md_token:
                  con = duckdb.connect(f"md:?motherduck_token={md_token}")
                  # Check if this database already exists
                  existing = con.execute(f"""
                      SELECT COUNT(*) FROM information_schema.schemata
                      WHERE catalog_name = '{db_name}'
                  """).fetchone()[0]

                  if existing > 0:
                      # Database exists - check if it's for the same league
                      try:
                          ctx_result = con.execute(f"""
                              SELECT league_id FROM {db_name}.public.league_context LIMIT 1
                          """).fetchone()
                          existing_league_id = ctx_result[0] if ctx_result else None

                          if existing_league_id and existing_league_id != league_id:
                              # Different league! Use hashed name
                              print(f"âš ï¸ Database '{db_name}' exists for different league ({existing_league_id})")
                              print(f"   Using unique name: {db_name_with_hash}")
                              final_db_name = db_name_with_hash
                          else:
                              print(f"âœ“ Database '{db_name}' exists for same league - will update")
                      except:
                          # No context table - assume it's the same league (legacy data)
                          print(f"âœ“ Database '{db_name}' exists (no context table) - will update")
                  else:
                      print(f"âœ“ Database '{db_name}' is new")
                  con.close()
          except Exception as e:
              print(f"âš ï¸ Could not check for collision: {e}")
              # If we can't check, use the base name

          # Set privacy flag if requested (using final_db_name after collision resolution)
          is_private = league_data.get("is_private", False)
          if is_private:
              try:
                  md_token = os.environ.get('MOTHERDUCK_TOKEN')
                  if md_token:
                      con = duckdb.connect(f"md:?motherduck_token={md_token}")
                      con.execute("""
                          CREATE TABLE IF NOT EXISTS ops.league_settings (
                              db_name TEXT PRIMARY KEY,
                              is_private BOOLEAN DEFAULT false,
                              created_at TIMESTAMP DEFAULT now(),
                              updated_at TIMESTAMP DEFAULT now()
                          )
                      """)
                      con.execute("""
                          INSERT INTO ops.league_settings (db_name, is_private, updated_at)
                          VALUES (?, true, now())
                          ON CONFLICT (db_name) DO UPDATE SET
                              is_private = true,
                              updated_at = now()
                      """, [final_db_name])
                      con.close()
                      print(f"âœ“ League marked as private: {final_db_name}")
              except Exception as e:
                  print(f"âš ï¸ Could not set privacy flag: {e}")

          # Set outputs for next steps
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"user_id={user_id}\n")
              f.write(f"league_id={league_id}\n")
              f.write(f"league_name={league_name}\n")
              f.write(f"season={season}\n")
              f.write(f"start_year={start_year}\n")
              f.write(f"database_name={final_db_name}\n")

          # Save OAuth token to file (now includes consumer credentials)
          os.makedirs("oauth", exist_ok=True)
          with open("oauth/Oauth.json", 'w') as f:
              json.dump(oauth_data, f, indent=2)

          # Save full league data for context creation
          with open("league_data_input.json", 'w') as f:
              json.dump(league_data, f, indent=2)

          print(f"âœ… Parsed league: {league_name} ({season})")
          print(f"   League ID: {league_id}")
          print(f"   Database: {db_name}")
          print(f"   User ID: {user_id}")
          PYEOF

      - name: Update job status - Starting
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p job_status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "running",
            "phase": "starting",
            "message": "Initializing import...",
            "started_at": "$(date -Iseconds)",
            "workflow_run_id": "${{ github.run_id }}"
          }
          EOF

          # Write job tracking to MotherDuck so Streamlit can find the run_id
          python - <<'PYEOF'
          import os
          import duckdb
          from datetime import datetime

          token = os.environ.get('MOTHERDUCK_TOKEN')
          if not token:
              print("âš ï¸ No MOTHERDUCK_TOKEN - skipping job tracking")
              exit(0)

          user_id = "${{ steps.parse.outputs.user_id }}"
          league_name = "${{ steps.parse.outputs.league_name }}"
          run_id = "${{ github.run_id }}"

          try:
              os.environ['MOTHERDUCK_TOKEN'] = token
              con = duckdb.connect("md:")

              # Create ops database and table if needed
              con.execute("CREATE DATABASE IF NOT EXISTS ops")
              con.execute("""
                  CREATE TABLE IF NOT EXISTS ops.import_jobs (
                      user_id TEXT PRIMARY KEY,
                      workflow_run_id TEXT,
                      league_name TEXT,
                      status TEXT,
                      started_at TIMESTAMP,
                      updated_at TIMESTAMP
                  )
              """)

              # Insert or update job record
              now = datetime.utcnow()
              con.execute("""
                  INSERT OR REPLACE INTO ops.import_jobs
                  (user_id, workflow_run_id, league_name, status, started_at, updated_at)
                  VALUES (?, ?, ?, 'running', ?, ?)
              """, [user_id, run_id, league_name, now, now])

              con.close()
              print(f"âœ… Job tracking saved: user_id={user_id}, run_id={run_id}")
          except Exception as e:
              print(f"âš ï¸ Could not save job tracking: {e}")
          PYEOF

      - name: Refresh OAuth token
        env:
          YAHOO_CLIENT_ID: ${{ secrets.YAHOO_CLIENT_ID }}
          YAHOO_CLIENT_SECRET: ${{ secrets.YAHOO_CLIENT_SECRET }}
        run: |
          echo "========================================"
          echo "Refreshing OAuth token"
          echo "========================================"
          python - <<'PYEOF'
          import json
          import os
          import time
          import requests

          # Read current OAuth data
          with open("oauth/Oauth.json", 'r') as f:
              oauth_data = json.load(f)

          refresh_token = oauth_data.get('refresh_token')
          if not refresh_token:
              print("âš ï¸ No refresh_token found - will try with existing access_token")
              exit(0)

          consumer_key = os.environ.get('YAHOO_CLIENT_ID')
          consumer_secret = os.environ.get('YAHOO_CLIENT_SECRET')

          if not consumer_key or not consumer_secret:
              print("âš ï¸ Missing Yahoo credentials - cannot refresh token")
              exit(0)

          print(f"Refreshing token using refresh_token...")

          # Yahoo OAuth token refresh endpoint
          token_url = "https://api.login.yahoo.com/oauth2/get_token"

          try:
              response = requests.post(
                  token_url,
                  data={
                      'grant_type': 'refresh_token',
                      'refresh_token': refresh_token,
                      'client_id': consumer_key,
                      'client_secret': consumer_secret,
                  },
                  headers={'Content-Type': 'application/x-www-form-urlencoded'},
                  timeout=30
              )

              if response.status_code == 200:
                  new_tokens = response.json()

                  # Update OAuth data with fresh tokens
                  oauth_data['access_token'] = new_tokens.get('access_token')
                  oauth_data['refresh_token'] = new_tokens.get('refresh_token', refresh_token)  # May return new refresh_token
                  oauth_data['token_time'] = time.time()  # Reset token_time to NOW
                  oauth_data['expires_in'] = new_tokens.get('expires_in', 3600)

                  # Save updated OAuth data
                  with open("oauth/Oauth.json", 'w') as f:
                      json.dump(oauth_data, f, indent=2)

                  print(f"âœ… Token refreshed successfully!")
                  print(f"   New token_time: {oauth_data['token_time']}")
                  print(f"   Expires in: {oauth_data['expires_in']} seconds")
              else:
                  print(f"âš ï¸ Token refresh failed: {response.status_code}")
                  print(f"   Response: {response.text[:500]}")
                  print("   Will try with existing access_token (may fail if expired)")

          except Exception as e:
              print(f"âš ï¸ Token refresh error: {e}")
              print("   Will try with existing access_token (may fail if expired)")
          PYEOF

      - name: Create league context
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Load the league data
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          # Get absolute path to OAuth file (script runs from fantasy_football_data_scripts subdirectory)
          oauth_file = Path("oauth/Oauth.json").resolve()

          # Ensure numeric values are integers (not strings from JSON)
          start_year = int(league_data.get('start_year', 2014))
          end_year = int(league_data.get('season', league_data.get('end_year', 2024)))
          num_teams = int(league_data.get('num_teams', 10))
          playoff_teams = int(league_data.get('playoff_teams', 6))
          regular_season_weeks = int(league_data.get('regular_season_weeks', 14))

          # Create league context file for initial_import_v2.py
          context = {
              "league_id": league_data.get('league_id'),
              "league_name": league_data.get('league_name'),
              "oauth_file_path": str(oauth_file),  # Use absolute path
              "game_code": league_data.get('game_code', 'nfl'),
              "start_year": start_year,
              "end_year": end_year,
              "num_teams": num_teams,
              "playoff_teams": playoff_teams,
              "regular_season_weeks": regular_season_weeks,
              "data_directory": str(Path("fantasy_football_data").resolve()),  # Absolute path
              "max_workers": 5,
              "enable_caching": True,
              "rate_limit_per_sec": 4.0,
              "manager_name_overrides": league_data.get('manager_name_overrides', {}),
              "keeper_rules": league_data.get('keeper_rules'),  # Pass keeper rules for economics calculation
              "has_external_data": league_data.get('has_external_data', False),  # Flag for staging merge
          }

          # Write context file
          with open("league_context.json", 'w') as f:
              json.dump(context, f, indent=2)

          print("âœ… League context created")
          print(json.dumps(context, indent=2))
          PYEOF

      - name: Create data directory
        run: |
          mkdir -p fantasy_football_data
          mkdir -p fantasy_football_data/matchup_data
          mkdir -p fantasy_football_data/player_data
          mkdir -p fantasy_football_data/draft_data
          mkdir -p fantasy_football_data/transaction_data
          mkdir -p fantasy_football_data/schedule_data
          echo "âœ… Data directories ready"

      - name: Download external staging data for transformations
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
        run: |
          echo "========================================"
          echo "Checking for external staging data to include in transformations"
          echo "========================================"

          python - <<'PYEOF'
          import os
          import re
          import json

          def _slug(s, prefix_if_digit="l"):
              x = re.sub(r"[^a-zA-Z0-9]+", "_", s.strip().lower()).strip("_")
              if re.match(r"^\d", x):
                  x = f"{prefix_if_digit}_{x}"
              return x[:63]

          # Check if has_external_data flag is set
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          has_external_data = league_data.get("has_external_data", False)

          if not has_external_data:
              print("â„¹ï¸ No external data flag - skipping staging download")
              exit(0)

          token = os.environ.get("MOTHERDUCK_TOKEN")
          league_name = league_data.get("league_name", "")
          db_name = _slug(league_name, "l")

          if not token or not db_name:
              print("âš ï¸ Missing token or database name")
              exit(0)

          print(f"ðŸ” Checking for staging data in {db_name}...")

          try:
              import duckdb
              os.environ["MOTHERDUCK_TOKEN"] = token
              con = duckdb.connect("md:")

              # Check for staging tables
              staging_tables = con.execute(f"""
                  SELECT table_name
                  FROM information_schema.tables
                  WHERE table_catalog = '{db_name}'
                  AND table_schema = 'staging'
              """).fetchall()

              if not staging_tables:
                  print("â„¹ï¸ No staging tables found")
                  con.close()
                  exit(0)

              print(f"Found {len(staging_tables)} staging tables")

              # Map staging tables to output directories
              table_dirs = {
                  "staging_matchup": "fantasy_football_data/matchup_data",
                  "staging_player": "fantasy_football_data/player_data",
                  "staging_draft": "fantasy_football_data/draft_data",
                  "staging_transactions": "fantasy_football_data/transaction_data",
                  "staging_schedule": "fantasy_football_data/schedule_data",
              }

              for (staging_name,) in staging_tables:
                  output_dir = table_dirs.get(staging_name)
                  if not output_dir:
                      print(f"  âš ï¸ Unknown staging table: {staging_name}")
                      continue

                  # Get data from staging
                  df = con.execute(f"SELECT * FROM {db_name}.staging.{staging_name}").fetchdf()

                  if df.empty:
                      print(f"  âš ï¸ {staging_name} is empty")
                      continue

                  # Get unique years
                  if 'year' in df.columns:
                      years = df['year'].dropna().unique()
                      for year in years:
                          year_df = df[df['year'] == year]
                          year_int = int(year)

                          # Determine output filename based on table type
                          if staging_name == "staging_matchup":
                              filename = f"matchup_data_week_all_year_{year_int}.parquet"
                          elif staging_name == "staging_player":
                              filename = f"player_stats_{year_int}_all_weeks.parquet"
                          elif staging_name == "staging_draft":
                              filename = f"draft_data_{year_int}.parquet"
                          elif staging_name == "staging_transactions":
                              filename = f"transactions_year_{year_int}.parquet"
                          elif staging_name == "staging_schedule":
                              filename = f"schedule_data_year_{year_int}.parquet"
                          else:
                              filename = f"{staging_name}_{year_int}.parquet"

                          output_path = f"{output_dir}/{filename}"
                          year_df.to_parquet(output_path, index=False)
                          print(f"  âœ“ {staging_name} year {year_int}: {len(year_df)} rows â†’ {output_path}")
                  else:
                      # No year column - save as single file
                      output_path = f"{output_dir}/{staging_name.replace('staging_', '')}_external.parquet"
                      df.to_parquet(output_path, index=False)
                      print(f"  âœ“ {staging_name}: {len(df)} rows â†’ {output_path}")

              con.close()
              print("âœ… External staging data downloaded for transformation processing")

          except Exception as e:
              print(f"âš ï¸ Error downloading staging data: {e}")
              print("   Import will continue with Yahoo data only")
          PYEOF

      - name: Run initial import
        id: import
        env:
          AUTO_CONFIRM: "1"
        working-directory: fantasy_football_data_scripts
        run: |
          echo "========================================"
          echo "Starting import for ${{ steps.parse.outputs.league_name }}"
          echo "Season: ${{ steps.parse.outputs.season }}"
          echo "User ID: ${{ steps.parse.outputs.user_id }}"
          echo "========================================"

          # Update status
          cat > ../job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "importing",
            "message": "Running initial_import_v2.py...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          # Run the import
          python initial_import_v2.py --context ../league_context.json 2>&1 | tee ../import.log

          # Capture exit code
          EXIT_CODE=${PIPESTATUS[0]}
          echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT

          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… Import completed successfully"
          else
            echo "âŒ Import failed with exit code ${EXIT_CODE}"
            echo "Last 100 lines of log:"
            tail -100 ../import.log
            exit $EXIT_CODE
          fi

      - name: Verify parquet files created
        if: steps.import.outputs.exit_code == '0'
        id: verify
        run: |
          echo "Verifying parquet files..."

          if [ ! -d "fantasy_football_data" ]; then
            echo "âŒ ERROR: Data directory not found!"
            exit 1
          fi

          PARQUET_FILES=$(find fantasy_football_data -name "*.parquet" -type f)
          PARQUET_COUNT=$(echo "$PARQUET_FILES" | grep -c ".parquet" || echo "0")

          if [ "$PARQUET_COUNT" -eq 0 ]; then
            echo "âŒ ERROR: No parquet files were created!"
            exit 1
          fi

          echo "âœ… Found ${PARQUET_COUNT} parquet files:"
          ls -lh fantasy_football_data/*.parquet

          # Save file list
          echo "$PARQUET_FILES" > parquet_files.txt
          echo "file_count=${PARQUET_COUNT}" >> $GITHUB_OUTPUT

      - name: Diagnose parquet files
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Diagnostic: Parquet file columns"
          echo "========================================"
          python - <<'PYEOF'
          import pandas as pd
          from pathlib import Path

          data_dir = Path("fantasy_football_data")

          # Check canonical files
          canonical_files = ["player.parquet", "matchup.parquet", "draft.parquet", "transactions.parquet", "schedule.parquet"]

          for fname in canonical_files:
              fpath = data_dir / fname
              print(f"\n{'='*60}")
              print(f"File: {fname}")
              print('='*60)
              if fpath.exists():
                  try:
                      df = pd.read_parquet(fpath)
                      print(f"  Rows: {len(df):,}")
                      print(f"  Columns: {len(df.columns)}")

                      # Check for critical columns
                      critical = ["manager_week", "cumulative_week", "manager", "year", "week", "fantasy_points"]
                      for col in critical:
                          if col in df.columns:
                              non_null = df[col].notna().sum()
                              print(f"  âœ“ {col}: {non_null:,} non-null values")
                          else:
                              print(f"  âœ— {col}: MISSING")

                      # Show first few column names
                      print(f"  Sample columns: {list(df.columns[:15])}...")
                  except Exception as e:
                      print(f"  ERROR reading file: {e}")
              else:
                  print(f"  FILE NOT FOUND")

          # Check subdirectories for intermediate files
          print(f"\n{'='*60}")
          print("Subdirectory file counts:")
          print('='*60)
          for subdir in ["player_data", "matchup_data", "draft_data", "transaction_data", "schedule_data"]:
              subpath = data_dir / subdir
              if subpath.exists():
                  files = list(subpath.glob("*.parquet"))
                  print(f"  {subdir}/: {len(files)} parquet files")
              else:
                  print(f"  {subdir}/: DIRECTORY NOT FOUND")
          PYEOF

      - name: Consolidate league settings to parquet
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Consolidating league settings to parquet"
          echo "========================================"
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import pandas as pd

          settings_dir = Path("fantasy_football_data/league_settings")
          output_file = Path("fantasy_football_data/league_settings.parquet")

          if not settings_dir.exists():
              print(f"âš ï¸  No league_settings directory found at {settings_dir}")
              print("   Playoff odds will use default configuration")
              exit(0)

          settings_files = list(settings_dir.glob("league_settings_*.json"))

          if not settings_files:
              print(f"âš ï¸  No league settings JSON files found in {settings_dir}")
              exit(0)

          print(f"Found {len(settings_files)} league settings files")

          rows = []
          for sf in sorted(settings_files):
              try:
                  with open(sf, 'r', encoding='utf-8') as f:
                      data = json.load(f)

                  # Extract key fields for playoff odds calculation
                  metadata = data.get('metadata', {})
                  row = {
                      'year': data.get('year'),
                      'league_key': data.get('league_key'),
                      'num_teams': metadata.get('num_teams'),
                      'num_playoff_teams': metadata.get('num_playoff_teams') or metadata.get('playoff_teams'),
                      'playoff_start_week': metadata.get('playoff_start_week'),
                      'uses_playoff_reseeding': metadata.get('uses_playoff_reseeding'),
                      'bye_teams': metadata.get('bye_teams'),
                      'current_week': metadata.get('current_week'),
                      'is_finished': metadata.get('is_finished'),
                      # Store full settings as JSON string for flexibility
                      'settings_json': json.dumps(data)
                  }
                  rows.append(row)
                  print(f"  âœ“ {sf.name}: year={row['year']}, playoff_teams={row['num_playoff_teams']}")
              except Exception as e:
                  print(f"  âœ— {sf.name}: {e}")

          if rows:
              df = pd.DataFrame(rows)
              df.to_parquet(output_file, index=False)
              print(f"\nâœ… Created {output_file} with {len(rows)} years of settings")
          else:
              print("âš ï¸  No valid settings to save")
          PYEOF

      - name: Save league context to parquet for MotherDuck
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Saving league context to parquet"
          echo "========================================"
          python - <<'PYEOF'
          import json
          import pandas as pd
          from pathlib import Path

          # Read the updated league_context.json (may have been updated by initial_import_v2.py with discovered league_ids)
          context_file = Path("league_context.json")
          if not context_file.exists():
              print("âš ï¸ league_context.json not found")
              exit(1)

          with open(context_file) as f:
              context = json.load(f)

          print(f"Saving context for: {context.get('league_name')}")
          print(f"  Years: {context.get('start_year')} - {context.get('end_year')}")
          print(f"  League IDs: {len(context.get('league_ids', {}))} years mapped")

          # Convert to a single-row DataFrame with JSON columns for complex fields
          row = {
              'league_id': context.get('league_id'),
              'league_name': context.get('league_name'),
              'game_code': context.get('game_code', 'nfl'),
              'start_year': context.get('start_year'),
              'end_year': context.get('end_year'),
              'num_teams': context.get('num_teams'),
              'playoff_teams': context.get('playoff_teams'),
              'regular_season_weeks': context.get('regular_season_weeks'),
              # Store complex objects as JSON strings
              'league_ids_json': json.dumps(context.get('league_ids', {})),
              'keeper_rules_json': json.dumps(context.get('keeper_rules')) if context.get('keeper_rules') else None,
              'manager_name_overrides_json': json.dumps(context.get('manager_name_overrides', {})),
              # Metadata
              'created_at': context.get('created_at'),
              'updated_at': context.get('updated_at'),
          }

          df = pd.DataFrame([row])

          # Save to fantasy_football_data so it gets uploaded with other parquets
          output_path = Path("fantasy_football_data/league_context.parquet")
          df.to_parquet(output_path, index=False)

          print(f"âœ… Saved {output_path}")
          print(f"   Columns: {list(df.columns)}")

          # Show the league_ids mapping
          if context.get('league_ids'):
              print(f"\n   League IDs mapping:")
              for year, lid in sorted(context.get('league_ids', {}).items()):
                  print(f"     {year}: {lid}")
          PYEOF

      - name: Upload to MotherDuck
        if: steps.import.outputs.exit_code == '0'
        id: motherduck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
          SEASON: ${{ steps.parse.outputs.season }}
        run: |
          # Update status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "uploading",
            "message": "Uploading to MotherDuck...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          if [ -z "$MOTHERDUCK_TOKEN" ]; then
            echo "âš ï¸  WARNING: MOTHERDUCK_TOKEN not set!"
            echo "Cannot upload to MotherDuck. Please add MOTHERDUCK_TOKEN as a GitHub secret."
            exit 1
          fi

          echo "========================================"
          echo "Uploading to MotherDuck"
          echo "Database: ${DATABASE_NAME}"
          echo "========================================"

          # Use the motherduck_upload.py script
          cd fantasy_football_data
          python motherduck_upload.py "${DATABASE_NAME}" . 2>&1 | tee ../motherduck.log

          UPLOAD_EXIT=$?

          if [ $UPLOAD_EXIT -eq 0 ]; then
            echo "âœ… MotherDuck upload complete!"
            echo "ðŸ“Š Database: ${DATABASE_NAME}"
            echo "ðŸ”— Access at: https://app.motherduck.com/"
            echo "upload_success=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ MotherDuck upload failed!"
            tail -50 ../motherduck.log
            echo "upload_success=false" >> $GITHUB_OUTPUT
            exit $UPLOAD_EXIT
          fi

      - name: Cleanup external staging data
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
        run: |
          echo "========================================"
          echo "Cleaning up staging tables"
          echo "========================================"

          # External data was already downloaded and processed during transformations
          # Now we just clean up the staging tables in MotherDuck

          python - <<'PYEOF'
          import json
          import os
          import re
          import duckdb

          def _slug(s, prefix_if_digit="l"):
              x = re.sub(r"[^a-zA-Z0-9]+", "_", s.strip().lower()).strip("_")
              if re.match(r"^\d", x):
                  x = f"{prefix_if_digit}_{x}"
              return x[:63]

          # Check if has_external_data flag is set
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          has_external_data = league_data.get("has_external_data", False)

          if not has_external_data:
              print("â„¹ï¸ No external data - no staging cleanup needed")
              exit(0)

          league_name = league_data.get("league_name", "")
          db_name = _slug(league_name, "l")
          token = os.environ.get("MOTHERDUCK_TOKEN")

          if not token or not db_name:
              print("âš ï¸ Missing token or database name")
              exit(0)

          print(f"ðŸ§¹ Cleaning up staging tables for: {db_name}")

          try:
              os.environ["MOTHERDUCK_TOKEN"] = token
              con = duckdb.connect("md:")

              # Check for staging tables
              staging_tables = con.execute(f"""
                  SELECT table_name
                  FROM information_schema.tables
                  WHERE table_catalog = '{db_name}'
                  AND table_schema = 'staging'
              """).fetchall()

              if not staging_tables:
                  print("â„¹ï¸ No staging tables to clean up")
                  con.close()
                  exit(0)

              print(f"Found {len(staging_tables)} staging tables to clean up")

              for (staging_name,) in staging_tables:
                  try:
                      con.execute(f"DROP TABLE IF EXISTS {db_name}.staging.{staging_name}")
                      print(f"  âœ“ Dropped {staging_name}")
                  except Exception as e:
                      print(f"  âš ï¸ Could not drop {staging_name}: {e}")

              con.close()
              print("âœ… Staging cleanup complete")

          except Exception as e:
              print(f"âš ï¸ Staging cleanup error: {e}")
          PYEOF

      - name: Store credentials for weekly updates
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          CREDENTIAL_ENCRYPTION_KEY: ${{ secrets.CREDENTIAL_ENCRYPTION_KEY }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_ID: ${{ steps.parse.outputs.league_id }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
        run: |
          echo "========================================"
          echo "Storing credentials for weekly updates"
          echo "========================================"

          # Check if encryption key is available
          if [ -z "$CREDENTIAL_ENCRYPTION_KEY" ]; then
            echo "âš ï¸ CREDENTIAL_ENCRYPTION_KEY not set - weekly updates will require manual auth"
            echo "To enable automatic weekly updates, add CREDENTIAL_ENCRYPTION_KEY to GitHub secrets"
            echo "Generate one with: python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\""
            exit 0
          fi

          python - <<'PYEOF'
          import json
          import sys
          sys.path.insert(0, 'fantasy_football_data_scripts')

          from multi_league.utils.credential_store import store_league_credentials
          import os

          # Read the refresh token from the OAuth file we created earlier
          with open("oauth/Oauth.json", 'r') as f:
              oauth_data = json.load(f)

          refresh_token = oauth_data.get('refresh_token')
          if not refresh_token:
              print("âš ï¸ No refresh_token in OAuth data - weekly updates may not work")
              exit(0)

          database_name = os.environ['DATABASE_NAME']
          league_id = os.environ['LEAGUE_ID']
          league_name = os.environ['LEAGUE_NAME']

          success = store_league_credentials(
              database_name=database_name,
              refresh_token=refresh_token,
              league_id=league_id,
              league_name=league_name
          )

          if success:
              print(f"âœ… Credentials stored - weekly updates enabled for {league_name}")
          else:
              print("âš ï¸ Failed to store credentials - weekly updates may require manual auth")
          PYEOF

      - name: Confirm weekly updates enabled
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          # Weekly updates are auto-discovered from MotherDuck
          # Any database with a 'matchup' table will receive weekly updates
          echo "âœ… League '${{ steps.parse.outputs.database_name }}' uploaded to MotherDuck"
          echo "â„¹ï¸ Weekly updates will be auto-discovered (database has matchup table)"

      - name: Create deployment manifest
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Create a deployment manifest with all the info needed to create the user's site
          manifest = {
              "user_id": "${{ steps.parse.outputs.user_id }}",
              "league_name": "${{ steps.parse.outputs.league_name }}",
              "league_id": "${{ steps.parse.outputs.league_id }}",
              "season": "${{ steps.parse.outputs.season }}",
              "motherduck_database": "${{ steps.parse.outputs.database_name }}",
              "parquet_file_count": ${{ steps.verify.outputs.file_count }},
              "created_at": os.popen('date -Iseconds').read().strip(),
              "workflow_run_id": "${{ github.run_id }}",
              "status": "ready",
              "weekly_updates_enabled": os.environ.get('CREDENTIAL_ENCRYPTION_KEY') is not None,
              "next_steps": [
                  "Create analytics_app-based UI repository",
                  "Configure MotherDuck connection",
                  "Deploy Streamlit site"
              ]
          }

          with open("deployment_manifest.json", 'w') as f:
              json.dump(manifest, f, indent=2)

          print("âœ… Deployment manifest created")
          print(json.dumps(manifest, indent=2))
          PYEOF

      - name: Cleanup on failure
        if: failure()
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
        run: |
          echo "========================================"
          echo "Cleaning up after failed import"
          echo "========================================"

          python - <<'PYEOF'
          import os
          import duckdb
          import re

          token = os.environ.get('MOTHERDUCK_TOKEN')
          db_name = os.environ.get('DATABASE_NAME', '')

          if not token or not db_name:
              print("âš ï¸ Missing token or database name - skipping cleanup")
              exit(0)

          def _slug(s, prefix_if_digit="l"):
              x = re.sub(r"[^a-zA-Z0-9]+", "_", s.strip().lower()).strip("_")
              if re.match(r"^\d", x):
                  x = f"{prefix_if_digit}_{x}"
              return x[:63]

          db = _slug(db_name, "l") if db_name else None

          try:
              con = duckdb.connect(f"md:?motherduck_token={token}")

              # Clean up staging tables if any exist
              try:
                  staging_tables = con.execute(f"""
                      SELECT table_name
                      FROM information_schema.tables
                      WHERE table_catalog = '{db}'
                      AND table_schema = 'staging'
                  """).fetchall()

                  if staging_tables:
                      print(f"ðŸ§¹ Found {len(staging_tables)} staging tables to clean up")
                      for (table_name,) in staging_tables:
                          try:
                              con.execute(f"DROP TABLE IF EXISTS {db}.staging.{table_name}")
                              print(f"   âœ“ Dropped {table_name}")
                          except Exception as e:
                              print(f"   âœ— Could not drop {table_name}: {e}")
                  else:
                      print("â„¹ï¸ No staging tables to clean up")
              except Exception as e:
                  print(f"â„¹ï¸ Could not check staging tables: {e}")

              con.close()
              print("âœ… Cleanup complete")
          except Exception as e:
              print(f"âš ï¸ Cleanup failed: {e}")
          PYEOF

      - name: Update job status - Complete
        if: always()
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
        run: |
          if [ "${{ steps.import.outputs.exit_code }}" == "0" ] && [ "${{ steps.motherduck.outputs.upload_success }}" == "true" ]; then
            STATUS="complete"
            MESSAGE="Import successful! Data uploaded to MotherDuck database: ${{ steps.parse.outputs.database_name }}"
          elif [ "${{ steps.import.outputs.exit_code }}" != "0" ]; then
            STATUS="failed"
            MESSAGE="Import failed during data collection phase"
          else
            STATUS="failed"
            MESSAGE="Import succeeded but MotherDuck upload failed"
          fi

          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "${STATUS}",
            "message": "${MESSAGE}",
            "parquet_files": ${{ steps.verify.outputs.file_count || 0 }},
            "motherduck_url": "https://app.motherduck.com/",
            "completed_at": "$(date -Iseconds)"
          }
          EOF

          # Update job status in MotherDuck
          python - <<'PYEOF'
          import os
          import duckdb
          from datetime import datetime

          token = os.environ.get('MOTHERDUCK_TOKEN')
          if not token:
              print("âš ï¸ No MOTHERDUCK_TOKEN - skipping job status update")
              exit(0)

          user_id = "${{ steps.parse.outputs.user_id }}"
          status = "${STATUS}"

          try:
              con = duckdb.connect(f"md:?motherduck_token={token}")
              now = datetime.utcnow()

              con.execute("""
                  UPDATE ops.import_jobs
                  SET status = ?, updated_at = ?
                  WHERE user_id = ?
              """, [status, now, user_id])

              con.close()
              print(f"âœ… Job status updated in MotherDuck: {status}")
          except Exception as e:
              print(f"âš ï¸ Could not update job status: {e}")
          PYEOF

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: league-import-${{ steps.parse.outputs.user_id }}
          path: |
            import.log
            motherduck.log
            job_status/
            deployment_manifest.json
            fantasy_football_data/*.parquet
            league_context.json
          retention-days: 30

      - name: Create workflow summary
        if: always()
        run: |
          echo "# League Import Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## League Information" >> $GITHUB_STEP_SUMMARY
          echo "- **League**: ${{ steps.parse.outputs.league_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Season**: ${{ steps.parse.outputs.season }}" >> $GITHUB_STEP_SUMMARY
          echo "- **User ID**: ${{ steps.parse.outputs.user_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **MotherDuck Database**: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.import.outputs.exit_code }}" == "0" ]; then
            echo "## âœ… Import Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Parquet files created: ${{ steps.verify.outputs.file_count }}" >> $GITHUB_STEP_SUMMARY
            echo "- MotherDuck upload: ${{ steps.motherduck.outputs.upload_success == 'true' && 'âœ… Success' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Files Created" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -lh fantasy_football_data/*.parquet 2>/dev/null || echo "No files found"
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. Data is now available in MotherDuck" >> $GITHUB_STEP_SUMMARY
            echo "2. Create custom analytics site based on analytics_app" >> $GITHUB_STEP_SUMMARY
            echo "3. Configure site to use database: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "## âŒ Import Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the logs for details" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Trigger Playoff Odds Calculation
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "========================================"
          echo "Triggering Playoff Odds Calculator"
          echo "========================================"

          gh workflow run playoff_odds_worker.yml \
            -f database_name="${{ steps.parse.outputs.database_name }}" \
            -f n_sims="10000" \
            -f user_id="${{ steps.parse.outputs.user_id }}"

          if [ $? -eq 0 ]; then
            echo "âœ… Playoff odds workflow triggered successfully"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Playoff Odds" >> $GITHUB_STEP_SUMMARY
            echo "A separate workflow has been triggered to calculate playoff odds with 10,000 Monte Carlo simulations." >> $GITHUB_STEP_SUMMARY
            echo "This may take up to 3 hours to complete." >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Could not trigger playoff odds workflow"
          fi

      # Note: Issue creation removed due to permission requirements
      # Failures are logged in workflow summary and artifacts instead
