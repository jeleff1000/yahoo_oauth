name: League Import Worker

# This workflow is triggered on-demand from Streamlit when a user requests
# to create their own fantasy football analytics site

on:
  workflow_dispatch:
    inputs:
      league_data_b64:
        description: 'Base64-encoded JSON with league configuration'
        required: false
        type: string
      league_data:
        description: 'DEPRECATED - raw JSON (for backward compat)'
        required: false
        type: string
      user_id:
        description: 'Unique identifier for this user/import job'
        required: true
        type: string

  repository_dispatch:
    types: [league_import]

# Permissions needed to trigger the playoff odds workflow
permissions:
  actions: write
  contents: read

jobs:
  import-league-data:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Allow up to 2 hours for full import

    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Parse league data
        id: parse
        env:
          YAHOO_CLIENT_ID: ${{ secrets.YAHOO_CLIENT_ID }}
          YAHOO_CLIENT_SECRET: ${{ secrets.YAHOO_CLIENT_SECRET }}
          LEAGUE_DATA_B64: ${{ github.event.inputs.league_data_b64 || github.event.client_payload.league_data_b64 || '' }}
          LEAGUE_DATA_RAW: ${{ github.event.inputs.league_data || github.event.client_payload.league_data || '' }}
          USER_ID_INPUT: ${{ github.event.inputs.user_id || github.event.client_payload.user_id }}
        run: |
          python - <<'PYEOF'
          import json
          import os
          import sys
          import base64

          # Get input data from environment (avoids shell escaping issues)
          league_data_b64 = os.environ.get('LEAGUE_DATA_B64', '').strip()
          league_data_raw = os.environ.get('LEAGUE_DATA_RAW', '').strip()
          user_id = os.environ.get('USER_ID_INPUT', '')

          print(f"[DEBUG] Received league_data_b64 length: {len(league_data_b64)}")
          print(f"[DEBUG] Received league_data_raw length: {len(league_data_raw)}")

          league_data = None

          # Try base64-encoded format first (new format)
          if league_data_b64:
              print("[DEBUG] Using base64-encoded format")
              print(f"[DEBUG] First 50 chars of base64: {league_data_b64[:50]}...")
              try:
                  league_data_json = base64.b64decode(league_data_b64).decode('utf-8')
                  league_data = json.loads(league_data_json)
                  print(f"‚úÖ Successfully decoded base64 league data")
              except Exception as e:
                  print(f"WARNING: Failed to decode base64: {e}")
                  league_data = None

          # Fallback to raw JSON format (old format - may be truncated!)
          if league_data is None and league_data_raw:
              print("[DEBUG] Falling back to raw JSON format (WARNING: may be truncated)")
              print(f"[DEBUG] First 100 chars of raw: {league_data_raw[:100]}...")
              try:
                  league_data = json.loads(league_data_raw)
                  print(f"‚úÖ Successfully parsed raw JSON")
              except Exception as e:
                  print(f"ERROR: Failed to parse raw JSON: {e}")
                  league_data = None

          if league_data is None:
              print("ERROR: No valid league data provided (neither base64 nor raw JSON)")
              print("Make sure your Streamlit app is sending league_data_b64")
              sys.exit(1)

          print(f"[DEBUG] Decoded league_id: {league_data.get('league_id')}")
          print(f"[DEBUG] Decoded league_name: {league_data.get('league_name')}")
          print(f"[DEBUG] Decoded season: {league_data.get('season')}")
          print(f"[DEBUG] Decoded start_year: {league_data.get('start_year')}")

          # Extract league configuration
          league_id = league_data.get('league_id', '')
          league_name = league_data.get('league_name', 'Unknown League')
          season = league_data.get('season', league_data.get('end_year', 2024))
          start_year = league_data.get('start_year', season)
          oauth_raw = league_data.get('oauth_token', {})

          # Add Yahoo app credentials from secrets (required for yahoo_oauth library)
          consumer_key = os.environ.get('YAHOO_CLIENT_ID')
          consumer_secret = os.environ.get('YAHOO_CLIENT_SECRET')

          if not consumer_key or not consumer_secret:
              print("‚ö†Ô∏è  WARNING: YAHOO_CLIENT_ID and YAHOO_CLIENT_SECRET not set")
              print("   Import will likely fail - add these to GitHub secrets")

          # Build oauth_data in the format yahoo_oauth library expects
          # The library checks for token_time to determine if refresh is needed
          import time
          oauth_data = {
              'access_token': oauth_raw.get('access_token'),
              'refresh_token': oauth_raw.get('refresh_token'),
              'consumer_key': consumer_key,
              'consumer_secret': consumer_secret,
              'token_type': oauth_raw.get('token_type', 'bearer'),
              'expires_in': oauth_raw.get('expires_in', 3600),
              # token_time is CRITICAL - yahoo_oauth uses this to check expiration
              # Use the provided token_time or set to now (token will try refresh)
              'token_time': oauth_raw.get('token_time', time.time()),
              'guid': oauth_raw.get('xoauth_yahoo_guid') or oauth_raw.get('guid'),
          }

          print(f"‚úÖ Built OAuth data with consumer credentials")
          print(f"   access_token present: {bool(oauth_data.get('access_token'))}")
          print(f"   refresh_token present: {bool(oauth_data.get('refresh_token'))}")
          print(f"   consumer_key present: {bool(oauth_data.get('consumer_key'))}")
          print(f"   token_time: {oauth_data.get('token_time')}")

          # Create sanitized database name (just league name, no year - data contains all historical years)
          db_name = league_name.lower().replace(' ', '_').replace('-', '_')
          db_name = ''.join(c if c.isalnum() or c == '_' else '' for c in db_name)

          # Set outputs for next steps
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"user_id={user_id}\n")
              f.write(f"league_id={league_id}\n")
              f.write(f"league_name={league_name}\n")
              f.write(f"season={season}\n")
              f.write(f"start_year={start_year}\n")
              f.write(f"database_name={db_name}\n")

          # Save OAuth token to file (now includes consumer credentials)
          os.makedirs("oauth", exist_ok=True)
          with open("oauth/Oauth.json", 'w') as f:
              json.dump(oauth_data, f, indent=2)

          # Save full league data for context creation
          with open("league_data_input.json", 'w') as f:
              json.dump(league_data, f, indent=2)

          print(f"‚úÖ Parsed league: {league_name} ({season})")
          print(f"   League ID: {league_id}")
          print(f"   Database: {db_name}")
          print(f"   User ID: {user_id}")
          PYEOF

      - name: Update job status - Starting
        run: |
          mkdir -p job_status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "running",
            "phase": "starting",
            "message": "Initializing import...",
            "started_at": "$(date -Iseconds)"
          }
          EOF

      - name: Create league context
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Load the league data
          with open("league_data_input.json") as f:
              league_data = json.load(f)

          # Get absolute path to OAuth file (script runs from fantasy_football_data_scripts subdirectory)
          oauth_file = Path("oauth/Oauth.json").resolve()

          # Ensure numeric values are integers (not strings from JSON)
          start_year = int(league_data.get('start_year', 2014))
          end_year = int(league_data.get('season', league_data.get('end_year', 2024)))
          num_teams = int(league_data.get('num_teams', 10))
          playoff_teams = int(league_data.get('playoff_teams', 6))
          regular_season_weeks = int(league_data.get('regular_season_weeks', 14))

          # Create league context file for initial_import_v2.py
          context = {
              "league_id": league_data.get('league_id'),
              "league_name": league_data.get('league_name'),
              "oauth_file_path": str(oauth_file),  # Use absolute path
              "game_code": league_data.get('game_code', 'nfl'),
              "start_year": start_year,
              "end_year": end_year,
              "num_teams": num_teams,
              "playoff_teams": playoff_teams,
              "regular_season_weeks": regular_season_weeks,
              "data_directory": str(Path("fantasy_football_data").resolve()),  # Absolute path
              "max_workers": 5,
              "enable_caching": True,
              "rate_limit_per_sec": 4.0,
              "manager_name_overrides": league_data.get('manager_name_overrides', {})
          }

          # Write context file
          with open("league_context.json", 'w') as f:
              json.dump(context, f, indent=2)

          print("‚úÖ League context created")
          print(json.dumps(context, indent=2))
          PYEOF

      - name: Create data directory
        run: |
          mkdir -p fantasy_football_data
          echo "‚úÖ Data directory ready"

      - name: Run initial import
        id: import
        env:
          AUTO_CONFIRM: "1"
        working-directory: fantasy_football_data_scripts
        run: |
          echo "========================================"
          echo "Starting import for ${{ steps.parse.outputs.league_name }}"
          echo "Season: ${{ steps.parse.outputs.season }}"
          echo "User ID: ${{ steps.parse.outputs.user_id }}"
          echo "========================================"

          # Update status
          cat > ../job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "importing",
            "message": "Running initial_import_v2.py...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          # Run the import
          python initial_import_v2.py --context ../league_context.json 2>&1 | tee ../import.log

          # Capture exit code
          EXIT_CODE=${PIPESTATUS[0]}
          echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT

          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Import completed successfully"
          else
            echo "‚ùå Import failed with exit code ${EXIT_CODE}"
            echo "Last 100 lines of log:"
            tail -100 ../import.log
            exit $EXIT_CODE
          fi

      - name: Verify parquet files created
        if: steps.import.outputs.exit_code == '0'
        id: verify
        run: |
          echo "Verifying parquet files..."

          if [ ! -d "fantasy_football_data" ]; then
            echo "‚ùå ERROR: Data directory not found!"
            exit 1
          fi

          PARQUET_FILES=$(find fantasy_football_data -name "*.parquet" -type f)
          PARQUET_COUNT=$(echo "$PARQUET_FILES" | grep -c ".parquet" || echo "0")

          if [ "$PARQUET_COUNT" -eq 0 ]; then
            echo "‚ùå ERROR: No parquet files were created!"
            exit 1
          fi

          echo "‚úÖ Found ${PARQUET_COUNT} parquet files:"
          ls -lh fantasy_football_data/*.parquet

          # Save file list
          echo "$PARQUET_FILES" > parquet_files.txt
          echo "file_count=${PARQUET_COUNT}" >> $GITHUB_OUTPUT

      - name: Diagnose parquet files
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Diagnostic: Parquet file columns"
          echo "========================================"
          python - <<'PYEOF'
          import pandas as pd
          from pathlib import Path

          data_dir = Path("fantasy_football_data")

          # Check canonical files
          canonical_files = ["player.parquet", "matchup.parquet", "draft.parquet", "transactions.parquet", "schedule.parquet"]

          for fname in canonical_files:
              fpath = data_dir / fname
              print(f"\n{'='*60}")
              print(f"File: {fname}")
              print('='*60)
              if fpath.exists():
                  try:
                      df = pd.read_parquet(fpath)
                      print(f"  Rows: {len(df):,}")
                      print(f"  Columns: {len(df.columns)}")

                      # Check for critical columns
                      critical = ["manager_week", "cumulative_week", "manager", "year", "week", "fantasy_points"]
                      for col in critical:
                          if col in df.columns:
                              non_null = df[col].notna().sum()
                              print(f"  ‚úì {col}: {non_null:,} non-null values")
                          else:
                              print(f"  ‚úó {col}: MISSING")

                      # Show first few column names
                      print(f"  Sample columns: {list(df.columns[:15])}...")
                  except Exception as e:
                      print(f"  ERROR reading file: {e}")
              else:
                  print(f"  FILE NOT FOUND")

          # Check subdirectories for intermediate files
          print(f"\n{'='*60}")
          print("Subdirectory file counts:")
          print('='*60)
          for subdir in ["player_data", "matchup_data", "draft_data", "transaction_data", "schedule_data"]:
              subpath = data_dir / subdir
              if subpath.exists():
                  files = list(subpath.glob("*.parquet"))
                  print(f"  {subdir}/: {len(files)} parquet files")
              else:
                  print(f"  {subdir}/: DIRECTORY NOT FOUND")
          PYEOF

      - name: Consolidate league settings to parquet
        if: steps.import.outputs.exit_code == '0'
        run: |
          echo "========================================"
          echo "Consolidating league settings to parquet"
          echo "========================================"
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import pandas as pd

          settings_dir = Path("fantasy_football_data/league_settings")
          output_file = Path("fantasy_football_data/league_settings.parquet")

          if not settings_dir.exists():
              print(f"‚ö†Ô∏è  No league_settings directory found at {settings_dir}")
              print("   Playoff odds will use default configuration")
              exit(0)

          settings_files = list(settings_dir.glob("league_settings_*.json"))

          if not settings_files:
              print(f"‚ö†Ô∏è  No league settings JSON files found in {settings_dir}")
              exit(0)

          print(f"Found {len(settings_files)} league settings files")

          rows = []
          for sf in sorted(settings_files):
              try:
                  with open(sf, 'r', encoding='utf-8') as f:
                      data = json.load(f)

                  # Extract key fields for playoff odds calculation
                  metadata = data.get('metadata', {})
                  row = {
                      'year': data.get('year'),
                      'league_key': data.get('league_key'),
                      'num_teams': metadata.get('num_teams'),
                      'num_playoff_teams': metadata.get('num_playoff_teams') or metadata.get('playoff_teams'),
                      'playoff_start_week': metadata.get('playoff_start_week'),
                      'uses_playoff_reseeding': metadata.get('uses_playoff_reseeding'),
                      'bye_teams': metadata.get('bye_teams'),
                      'current_week': metadata.get('current_week'),
                      'is_finished': metadata.get('is_finished'),
                      # Store full settings as JSON string for flexibility
                      'settings_json': json.dumps(data)
                  }
                  rows.append(row)
                  print(f"  ‚úì {sf.name}: year={row['year']}, playoff_teams={row['num_playoff_teams']}")
              except Exception as e:
                  print(f"  ‚úó {sf.name}: {e}")

          if rows:
              df = pd.DataFrame(rows)
              df.to_parquet(output_file, index=False)
              print(f"\n‚úÖ Created {output_file} with {len(rows)} years of settings")
          else:
              print("‚ö†Ô∏è  No valid settings to save")
          PYEOF

      - name: Upload to MotherDuck
        if: steps.import.outputs.exit_code == '0'
        id: motherduck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          DATABASE_NAME: ${{ steps.parse.outputs.database_name }}
          LEAGUE_NAME: ${{ steps.parse.outputs.league_name }}
          SEASON: ${{ steps.parse.outputs.season }}
        run: |
          # Update status
          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "status": "running",
            "phase": "uploading",
            "message": "Uploading to MotherDuck...",
            "updated_at": "$(date -Iseconds)"
          }
          EOF

          if [ -z "$MOTHERDUCK_TOKEN" ]; then
            echo "‚ö†Ô∏è  WARNING: MOTHERDUCK_TOKEN not set!"
            echo "Cannot upload to MotherDuck. Please add MOTHERDUCK_TOKEN as a GitHub secret."
            exit 1
          fi

          echo "========================================"
          echo "Uploading to MotherDuck"
          echo "Database: ${DATABASE_NAME}"
          echo "========================================"

          # Use the motherduck_upload.py script
          cd fantasy_football_data
          python motherduck_upload.py "${DATABASE_NAME}" . 2>&1 | tee ../motherduck.log

          UPLOAD_EXIT=$?

          if [ $UPLOAD_EXIT -eq 0 ]; then
            echo "‚úÖ MotherDuck upload complete!"
            echo "üìä Database: ${DATABASE_NAME}"
            echo "üîó Access at: https://app.motherduck.com/"
            echo "upload_success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå MotherDuck upload failed!"
            tail -50 ../motherduck.log
            echo "upload_success=false" >> $GITHUB_OUTPUT
            exit $UPLOAD_EXIT
          fi

      - name: Create deployment manifest
        if: steps.motherduck.outputs.upload_success == 'true'
        run: |
          python - <<'PYEOF'
          import json
          from pathlib import Path
          import os

          # Create a deployment manifest with all the info needed to create the user's site
          manifest = {
              "user_id": "${{ steps.parse.outputs.user_id }}",
              "league_name": "${{ steps.parse.outputs.league_name }}",
              "league_id": "${{ steps.parse.outputs.league_id }}",
              "season": "${{ steps.parse.outputs.season }}",
              "motherduck_database": "${{ steps.parse.outputs.database_name }}",
              "parquet_file_count": ${{ steps.verify.outputs.file_count }},
              "created_at": os.popen('date -Iseconds').read().strip(),
              "workflow_run_id": "${{ github.run_id }}",
              "status": "ready",
              "next_steps": [
                  "Create KMFFLApp-based UI repository",
                  "Configure MotherDuck connection",
                  "Deploy Streamlit site"
              ]
          }

          with open("deployment_manifest.json", 'w') as f:
              json.dump(manifest, f, indent=2)

          print("‚úÖ Deployment manifest created")
          print(json.dumps(manifest, indent=2))
          PYEOF

      - name: Update job status - Complete
        if: always()
        run: |
          if [ "${{ steps.import.outputs.exit_code }}" == "0" ] && [ "${{ steps.motherduck.outputs.upload_success }}" == "true" ]; then
            STATUS="complete"
            MESSAGE="Import successful! Data uploaded to MotherDuck database: ${{ steps.parse.outputs.database_name }}"
          elif [ "${{ steps.import.outputs.exit_code }}" != "0" ]; then
            STATUS="failed"
            MESSAGE="Import failed during data collection phase"
          else
            STATUS="failed"
            MESSAGE="Import succeeded but MotherDuck upload failed"
          fi

          cat > job_status/${{ steps.parse.outputs.user_id }}.json <<EOF
          {
            "user_id": "${{ steps.parse.outputs.user_id }}",
            "league_name": "${{ steps.parse.outputs.league_name }}",
            "season": "${{ steps.parse.outputs.season }}",
            "database_name": "${{ steps.parse.outputs.database_name }}",
            "status": "${STATUS}",
            "message": "${MESSAGE}",
            "parquet_files": ${{ steps.verify.outputs.file_count || 0 }},
            "motherduck_url": "https://app.motherduck.com/",
            "completed_at": "$(date -Iseconds)"
          }
          EOF

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: league-import-${{ steps.parse.outputs.user_id }}
          path: |
            import.log
            motherduck.log
            job_status/
            deployment_manifest.json
            fantasy_football_data/*.parquet
            league_context.json
          retention-days: 30

      - name: Create workflow summary
        if: always()
        run: |
          echo "# League Import Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## League Information" >> $GITHUB_STEP_SUMMARY
          echo "- **League**: ${{ steps.parse.outputs.league_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Season**: ${{ steps.parse.outputs.season }}" >> $GITHUB_STEP_SUMMARY
          echo "- **User ID**: ${{ steps.parse.outputs.user_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **MotherDuck Database**: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.import.outputs.exit_code }}" == "0" ]; then
            echo "## ‚úÖ Import Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Parquet files created: ${{ steps.verify.outputs.file_count }}" >> $GITHUB_STEP_SUMMARY
            echo "- MotherDuck upload: ${{ steps.motherduck.outputs.upload_success == 'true' && '‚úÖ Success' || '‚ùå Failed' }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Files Created" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -lh fantasy_football_data/*.parquet 2>/dev/null || echo "No files found"
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. Data is now available in MotherDuck" >> $GITHUB_STEP_SUMMARY
            echo "2. Create custom analytics site based on KMFFLApp" >> $GITHUB_STEP_SUMMARY
            echo "3. Configure site to use database: \`${{ steps.parse.outputs.database_name }}\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ùå Import Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the logs for details" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Trigger Playoff Odds Calculation
        if: steps.motherduck.outputs.upload_success == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "========================================"
          echo "Triggering Playoff Odds Calculator"
          echo "========================================"

          gh workflow run playoff_odds_worker.yml \
            -f database_name="${{ steps.parse.outputs.database_name }}" \
            -f n_sims="10000" \
            -f user_id="${{ steps.parse.outputs.user_id }}"

          if [ $? -eq 0 ]; then
            echo "‚úÖ Playoff odds workflow triggered successfully"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Playoff Odds" >> $GITHUB_STEP_SUMMARY
            echo "A separate workflow has been triggered to calculate playoff odds with 10,000 Monte Carlo simulations." >> $GITHUB_STEP_SUMMARY
            echo "This may take up to 3 hours to complete." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è Could not trigger playoff odds workflow"
          fi

      # Note: Issue creation removed due to permission requirements
      # Failures are logged in workflow summary and artifacts instead
